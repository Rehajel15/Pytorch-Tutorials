{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rehajel15/Pytorch-Tutorials/blob/main/01_pytorch_workflow_exercises_run2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01. PyTorch Workflow Exercise Template\n",
        "\n",
        "The following is a template for the PyTorch workflow exercises.\n",
        "\n",
        "It's only starter code and it's your job to fill in the blanks.\n",
        "\n",
        "Because of the flexibility of PyTorch, there may be more than one way to answer the question.\n",
        "\n",
        "Don't worry about trying to be *right* just try writing code that suffices the question.\n",
        "\n",
        "You can see one form of [solutions on GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions) (but try the exercises below yourself first!)."
      ],
      "metadata": {
        "id": "N8LsPXZti9Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Glu2fM4dkNlx"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "LqKhXY26m31s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ff0eb05-7cab-4fed-b94e-82c5b259f6a7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Create a straight line dataset using the linear regression formula (`weight * X + bias`).\n",
        "  * Set `weight=0.3` and `bias=0.9` there should be at least 100 datapoints total.\n",
        "  * Split the data into 80% training, 20% testing.\n",
        "  * Plot the training and testing data so it becomes visual.\n",
        "\n",
        "Your output of the below cell should look something like:\n",
        "```\n",
        "Number of X samples: 100\n",
        "Number of y samples: 100\n",
        "First 10 X & y samples:\n",
        "X: tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n",
        "        0.0900])\n",
        "y: tensor([0.9000, 0.9030, 0.9060, 0.9090, 0.9120, 0.9150, 0.9180, 0.9210, 0.9240,\n",
        "        0.9270])\n",
        "```\n",
        "\n",
        "Of course the numbers in `X` and `y` may be different but ideally they're created using the linear regression formula."
      ],
      "metadata": {
        "id": "g7HUhxCxjeBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data parameters\n",
        "weight = 0.3\n",
        "bias = 0.9\n",
        "\n",
        "# Make X and y using linear regression feature\n",
        "X = torch.arange(start=0, end=1, step=0.01).unsqueeze(dim=1)\n",
        "y = weight * X + bias\n",
        "print(f\"Number of X samples: {len(X)}\")\n",
        "print(f\"Number of y samples: {len(y)}\")\n",
        "print(f\"First 10 X & y samples:\\nX: {X[:10]}\\ny: {y[:10]}\")"
      ],
      "metadata": {
        "id": "KbDG5MV7jhvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b934e93-396b-455a-df2c-73de554aa947"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of X samples: 100\n",
            "Number of y samples: 100\n",
            "First 10 X & y samples:\n",
            "X: tensor([[0.0000],\n",
            "        [0.0100],\n",
            "        [0.0200],\n",
            "        [0.0300],\n",
            "        [0.0400],\n",
            "        [0.0500],\n",
            "        [0.0600],\n",
            "        [0.0700],\n",
            "        [0.0800],\n",
            "        [0.0900]])\n",
            "y: tensor([[0.9000],\n",
            "        [0.9030],\n",
            "        [0.9060],\n",
            "        [0.9090],\n",
            "        [0.9120],\n",
            "        [0.9150],\n",
            "        [0.9180],\n",
            "        [0.9210],\n",
            "        [0.9240],\n",
            "        [0.9270]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:train_split], X[train_split:]\n",
        "y_train, y_test = y[:train_split], y[train_split:]\n",
        "\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "GlwtT1djkmLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "868fa700-72dd-4e32-f0d3-25aeb64fe289"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 20, 80, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(train_data=X_train, test_data=X_test, train_labels=y_train, test_labels=y_test, predictions=None):\n",
        "\n",
        "  plt.figure(figsize=(10,7))\n",
        "\n",
        "  # Plot training data in blue\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
        "\n",
        "  # Plot test data in green\n",
        "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
        "\n",
        "  # Are there predictions?\n",
        "  if predictions is not None:\n",
        "    # Plot the predictions if they exists\n",
        "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "  # Show the legend\n",
        "  plt.legend(prop={\"size\":14})"
      ],
      "metadata": {
        "id": "29iQZFNhlYJ-"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "P7q66GlDADJq",
        "outputId": "3e9e532a-fcf5-43bd-d5df-6d2d02ef87a5"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Build a PyTorch model by subclassing `nn.Module`.\n",
        "  * Inside should be a randomly initialized `nn.Parameter()` with `requires_grad=True`, one for `weights` and one for `bias`.\n",
        "  * Implement the `forward()` method to compute the linear regression function you used to create the dataset in 1.\n",
        "  * Once you've constructed the model, make an instance of it and check its `state_dict()`.\n",
        "  * **Note:** If you'd like to use `nn.Linear()` instead of `nn.Parameter()` you can."
      ],
      "metadata": {
        "id": "ImZoe3v8jif8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PyTorch linear regression model by subclassing nn.Module\n",
        "class LinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(in_features=1, out_features=1)\n",
        "\n",
        "  def forward(self, x:torch.Tensor)->torch.Tensor:\n",
        "    return self.l1(x)"
      ],
      "metadata": {
        "id": "qzd__Y5rjtB8"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model and put it to the target device\n",
        "model_0 = LinearRegressionModel()\n",
        "model_0.to(device)"
      ],
      "metadata": {
        "id": "5LdcDnmOmyQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0833ce69-ec0f-4c34-8f17-07965a5f5fd9"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegressionModel(\n",
              "  (l1): Linear(in_features=1, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create a loss function and optimizer using `nn.L1Loss()` and `torch.optim.SGD(params, lr)` respectively.\n",
        "  * Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2.\n",
        "  * Write a training loop to perform the appropriate training steps for 300 epochs.\n",
        "  * The training loop should test the model on the test dataset every 20 epochs."
      ],
      "metadata": {
        "id": "G6nYOrJhjtfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def training_function(model:nn.Module, train_data:torch.Tensor, train_labels:torch.Tensor, loss_fn:torch.nn.Module, optimizer:torch.optim.Optimizer) -> torch.Tensor:\n",
        "  model.train()\n",
        "\n",
        "  y_preds = model(train_data)\n",
        "\n",
        "  loss = loss_fn(y_preds, train_labels)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "KhGqtHD1Juwd"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function\n",
        "def test_function(model:nn.Module, test_data:torch.Tensor, test_labels:torch.Tensor, loss_fn:nn.Module) -> torch.Tensor:\n",
        "  model.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    test_preds = model(test_data)\n",
        "    test_loss = loss_fn(test_preds, test_labels)\n",
        "\n",
        "  return test_loss"
      ],
      "metadata": {
        "id": "wXdY3FqsOLMX"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "y_train = y_train.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "# Training loop\n",
        "epochs = 100000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss = training_function(model_0, X_train, y_train, nn.L1Loss(), torch.optim.SGD(model_0.parameters(), lr=0.0001))\n",
        "\n",
        "  if epoch % 20 == 0:\n",
        "    test_loss = test_function(model_0, X_test, y_test, nn.L1Loss())\n",
        "\n",
        "    print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")"
      ],
      "metadata": {
        "id": "xpE83NvNnkdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89fe8086-9576-4434-f766-93f1a76145a9"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Train loss: 1.2126580476760864 | Test loss: 1.743800163269043\n",
            "Epoch: 20 | Train loss: 1.2103458642959595 | Test loss: 1.7410929203033447\n",
            "Epoch: 40 | Train loss: 1.208033561706543 | Test loss: 1.7383854389190674\n",
            "Epoch: 60 | Train loss: 1.205721378326416 | Test loss: 1.73567795753479\n",
            "Epoch: 80 | Train loss: 1.2034090757369995 | Test loss: 1.7329705953598022\n",
            "Epoch: 100 | Train loss: 1.2010968923568726 | Test loss: 1.730263113975525\n",
            "Epoch: 120 | Train loss: 1.1987847089767456 | Test loss: 1.7275556325912476\n",
            "Epoch: 140 | Train loss: 1.196472406387329 | Test loss: 1.7248483896255493\n",
            "Epoch: 160 | Train loss: 1.1941603422164917 | Test loss: 1.722140908241272\n",
            "Epoch: 180 | Train loss: 1.1918480396270752 | Test loss: 1.7194336652755737\n",
            "Epoch: 200 | Train loss: 1.1895357370376587 | Test loss: 1.7167261838912964\n",
            "Epoch: 220 | Train loss: 1.1872236728668213 | Test loss: 1.7140188217163086\n",
            "Epoch: 240 | Train loss: 1.1849112510681152 | Test loss: 1.7113113403320312\n",
            "Epoch: 260 | Train loss: 1.1825990676879883 | Test loss: 1.708603858947754\n",
            "Epoch: 280 | Train loss: 1.1802868843078613 | Test loss: 1.7058966159820557\n",
            "Epoch: 300 | Train loss: 1.1779747009277344 | Test loss: 1.7031891345977783\n",
            "Epoch: 320 | Train loss: 1.1756623983383179 | Test loss: 1.7004817724227905\n",
            "Epoch: 340 | Train loss: 1.173350214958191 | Test loss: 1.6977742910385132\n",
            "Epoch: 360 | Train loss: 1.171038031578064 | Test loss: 1.6950668096542358\n",
            "Epoch: 380 | Train loss: 1.168725848197937 | Test loss: 1.6923595666885376\n",
            "Epoch: 400 | Train loss: 1.1664135456085205 | Test loss: 1.6896523237228394\n",
            "Epoch: 420 | Train loss: 1.164101243019104 | Test loss: 1.686944603919983\n",
            "Epoch: 440 | Train loss: 1.161789059638977 | Test loss: 1.6842373609542847\n",
            "Epoch: 460 | Train loss: 1.15947687625885 | Test loss: 1.6815299987792969\n",
            "Epoch: 480 | Train loss: 1.1571645736694336 | Test loss: 1.6788225173950195\n",
            "Epoch: 500 | Train loss: 1.1548523902893066 | Test loss: 1.6761150360107422\n",
            "Epoch: 520 | Train loss: 1.1525400876998901 | Test loss: 1.673407793045044\n",
            "Epoch: 540 | Train loss: 1.1502279043197632 | Test loss: 1.6707004308700562\n",
            "Epoch: 560 | Train loss: 1.1479157209396362 | Test loss: 1.6679928302764893\n",
            "Epoch: 580 | Train loss: 1.1456035375595093 | Test loss: 1.6652854681015015\n",
            "Epoch: 600 | Train loss: 1.1432913541793823 | Test loss: 1.6625782251358032\n",
            "Epoch: 620 | Train loss: 1.1409790515899658 | Test loss: 1.6598705053329468\n",
            "Epoch: 640 | Train loss: 1.1386668682098389 | Test loss: 1.6571632623672485\n",
            "Epoch: 660 | Train loss: 1.136354684829712 | Test loss: 1.6544560194015503\n",
            "Epoch: 680 | Train loss: 1.1340423822402954 | Test loss: 1.6517482995986938\n",
            "Epoch: 700 | Train loss: 1.131730079650879 | Test loss: 1.6490411758422852\n",
            "Epoch: 720 | Train loss: 1.129417896270752 | Test loss: 1.6463336944580078\n",
            "Epoch: 740 | Train loss: 1.1271055936813354 | Test loss: 1.6436262130737305\n",
            "Epoch: 760 | Train loss: 1.1247934103012085 | Test loss: 1.6409187316894531\n",
            "Epoch: 780 | Train loss: 1.1224812269210815 | Test loss: 1.6382114887237549\n",
            "Epoch: 800 | Train loss: 1.1201690435409546 | Test loss: 1.635504126548767\n",
            "Epoch: 820 | Train loss: 1.1178568601608276 | Test loss: 1.6327965259552002\n",
            "Epoch: 840 | Train loss: 1.1155445575714111 | Test loss: 1.6300891637802124\n",
            "Epoch: 860 | Train loss: 1.1132323741912842 | Test loss: 1.6273819208145142\n",
            "Epoch: 880 | Train loss: 1.1109201908111572 | Test loss: 1.6246744394302368\n",
            "Epoch: 900 | Train loss: 1.1086078882217407 | Test loss: 1.6219671964645386\n",
            "Epoch: 920 | Train loss: 1.1062955856323242 | Test loss: 1.6192597150802612\n",
            "Epoch: 940 | Train loss: 1.1039834022521973 | Test loss: 1.6165522336959839\n",
            "Epoch: 960 | Train loss: 1.1016712188720703 | Test loss: 1.613844871520996\n",
            "Epoch: 980 | Train loss: 1.0993589162826538 | Test loss: 1.6111373901367188\n",
            "Epoch: 1000 | Train loss: 1.0970467329025269 | Test loss: 1.6084301471710205\n",
            "Epoch: 1020 | Train loss: 1.0947345495224 | Test loss: 1.6057226657867432\n",
            "Epoch: 1040 | Train loss: 1.092422366142273 | Test loss: 1.6030151844024658\n",
            "Epoch: 1060 | Train loss: 1.0901100635528564 | Test loss: 1.6003077030181885\n",
            "Epoch: 1080 | Train loss: 1.0877978801727295 | Test loss: 1.5976003408432007\n",
            "Epoch: 1100 | Train loss: 1.0854856967926025 | Test loss: 1.594892978668213\n",
            "Epoch: 1120 | Train loss: 1.083173394203186 | Test loss: 1.592185616493225\n",
            "Epoch: 1140 | Train loss: 1.080861210823059 | Test loss: 1.5894782543182373\n",
            "Epoch: 1160 | Train loss: 1.0785489082336426 | Test loss: 1.5867708921432495\n",
            "Epoch: 1180 | Train loss: 1.0762367248535156 | Test loss: 1.5840634107589722\n",
            "Epoch: 1200 | Train loss: 1.0739245414733887 | Test loss: 1.5813560485839844\n",
            "Epoch: 1220 | Train loss: 1.0716122388839722 | Test loss: 1.578648567199707\n",
            "Epoch: 1240 | Train loss: 1.0693000555038452 | Test loss: 1.5759410858154297\n",
            "Epoch: 1260 | Train loss: 1.0669878721237183 | Test loss: 1.5732338428497314\n",
            "Epoch: 1280 | Train loss: 1.0646756887435913 | Test loss: 1.570526361465454\n",
            "Epoch: 1300 | Train loss: 1.0623635053634644 | Test loss: 1.5678189992904663\n",
            "Epoch: 1320 | Train loss: 1.0600512027740479 | Test loss: 1.565111517906189\n",
            "Epoch: 1340 | Train loss: 1.0577389001846313 | Test loss: 1.5624041557312012\n",
            "Epoch: 1360 | Train loss: 1.0554267168045044 | Test loss: 1.5596967935562134\n",
            "Epoch: 1380 | Train loss: 1.053114414215088 | Test loss: 1.5569894313812256\n",
            "Epoch: 1400 | Train loss: 1.050802230834961 | Test loss: 1.5542819499969482\n",
            "Epoch: 1420 | Train loss: 1.048490047454834 | Test loss: 1.5515745878219604\n",
            "Epoch: 1440 | Train loss: 1.046177864074707 | Test loss: 1.5488673448562622\n",
            "Epoch: 1460 | Train loss: 1.0438660383224487 | Test loss: 1.546160101890564\n",
            "Epoch: 1480 | Train loss: 1.0415540933609009 | Test loss: 1.5434529781341553\n",
            "Epoch: 1500 | Train loss: 1.039242148399353 | Test loss: 1.5407459735870361\n",
            "Epoch: 1520 | Train loss: 1.0369303226470947 | Test loss: 1.5380388498306274\n",
            "Epoch: 1540 | Train loss: 1.0346182584762573 | Test loss: 1.5353317260742188\n",
            "Epoch: 1560 | Train loss: 1.0323063135147095 | Test loss: 1.5326247215270996\n",
            "Epoch: 1580 | Train loss: 1.0299944877624512 | Test loss: 1.529917597770691\n",
            "Epoch: 1600 | Train loss: 1.0276825428009033 | Test loss: 1.5272104740142822\n",
            "Epoch: 1620 | Train loss: 1.0253705978393555 | Test loss: 1.524503231048584\n",
            "Epoch: 1640 | Train loss: 1.0230587720870972 | Test loss: 1.5217962265014648\n",
            "Epoch: 1660 | Train loss: 1.0207467079162598 | Test loss: 1.5190891027450562\n",
            "Epoch: 1680 | Train loss: 1.0184348821640015 | Test loss: 1.516382098197937\n",
            "Epoch: 1700 | Train loss: 1.016122817993164 | Test loss: 1.5136749744415283\n",
            "Epoch: 1720 | Train loss: 1.0138109922409058 | Test loss: 1.5109678506851196\n",
            "Epoch: 1740 | Train loss: 1.011499047279358 | Test loss: 1.508260726928711\n",
            "Epoch: 1760 | Train loss: 1.00918710231781 | Test loss: 1.5055536031723022\n",
            "Epoch: 1780 | Train loss: 1.0068752765655518 | Test loss: 1.502846598625183\n",
            "Epoch: 1800 | Train loss: 1.004563331604004 | Test loss: 1.5001394748687744\n",
            "Epoch: 1820 | Train loss: 1.0022512674331665 | Test loss: 1.4974322319030762\n",
            "Epoch: 1840 | Train loss: 0.9999394416809082 | Test loss: 1.494725227355957\n",
            "Epoch: 1860 | Train loss: 0.9976274371147156 | Test loss: 1.4920179843902588\n",
            "Epoch: 1880 | Train loss: 0.9953155517578125 | Test loss: 1.4893109798431396\n",
            "Epoch: 1900 | Train loss: 0.9930036664009094 | Test loss: 1.4866037368774414\n",
            "Epoch: 1920 | Train loss: 0.9906916618347168 | Test loss: 1.4838968515396118\n",
            "Epoch: 1940 | Train loss: 0.9883797764778137 | Test loss: 1.4811896085739136\n",
            "Epoch: 1960 | Train loss: 0.9860678911209106 | Test loss: 1.4784826040267944\n",
            "Epoch: 1980 | Train loss: 0.983755886554718 | Test loss: 1.4757755994796753\n",
            "Epoch: 2000 | Train loss: 0.9814440011978149 | Test loss: 1.4730682373046875\n",
            "Epoch: 2020 | Train loss: 0.9791321158409119 | Test loss: 1.4703612327575684\n",
            "Epoch: 2040 | Train loss: 0.976820170879364 | Test loss: 1.4676541090011597\n",
            "Epoch: 2060 | Train loss: 0.9745082855224609 | Test loss: 1.464946985244751\n",
            "Epoch: 2080 | Train loss: 0.9721962809562683 | Test loss: 1.4622398614883423\n",
            "Epoch: 2100 | Train loss: 0.9698843359947205 | Test loss: 1.4595328569412231\n",
            "Epoch: 2120 | Train loss: 0.9675723910331726 | Test loss: 1.456825613975525\n",
            "Epoch: 2140 | Train loss: 0.9652605056762695 | Test loss: 1.4541186094284058\n",
            "Epoch: 2160 | Train loss: 0.9629485011100769 | Test loss: 1.4514116048812866\n",
            "Epoch: 2180 | Train loss: 0.9606366157531738 | Test loss: 1.4487043619155884\n",
            "Epoch: 2200 | Train loss: 0.9583247303962708 | Test loss: 1.4459972381591797\n",
            "Epoch: 2220 | Train loss: 0.9560127258300781 | Test loss: 1.443290114402771\n",
            "Epoch: 2240 | Train loss: 0.953700840473175 | Test loss: 1.4405831098556519\n",
            "Epoch: 2260 | Train loss: 0.951388955116272 | Test loss: 1.4378759860992432\n",
            "Epoch: 2280 | Train loss: 0.9490770697593689 | Test loss: 1.435168743133545\n",
            "Epoch: 2300 | Train loss: 0.946765124797821 | Test loss: 1.4324617385864258\n",
            "Epoch: 2320 | Train loss: 0.9444531798362732 | Test loss: 1.429754614830017\n",
            "Epoch: 2340 | Train loss: 0.9421411752700806 | Test loss: 1.427047610282898\n",
            "Epoch: 2360 | Train loss: 0.9398292899131775 | Test loss: 1.4243404865264893\n",
            "Epoch: 2380 | Train loss: 0.9375173449516296 | Test loss: 1.4216331243515015\n",
            "Epoch: 2400 | Train loss: 0.9352054595947266 | Test loss: 1.4189262390136719\n",
            "Epoch: 2420 | Train loss: 0.9328935742378235 | Test loss: 1.4162191152572632\n",
            "Epoch: 2440 | Train loss: 0.9305815696716309 | Test loss: 1.413512110710144\n",
            "Epoch: 2460 | Train loss: 0.9282696843147278 | Test loss: 1.4108049869537354\n",
            "Epoch: 2480 | Train loss: 0.9259576797485352 | Test loss: 1.408097743988037\n",
            "Epoch: 2500 | Train loss: 0.9236457943916321 | Test loss: 1.405390739440918\n",
            "Epoch: 2520 | Train loss: 0.921333909034729 | Test loss: 1.4026836156845093\n",
            "Epoch: 2540 | Train loss: 0.9190220236778259 | Test loss: 1.3999764919281006\n",
            "Epoch: 2560 | Train loss: 0.9167100191116333 | Test loss: 1.3972692489624023\n",
            "Epoch: 2580 | Train loss: 0.9143981337547302 | Test loss: 1.3945623636245728\n",
            "Epoch: 2600 | Train loss: 0.9120861887931824 | Test loss: 1.3918551206588745\n",
            "Epoch: 2620 | Train loss: 0.9097742438316345 | Test loss: 1.3891481161117554\n",
            "Epoch: 2640 | Train loss: 0.9074622988700867 | Test loss: 1.3864408731460571\n",
            "Epoch: 2660 | Train loss: 0.9051504135131836 | Test loss: 1.3837337493896484\n",
            "Epoch: 2680 | Train loss: 0.9028385281562805 | Test loss: 1.3810267448425293\n",
            "Epoch: 2700 | Train loss: 0.9005265235900879 | Test loss: 1.3783196210861206\n",
            "Epoch: 2720 | Train loss: 0.8982146382331848 | Test loss: 1.375612497329712\n",
            "Epoch: 2740 | Train loss: 0.8959026336669922 | Test loss: 1.3729054927825928\n",
            "Epoch: 2760 | Train loss: 0.8935907483100891 | Test loss: 1.370198369026184\n",
            "Epoch: 2780 | Train loss: 0.891278862953186 | Test loss: 1.3674911260604858\n",
            "Epoch: 2800 | Train loss: 0.8889668583869934 | Test loss: 1.3647841215133667\n",
            "Epoch: 2820 | Train loss: 0.8866550326347351 | Test loss: 1.3620771169662476\n",
            "Epoch: 2840 | Train loss: 0.8843430876731873 | Test loss: 1.3593698740005493\n",
            "Epoch: 2860 | Train loss: 0.8820310831069946 | Test loss: 1.356662631034851\n",
            "Epoch: 2880 | Train loss: 0.8797192573547363 | Test loss: 1.353955626487732\n",
            "Epoch: 2900 | Train loss: 0.8774072527885437 | Test loss: 1.3512486219406128\n",
            "Epoch: 2920 | Train loss: 0.8750953674316406 | Test loss: 1.348541498184204\n",
            "Epoch: 2940 | Train loss: 0.8727834820747375 | Test loss: 1.3458343744277954\n",
            "Epoch: 2960 | Train loss: 0.8704714775085449 | Test loss: 1.3431272506713867\n",
            "Epoch: 2980 | Train loss: 0.8681595921516418 | Test loss: 1.340420126914978\n",
            "Epoch: 3000 | Train loss: 0.8658475875854492 | Test loss: 1.3377130031585693\n",
            "Epoch: 3020 | Train loss: 0.8635357022285461 | Test loss: 1.3350059986114502\n",
            "Epoch: 3040 | Train loss: 0.8612238168716431 | Test loss: 1.3322988748550415\n",
            "Epoch: 3060 | Train loss: 0.85891193151474 | Test loss: 1.3295917510986328\n",
            "Epoch: 3080 | Train loss: 0.8565999865531921 | Test loss: 1.3268846273422241\n",
            "Epoch: 3100 | Train loss: 0.8542879223823547 | Test loss: 1.324177622795105\n",
            "Epoch: 3120 | Train loss: 0.8519760370254517 | Test loss: 1.3214704990386963\n",
            "Epoch: 3140 | Train loss: 0.8496641516685486 | Test loss: 1.318763256072998\n",
            "Epoch: 3160 | Train loss: 0.8473522067070007 | Test loss: 1.3160561323165894\n",
            "Epoch: 3180 | Train loss: 0.8450403213500977 | Test loss: 1.3133491277694702\n",
            "Epoch: 3200 | Train loss: 0.842728316783905 | Test loss: 1.3106420040130615\n",
            "Epoch: 3220 | Train loss: 0.840416431427002 | Test loss: 1.3079347610473633\n",
            "Epoch: 3240 | Train loss: 0.8381044268608093 | Test loss: 1.3052277565002441\n",
            "Epoch: 3260 | Train loss: 0.8357925415039062 | Test loss: 1.3025206327438354\n",
            "Epoch: 3280 | Train loss: 0.8334806561470032 | Test loss: 1.2998135089874268\n",
            "Epoch: 3300 | Train loss: 0.8311686515808105 | Test loss: 1.297106385231018\n",
            "Epoch: 3320 | Train loss: 0.8288567662239075 | Test loss: 1.2943992614746094\n",
            "Epoch: 3340 | Train loss: 0.8265448808670044 | Test loss: 1.2916922569274902\n",
            "Epoch: 3360 | Train loss: 0.8242329955101013 | Test loss: 1.2889851331710815\n",
            "Epoch: 3380 | Train loss: 0.8219209909439087 | Test loss: 1.2862780094146729\n",
            "Epoch: 3400 | Train loss: 0.8196091055870056 | Test loss: 1.2835708856582642\n",
            "Epoch: 3420 | Train loss: 0.8172971606254578 | Test loss: 1.2808637619018555\n",
            "Epoch: 3440 | Train loss: 0.8149852752685547 | Test loss: 1.2781566381454468\n",
            "Epoch: 3460 | Train loss: 0.8126732707023621 | Test loss: 1.2754496335983276\n",
            "Epoch: 3480 | Train loss: 0.810361385345459 | Test loss: 1.272742509841919\n",
            "Epoch: 3500 | Train loss: 0.8080493807792664 | Test loss: 1.2700353860855103\n",
            "Epoch: 3520 | Train loss: 0.8057374954223633 | Test loss: 1.2673282623291016\n",
            "Epoch: 3540 | Train loss: 0.8034256100654602 | Test loss: 1.2646211385726929\n",
            "Epoch: 3560 | Train loss: 0.8011137247085571 | Test loss: 1.2619141340255737\n",
            "Epoch: 3580 | Train loss: 0.7988017201423645 | Test loss: 1.259207010269165\n",
            "Epoch: 3600 | Train loss: 0.7964898347854614 | Test loss: 1.2564998865127563\n",
            "Epoch: 3620 | Train loss: 0.7941778898239136 | Test loss: 1.2537927627563477\n",
            "Epoch: 3640 | Train loss: 0.7918660044670105 | Test loss: 1.251085638999939\n",
            "Epoch: 3660 | Train loss: 0.7895540595054626 | Test loss: 1.2483785152435303\n",
            "Epoch: 3680 | Train loss: 0.7872421145439148 | Test loss: 1.2456715106964111\n",
            "Epoch: 3700 | Train loss: 0.7849301695823669 | Test loss: 1.2429643869400024\n",
            "Epoch: 3720 | Train loss: 0.7826182246208191 | Test loss: 1.2402572631835938\n",
            "Epoch: 3740 | Train loss: 0.780306339263916 | Test loss: 1.237550139427185\n",
            "Epoch: 3760 | Train loss: 0.7779943943023682 | Test loss: 1.234843134880066\n",
            "Epoch: 3780 | Train loss: 0.7756824493408203 | Test loss: 1.2321360111236572\n",
            "Epoch: 3800 | Train loss: 0.7733705639839172 | Test loss: 1.229428768157959\n",
            "Epoch: 3820 | Train loss: 0.7710586190223694 | Test loss: 1.2267216444015503\n",
            "Epoch: 3840 | Train loss: 0.7687467336654663 | Test loss: 1.2240146398544312\n",
            "Epoch: 3860 | Train loss: 0.7664347887039185 | Test loss: 1.2213075160980225\n",
            "Epoch: 3880 | Train loss: 0.7641228437423706 | Test loss: 1.2186002731323242\n",
            "Epoch: 3900 | Train loss: 0.7618109583854675 | Test loss: 1.2158931493759155\n",
            "Epoch: 3920 | Train loss: 0.7594990134239197 | Test loss: 1.2131861448287964\n",
            "Epoch: 3940 | Train loss: 0.7571867108345032 | Test loss: 1.210478663444519\n",
            "Epoch: 3960 | Train loss: 0.7548741698265076 | Test loss: 1.2077709436416626\n",
            "Epoch: 3980 | Train loss: 0.7525616884231567 | Test loss: 1.2050632238388062\n",
            "Epoch: 4000 | Train loss: 0.7502491474151611 | Test loss: 1.2023556232452393\n",
            "Epoch: 4020 | Train loss: 0.7479366660118103 | Test loss: 1.1996479034423828\n",
            "Epoch: 4040 | Train loss: 0.7456240653991699 | Test loss: 1.196940302848816\n",
            "Epoch: 4060 | Train loss: 0.7433115839958191 | Test loss: 1.1942323446273804\n",
            "Epoch: 4080 | Train loss: 0.7409990429878235 | Test loss: 1.1915247440338135\n",
            "Epoch: 4100 | Train loss: 0.7386865615844727 | Test loss: 1.188817024230957\n",
            "Epoch: 4120 | Train loss: 0.7363740801811218 | Test loss: 1.1861094236373901\n",
            "Epoch: 4140 | Train loss: 0.7340614795684814 | Test loss: 1.1834017038345337\n",
            "Epoch: 4160 | Train loss: 0.7317489981651306 | Test loss: 1.1806939840316772\n",
            "Epoch: 4180 | Train loss: 0.7294365167617798 | Test loss: 1.1779861450195312\n",
            "Epoch: 4200 | Train loss: 0.7271239161491394 | Test loss: 1.1752785444259644\n",
            "Epoch: 4220 | Train loss: 0.7248114347457886 | Test loss: 1.172570824623108\n",
            "Epoch: 4240 | Train loss: 0.722498893737793 | Test loss: 1.1698631048202515\n",
            "Epoch: 4260 | Train loss: 0.7201864123344421 | Test loss: 1.1671555042266846\n",
            "Epoch: 4280 | Train loss: 0.7178738713264465 | Test loss: 1.1644477844238281\n",
            "Epoch: 4300 | Train loss: 0.7155613303184509 | Test loss: 1.1617401838302612\n",
            "Epoch: 4320 | Train loss: 0.7132488489151001 | Test loss: 1.1590323448181152\n",
            "Epoch: 4340 | Train loss: 0.7109363079071045 | Test loss: 1.1563246250152588\n",
            "Epoch: 4360 | Train loss: 0.7086237668991089 | Test loss: 1.153617024421692\n",
            "Epoch: 4380 | Train loss: 0.7063112258911133 | Test loss: 1.1509093046188354\n",
            "Epoch: 4400 | Train loss: 0.7039987444877625 | Test loss: 1.148201584815979\n",
            "Epoch: 4420 | Train loss: 0.7016862034797668 | Test loss: 1.1454938650131226\n",
            "Epoch: 4440 | Train loss: 0.6993736624717712 | Test loss: 1.1427861452102661\n",
            "Epoch: 4460 | Train loss: 0.6970611810684204 | Test loss: 1.1400784254074097\n",
            "Epoch: 4480 | Train loss: 0.69474858045578 | Test loss: 1.1373707056045532\n",
            "Epoch: 4500 | Train loss: 0.6924360990524292 | Test loss: 1.1346629858016968\n",
            "Epoch: 4520 | Train loss: 0.6901235580444336 | Test loss: 1.1319553852081299\n",
            "Epoch: 4540 | Train loss: 0.687811017036438 | Test loss: 1.1292476654052734\n",
            "Epoch: 4560 | Train loss: 0.6854984760284424 | Test loss: 1.1265400648117065\n",
            "Epoch: 4580 | Train loss: 0.6831859946250916 | Test loss: 1.1238322257995605\n",
            "Epoch: 4600 | Train loss: 0.680873453617096 | Test loss: 1.121124505996704\n",
            "Epoch: 4620 | Train loss: 0.6785609722137451 | Test loss: 1.1184167861938477\n",
            "Epoch: 4640 | Train loss: 0.6762484312057495 | Test loss: 1.1157091856002808\n",
            "Epoch: 4660 | Train loss: 0.6739358901977539 | Test loss: 1.1130014657974243\n",
            "Epoch: 4680 | Train loss: 0.6716234087944031 | Test loss: 1.1102937459945679\n",
            "Epoch: 4700 | Train loss: 0.6693108677864075 | Test loss: 1.1075860261917114\n",
            "Epoch: 4720 | Train loss: 0.6669983267784119 | Test loss: 1.104878306388855\n",
            "Epoch: 4740 | Train loss: 0.664685845375061 | Test loss: 1.102170705795288\n",
            "Epoch: 4760 | Train loss: 0.6623733043670654 | Test loss: 1.099462866783142\n",
            "Epoch: 4780 | Train loss: 0.6600608229637146 | Test loss: 1.0967552661895752\n",
            "Epoch: 4800 | Train loss: 0.6577482223510742 | Test loss: 1.0940475463867188\n",
            "Epoch: 4820 | Train loss: 0.6554357409477234 | Test loss: 1.0913399457931519\n",
            "Epoch: 4840 | Train loss: 0.653123140335083 | Test loss: 1.0886321067810059\n",
            "Epoch: 4860 | Train loss: 0.6508106589317322 | Test loss: 1.0859243869781494\n",
            "Epoch: 4880 | Train loss: 0.6484981775283813 | Test loss: 1.083216667175293\n",
            "Epoch: 4900 | Train loss: 0.6461856365203857 | Test loss: 1.080509066581726\n",
            "Epoch: 4920 | Train loss: 0.6438731551170349 | Test loss: 1.0778013467788696\n",
            "Epoch: 4940 | Train loss: 0.6415606141090393 | Test loss: 1.0750936269760132\n",
            "Epoch: 4960 | Train loss: 0.6392480731010437 | Test loss: 1.0723857879638672\n",
            "Epoch: 4980 | Train loss: 0.6369355320930481 | Test loss: 1.0696781873703003\n",
            "Epoch: 5000 | Train loss: 0.6346229910850525 | Test loss: 1.0669704675674438\n",
            "Epoch: 5020 | Train loss: 0.6323105096817017 | Test loss: 1.0642627477645874\n",
            "Epoch: 5040 | Train loss: 0.629997968673706 | Test loss: 1.0615551471710205\n",
            "Epoch: 5060 | Train loss: 0.6276854872703552 | Test loss: 1.058847427368164\n",
            "Epoch: 5080 | Train loss: 0.6253728866577148 | Test loss: 1.0561398267745972\n",
            "Epoch: 5100 | Train loss: 0.623060405254364 | Test loss: 1.0534319877624512\n",
            "Epoch: 5120 | Train loss: 0.6207478642463684 | Test loss: 1.0507242679595947\n",
            "Epoch: 5140 | Train loss: 0.6184353232383728 | Test loss: 1.0480165481567383\n",
            "Epoch: 5160 | Train loss: 0.616122841835022 | Test loss: 1.0453089475631714\n",
            "Epoch: 5180 | Train loss: 0.6138103008270264 | Test loss: 1.0426011085510254\n",
            "Epoch: 5200 | Train loss: 0.6114978194236755 | Test loss: 1.0398935079574585\n",
            "Epoch: 5220 | Train loss: 0.6091852188110352 | Test loss: 1.037185788154602\n",
            "Epoch: 5240 | Train loss: 0.6068727374076843 | Test loss: 1.0344780683517456\n",
            "Epoch: 5260 | Train loss: 0.6045601963996887 | Test loss: 1.0317703485488892\n",
            "Epoch: 5280 | Train loss: 0.6022476553916931 | Test loss: 1.0290626287460327\n",
            "Epoch: 5300 | Train loss: 0.5999351739883423 | Test loss: 1.0263550281524658\n",
            "Epoch: 5320 | Train loss: 0.5976226329803467 | Test loss: 1.0236473083496094\n",
            "Epoch: 5340 | Train loss: 0.5953101515769958 | Test loss: 1.020939588546753\n",
            "Epoch: 5360 | Train loss: 0.5929975509643555 | Test loss: 1.0182318687438965\n",
            "Epoch: 5380 | Train loss: 0.5906850695610046 | Test loss: 1.01552414894104\n",
            "Epoch: 5400 | Train loss: 0.588372528553009 | Test loss: 1.0128164291381836\n",
            "Epoch: 5420 | Train loss: 0.5860600471496582 | Test loss: 1.0101088285446167\n",
            "Epoch: 5440 | Train loss: 0.5837475061416626 | Test loss: 1.0074011087417603\n",
            "Epoch: 5460 | Train loss: 0.581434965133667 | Test loss: 1.0046933889389038\n",
            "Epoch: 5480 | Train loss: 0.5791224837303162 | Test loss: 1.0019855499267578\n",
            "Epoch: 5500 | Train loss: 0.5768099427223206 | Test loss: 0.9992778897285461\n",
            "Epoch: 5520 | Train loss: 0.574497401714325 | Test loss: 0.9965702295303345\n",
            "Epoch: 5540 | Train loss: 0.5721848607063293 | Test loss: 0.9938625693321228\n",
            "Epoch: 5560 | Train loss: 0.5698723793029785 | Test loss: 0.9911548495292664\n",
            "Epoch: 5580 | Train loss: 0.5675598382949829 | Test loss: 0.9884471893310547\n",
            "Epoch: 5600 | Train loss: 0.5652472972869873 | Test loss: 0.9857394099235535\n",
            "Epoch: 5620 | Train loss: 0.5629348158836365 | Test loss: 0.9830317497253418\n",
            "Epoch: 5640 | Train loss: 0.5606222748756409 | Test loss: 0.9803239703178406\n",
            "Epoch: 5660 | Train loss: 0.5583097338676453 | Test loss: 0.9776163101196289\n",
            "Epoch: 5680 | Train loss: 0.5559971928596497 | Test loss: 0.9749086499214172\n",
            "Epoch: 5700 | Train loss: 0.5536847114562988 | Test loss: 0.9722009897232056\n",
            "Epoch: 5720 | Train loss: 0.5513721704483032 | Test loss: 0.9694932103157043\n",
            "Epoch: 5740 | Train loss: 0.5490596294403076 | Test loss: 0.9667855501174927\n",
            "Epoch: 5760 | Train loss: 0.5467471480369568 | Test loss: 0.964077889919281\n",
            "Epoch: 5780 | Train loss: 0.5444346070289612 | Test loss: 0.9613701105117798\n",
            "Epoch: 5800 | Train loss: 0.5421220660209656 | Test loss: 0.9586624503135681\n",
            "Epoch: 5820 | Train loss: 0.5398095846176147 | Test loss: 0.9559547305107117\n",
            "Epoch: 5840 | Train loss: 0.5374970436096191 | Test loss: 0.9532470703125\n",
            "Epoch: 5860 | Train loss: 0.5351845622062683 | Test loss: 0.9505392909049988\n",
            "Epoch: 5880 | Train loss: 0.5328720211982727 | Test loss: 0.9478316307067871\n",
            "Epoch: 5900 | Train loss: 0.5305594801902771 | Test loss: 0.9451239705085754\n",
            "Epoch: 5920 | Train loss: 0.5282469987869263 | Test loss: 0.9424161911010742\n",
            "Epoch: 5940 | Train loss: 0.5259343981742859 | Test loss: 0.9397085309028625\n",
            "Epoch: 5960 | Train loss: 0.5236219167709351 | Test loss: 0.9370007514953613\n",
            "Epoch: 5980 | Train loss: 0.5213093757629395 | Test loss: 0.9342930912971497\n",
            "Epoch: 6000 | Train loss: 0.5189968347549438 | Test loss: 0.931585431098938\n",
            "Epoch: 6020 | Train loss: 0.516684353351593 | Test loss: 0.9288776516914368\n",
            "Epoch: 6040 | Train loss: 0.5143718123435974 | Test loss: 0.9261699914932251\n",
            "Epoch: 6060 | Train loss: 0.5120593309402466 | Test loss: 0.9234623312950134\n",
            "Epoch: 6080 | Train loss: 0.5097467303276062 | Test loss: 0.920754611492157\n",
            "Epoch: 6100 | Train loss: 0.5074343085289001 | Test loss: 0.9180468916893005\n",
            "Epoch: 6120 | Train loss: 0.5051217079162598 | Test loss: 0.9153391718864441\n",
            "Epoch: 6140 | Train loss: 0.5028092265129089 | Test loss: 0.9126315116882324\n",
            "Epoch: 6160 | Train loss: 0.5004966259002686 | Test loss: 0.9099237322807312\n",
            "Epoch: 6180 | Train loss: 0.49818411469459534 | Test loss: 0.9072160720825195\n",
            "Epoch: 6200 | Train loss: 0.4958716332912445 | Test loss: 0.9045082926750183\n",
            "Epoch: 6220 | Train loss: 0.4935591220855713 | Test loss: 0.9018007516860962\n",
            "Epoch: 6240 | Train loss: 0.4912465512752533 | Test loss: 0.899092972278595\n",
            "Epoch: 6260 | Train loss: 0.4889340400695801 | Test loss: 0.8963853120803833\n",
            "Epoch: 6280 | Train loss: 0.48662152886390686 | Test loss: 0.8936775326728821\n",
            "Epoch: 6300 | Train loss: 0.48430901765823364 | Test loss: 0.8909698724746704\n",
            "Epoch: 6320 | Train loss: 0.48199644684791565 | Test loss: 0.8882622122764587\n",
            "Epoch: 6340 | Train loss: 0.4796839654445648 | Test loss: 0.8855544328689575\n",
            "Epoch: 6360 | Train loss: 0.4773714542388916 | Test loss: 0.8828467726707458\n",
            "Epoch: 6380 | Train loss: 0.4750589430332184 | Test loss: 0.8801390528678894\n",
            "Epoch: 6400 | Train loss: 0.4727463722229004 | Test loss: 0.8774313926696777\n",
            "Epoch: 6420 | Train loss: 0.4704338610172272 | Test loss: 0.8747236132621765\n",
            "Epoch: 6440 | Train loss: 0.46812134981155396 | Test loss: 0.8720159530639648\n",
            "Epoch: 6460 | Train loss: 0.46580877900123596 | Test loss: 0.8693082928657532\n",
            "Epoch: 6480 | Train loss: 0.46349629759788513 | Test loss: 0.866600513458252\n",
            "Epoch: 6500 | Train loss: 0.4611837863922119 | Test loss: 0.8638928532600403\n",
            "Epoch: 6520 | Train loss: 0.4588712751865387 | Test loss: 0.8611851930618286\n",
            "Epoch: 6540 | Train loss: 0.4565587043762207 | Test loss: 0.8584774136543274\n",
            "Epoch: 6560 | Train loss: 0.4542461931705475 | Test loss: 0.8557697534561157\n",
            "Epoch: 6580 | Train loss: 0.45193368196487427 | Test loss: 0.853062093257904\n",
            "Epoch: 6600 | Train loss: 0.44962117075920105 | Test loss: 0.8503543138504028\n",
            "Epoch: 6620 | Train loss: 0.44730862975120544 | Test loss: 0.8476466536521912\n",
            "Epoch: 6640 | Train loss: 0.4449961185455322 | Test loss: 0.8449388742446899\n",
            "Epoch: 6660 | Train loss: 0.44268378615379333 | Test loss: 0.8422316908836365\n",
            "Epoch: 6680 | Train loss: 0.4403715133666992 | Test loss: 0.8395245671272278\n",
            "Epoch: 6700 | Train loss: 0.4380592405796051 | Test loss: 0.8368173837661743\n",
            "Epoch: 6720 | Train loss: 0.4357469081878662 | Test loss: 0.8341102600097656\n",
            "Epoch: 6740 | Train loss: 0.4334346354007721 | Test loss: 0.8314029574394226\n",
            "Epoch: 6760 | Train loss: 0.431122362613678 | Test loss: 0.8286958932876587\n",
            "Epoch: 6780 | Train loss: 0.42881008982658386 | Test loss: 0.8259887099266052\n",
            "Epoch: 6800 | Train loss: 0.42649775743484497 | Test loss: 0.8232815861701965\n",
            "Epoch: 6820 | Train loss: 0.42418548464775085 | Test loss: 0.8205744028091431\n",
            "Epoch: 6840 | Train loss: 0.42187318205833435 | Test loss: 0.8178672194480896\n",
            "Epoch: 6860 | Train loss: 0.41956090927124023 | Test loss: 0.8151599764823914\n",
            "Epoch: 6880 | Train loss: 0.4172486364841461 | Test loss: 0.8124529123306274\n",
            "Epoch: 6900 | Train loss: 0.414936363697052 | Test loss: 0.809745728969574\n",
            "Epoch: 6920 | Train loss: 0.4126240909099579 | Test loss: 0.8070384860038757\n",
            "Epoch: 6940 | Train loss: 0.410311758518219 | Test loss: 0.8043314218521118\n",
            "Epoch: 6960 | Train loss: 0.4079994261264801 | Test loss: 0.8016242384910583\n",
            "Epoch: 6980 | Train loss: 0.405687153339386 | Test loss: 0.7989169955253601\n",
            "Epoch: 7000 | Train loss: 0.40337491035461426 | Test loss: 0.7962098717689514\n",
            "Epoch: 7020 | Train loss: 0.40106257796287537 | Test loss: 0.793502688407898\n",
            "Epoch: 7040 | Train loss: 0.39875030517578125 | Test loss: 0.7907955050468445\n",
            "Epoch: 7060 | Train loss: 0.39643803238868713 | Test loss: 0.788088321685791\n",
            "Epoch: 7080 | Train loss: 0.394125759601593 | Test loss: 0.7853811979293823\n",
            "Epoch: 7100 | Train loss: 0.3918134868144989 | Test loss: 0.7826740145683289\n",
            "Epoch: 7120 | Train loss: 0.3895011842250824 | Test loss: 0.7799668312072754\n",
            "Epoch: 7140 | Train loss: 0.3871888816356659 | Test loss: 0.7772596478462219\n",
            "Epoch: 7160 | Train loss: 0.3848765790462494 | Test loss: 0.7745525240898132\n",
            "Epoch: 7180 | Train loss: 0.3825643062591553 | Test loss: 0.7718453407287598\n",
            "Epoch: 7200 | Train loss: 0.38025203347206116 | Test loss: 0.7691381573677063\n",
            "Epoch: 7220 | Train loss: 0.37793973088264465 | Test loss: 0.7664310336112976\n",
            "Epoch: 7240 | Train loss: 0.37562745809555054 | Test loss: 0.7637238502502441\n",
            "Epoch: 7260 | Train loss: 0.3733151853084564 | Test loss: 0.7610166668891907\n",
            "Epoch: 7280 | Train loss: 0.37100285291671753 | Test loss: 0.7583094835281372\n",
            "Epoch: 7300 | Train loss: 0.3686905801296234 | Test loss: 0.7556023001670837\n",
            "Epoch: 7320 | Train loss: 0.3663783073425293 | Test loss: 0.752895176410675\n",
            "Epoch: 7340 | Train loss: 0.3640660047531128 | Test loss: 0.7501880526542664\n",
            "Epoch: 7360 | Train loss: 0.3617537021636963 | Test loss: 0.7474808096885681\n",
            "Epoch: 7380 | Train loss: 0.3594414293766022 | Test loss: 0.7447736263275146\n",
            "Epoch: 7400 | Train loss: 0.35712915658950806 | Test loss: 0.742066502571106\n",
            "Epoch: 7420 | Train loss: 0.35481688380241394 | Test loss: 0.7393593192100525\n",
            "Epoch: 7440 | Train loss: 0.35250458121299744 | Test loss: 0.7366521954536438\n",
            "Epoch: 7460 | Train loss: 0.35019227862358093 | Test loss: 0.7339450120925903\n",
            "Epoch: 7480 | Train loss: 0.34787997603416443 | Test loss: 0.7312378287315369\n",
            "Epoch: 7500 | Train loss: 0.3455677032470703 | Test loss: 0.7285306453704834\n",
            "Epoch: 7520 | Train loss: 0.3432554006576538 | Test loss: 0.7258235216140747\n",
            "Epoch: 7540 | Train loss: 0.3409431278705597 | Test loss: 0.7231163382530212\n",
            "Epoch: 7560 | Train loss: 0.3386308550834656 | Test loss: 0.720409095287323\n",
            "Epoch: 7580 | Train loss: 0.3363185524940491 | Test loss: 0.7177019715309143\n",
            "Epoch: 7600 | Train loss: 0.33400624990463257 | Test loss: 0.7149948477745056\n",
            "Epoch: 7620 | Train loss: 0.33169397711753845 | Test loss: 0.7122876048088074\n",
            "Epoch: 7640 | Train loss: 0.32938170433044434 | Test loss: 0.7095804810523987\n",
            "Epoch: 7660 | Train loss: 0.32706940174102783 | Test loss: 0.7068732976913452\n",
            "Epoch: 7680 | Train loss: 0.32475709915161133 | Test loss: 0.7041661143302917\n",
            "Epoch: 7700 | Train loss: 0.3224448263645172 | Test loss: 0.7014589309692383\n",
            "Epoch: 7720 | Train loss: 0.3201325535774231 | Test loss: 0.6987518072128296\n",
            "Epoch: 7740 | Train loss: 0.317820280790329 | Test loss: 0.6960446238517761\n",
            "Epoch: 7760 | Train loss: 0.3155079782009125 | Test loss: 0.6933374404907227\n",
            "Epoch: 7780 | Train loss: 0.31319567561149597 | Test loss: 0.6906303763389587\n",
            "Epoch: 7800 | Train loss: 0.31088337302207947 | Test loss: 0.6879231333732605\n",
            "Epoch: 7820 | Train loss: 0.30857110023498535 | Test loss: 0.685215950012207\n",
            "Epoch: 7840 | Train loss: 0.30625879764556885 | Test loss: 0.6825088262557983\n",
            "Epoch: 7860 | Train loss: 0.30394652485847473 | Test loss: 0.6798016428947449\n",
            "Epoch: 7880 | Train loss: 0.3016342520713806 | Test loss: 0.6770944595336914\n",
            "Epoch: 7900 | Train loss: 0.2993219494819641 | Test loss: 0.6743872761726379\n",
            "Epoch: 7920 | Train loss: 0.29700967669487 | Test loss: 0.6716800928115845\n",
            "Epoch: 7940 | Train loss: 0.2947489619255066 | Test loss: 0.669000506401062\n",
            "Epoch: 7960 | Train loss: 0.2925355136394501 | Test loss: 0.6663433313369751\n",
            "Epoch: 7980 | Train loss: 0.29032203555107117 | Test loss: 0.6636862754821777\n",
            "Epoch: 8000 | Train loss: 0.28810855746269226 | Test loss: 0.6610292196273804\n",
            "Epoch: 8020 | Train loss: 0.2859635651111603 | Test loss: 0.658409595489502\n",
            "Epoch: 8040 | Train loss: 0.2838466465473175 | Test loss: 0.6558026075363159\n",
            "Epoch: 8060 | Train loss: 0.28172966837882996 | Test loss: 0.6531955599784851\n",
            "Epoch: 8080 | Train loss: 0.2796127498149872 | Test loss: 0.6505885124206543\n",
            "Epoch: 8100 | Train loss: 0.2775726914405823 | Test loss: 0.6480249762535095\n",
            "Epoch: 8120 | Train loss: 0.2755504548549652 | Test loss: 0.6454691290855408\n",
            "Epoch: 8140 | Train loss: 0.27352815866470337 | Test loss: 0.6429131627082825\n",
            "Epoch: 8160 | Train loss: 0.2715058922767639 | Test loss: 0.640357255935669\n",
            "Epoch: 8180 | Train loss: 0.2695627808570862 | Test loss: 0.637847900390625\n",
            "Epoch: 8200 | Train loss: 0.2676331102848053 | Test loss: 0.6353436708450317\n",
            "Epoch: 8220 | Train loss: 0.2657034397125244 | Test loss: 0.6328394412994385\n",
            "Epoch: 8240 | Train loss: 0.2637737989425659 | Test loss: 0.63033527135849\n",
            "Epoch: 8260 | Train loss: 0.26191726326942444 | Test loss: 0.6278748512268066\n",
            "Epoch: 8280 | Train loss: 0.26007792353630066 | Test loss: 0.6254222989082336\n",
            "Epoch: 8300 | Train loss: 0.2582385241985321 | Test loss: 0.6229697465896606\n",
            "Epoch: 8320 | Train loss: 0.2563991844654083 | Test loss: 0.6205171942710876\n",
            "Epoch: 8340 | Train loss: 0.25462159514427185 | Test loss: 0.6181037425994873\n",
            "Epoch: 8360 | Train loss: 0.25287050008773804 | Test loss: 0.6157034039497375\n",
            "Epoch: 8380 | Train loss: 0.25111934542655945 | Test loss: 0.6133030652999878\n",
            "Epoch: 8400 | Train loss: 0.24936822056770325 | Test loss: 0.610902726650238\n",
            "Epoch: 8420 | Train loss: 0.24766020476818085 | Test loss: 0.6085312962532043\n",
            "Epoch: 8440 | Train loss: 0.24599520862102509 | Test loss: 0.6061837077140808\n",
            "Epoch: 8460 | Train loss: 0.24433021247386932 | Test loss: 0.6038360595703125\n",
            "Epoch: 8480 | Train loss: 0.24266524612903595 | Test loss: 0.6014885306358337\n",
            "Epoch: 8500 | Train loss: 0.24101772904396057 | Test loss: 0.5991541743278503\n",
            "Epoch: 8520 | Train loss: 0.2394367903470993 | Test loss: 0.596859872341156\n",
            "Epoch: 8540 | Train loss: 0.23785586655139923 | Test loss: 0.5945654511451721\n",
            "Epoch: 8560 | Train loss: 0.23627491295337677 | Test loss: 0.592271089553833\n",
            "Epoch: 8580 | Train loss: 0.2346940040588379 | Test loss: 0.5899767279624939\n",
            "Epoch: 8600 | Train loss: 0.23318038880825043 | Test loss: 0.5877281427383423\n",
            "Epoch: 8620 | Train loss: 0.23168139159679413 | Test loss: 0.5854875445365906\n",
            "Epoch: 8640 | Train loss: 0.23018240928649902 | Test loss: 0.5832470059394836\n",
            "Epoch: 8660 | Train loss: 0.22868342697620392 | Test loss: 0.5810064673423767\n",
            "Epoch: 8680 | Train loss: 0.22721333801746368 | Test loss: 0.5787874460220337\n",
            "Epoch: 8700 | Train loss: 0.2257939875125885 | Test loss: 0.5766006708145142\n",
            "Epoch: 8720 | Train loss: 0.22437463700771332 | Test loss: 0.5744138956069946\n",
            "Epoch: 8740 | Train loss: 0.22295527160167694 | Test loss: 0.5722271800041199\n",
            "Epoch: 8760 | Train loss: 0.22153592109680176 | Test loss: 0.5700404047966003\n",
            "Epoch: 8780 | Train loss: 0.22017908096313477 | Test loss: 0.5679003000259399\n",
            "Epoch: 8800 | Train loss: 0.21883749961853027 | Test loss: 0.5657684206962585\n",
            "Epoch: 8820 | Train loss: 0.2174959033727646 | Test loss: 0.5636364817619324\n",
            "Epoch: 8840 | Train loss: 0.2161542922258377 | Test loss: 0.5615046620368958\n",
            "Epoch: 8860 | Train loss: 0.21482157707214355 | Test loss: 0.5593810081481934\n",
            "Epoch: 8880 | Train loss: 0.21355538070201874 | Test loss: 0.5573040246963501\n",
            "Epoch: 8900 | Train loss: 0.21228919923305511 | Test loss: 0.5552269816398621\n",
            "Epoch: 8920 | Train loss: 0.2110230177640915 | Test loss: 0.553149938583374\n",
            "Epoch: 8940 | Train loss: 0.20975685119628906 | Test loss: 0.5510730147361755\n",
            "Epoch: 8960 | Train loss: 0.20851635932922363 | Test loss: 0.5490180850028992\n",
            "Epoch: 8980 | Train loss: 0.20732346177101135 | Test loss: 0.546996533870697\n",
            "Epoch: 9000 | Train loss: 0.20613057911396027 | Test loss: 0.5449748635292053\n",
            "Epoch: 9020 | Train loss: 0.20493769645690918 | Test loss: 0.5429532527923584\n",
            "Epoch: 9040 | Train loss: 0.2037447988986969 | Test loss: 0.5409316420555115\n",
            "Epoch: 9060 | Train loss: 0.20258180797100067 | Test loss: 0.5389347076416016\n",
            "Epoch: 9080 | Train loss: 0.20145931839942932 | Test loss: 0.5369677543640137\n",
            "Epoch: 9100 | Train loss: 0.20033679902553558 | Test loss: 0.5350009202957153\n",
            "Epoch: 9120 | Train loss: 0.19921432435512543 | Test loss: 0.533034086227417\n",
            "Epoch: 9140 | Train loss: 0.19809181988239288 | Test loss: 0.5310672521591187\n",
            "Epoch: 9160 | Train loss: 0.19699783623218536 | Test loss: 0.5291257500648499\n",
            "Epoch: 9180 | Train loss: 0.19594444334506989 | Test loss: 0.5272154808044434\n",
            "Epoch: 9200 | Train loss: 0.19489173591136932 | Test loss: 0.525306224822998\n",
            "Epoch: 9220 | Train loss: 0.19383901357650757 | Test loss: 0.5233970880508423\n",
            "Epoch: 9240 | Train loss: 0.1927863210439682 | Test loss: 0.521487832069397\n",
            "Epoch: 9260 | Train loss: 0.19175080955028534 | Test loss: 0.5195956230163574\n",
            "Epoch: 9280 | Train loss: 0.19076475501060486 | Test loss: 0.5177428126335144\n",
            "Epoch: 9300 | Train loss: 0.18977871537208557 | Test loss: 0.5158901214599609\n",
            "Epoch: 9320 | Train loss: 0.1887926310300827 | Test loss: 0.5140373706817627\n",
            "Epoch: 9340 | Train loss: 0.18780659139156342 | Test loss: 0.5121846199035645\n",
            "Epoch: 9360 | Train loss: 0.18682055175304413 | Test loss: 0.510331928730011\n",
            "Epoch: 9380 | Train loss: 0.18589630722999573 | Test loss: 0.5085367560386658\n",
            "Epoch: 9400 | Train loss: 0.18497495353221893 | Test loss: 0.5067415237426758\n",
            "Epoch: 9420 | Train loss: 0.18405359983444214 | Test loss: 0.5049463510513306\n",
            "Epoch: 9440 | Train loss: 0.18313224613666534 | Test loss: 0.5031511187553406\n",
            "Epoch: 9460 | Train loss: 0.18221090734004974 | Test loss: 0.5013558864593506\n",
            "Epoch: 9480 | Train loss: 0.18132077157497406 | Test loss: 0.4995923638343811\n",
            "Epoch: 9500 | Train loss: 0.18046171963214874 | Test loss: 0.49785470962524414\n",
            "Epoch: 9520 | Train loss: 0.17960266768932343 | Test loss: 0.4961170256137848\n",
            "Epoch: 9540 | Train loss: 0.1787436455488205 | Test loss: 0.4943793714046478\n",
            "Epoch: 9560 | Train loss: 0.17788460850715637 | Test loss: 0.49264174699783325\n",
            "Epoch: 9580 | Train loss: 0.17702555656433105 | Test loss: 0.4909040629863739\n",
            "Epoch: 9600 | Train loss: 0.17621846497058868 | Test loss: 0.4892186224460602\n",
            "Epoch: 9620 | Train loss: 0.17541955411434174 | Test loss: 0.48753905296325684\n",
            "Epoch: 9640 | Train loss: 0.1746206432580948 | Test loss: 0.4858594536781311\n",
            "Epoch: 9660 | Train loss: 0.17382171750068665 | Test loss: 0.48417988419532776\n",
            "Epoch: 9680 | Train loss: 0.1730227917432785 | Test loss: 0.482500284910202\n",
            "Epoch: 9700 | Train loss: 0.17222756147384644 | Test loss: 0.48082658648490906\n",
            "Epoch: 9720 | Train loss: 0.17148655652999878 | Test loss: 0.47920557856559753\n",
            "Epoch: 9740 | Train loss: 0.17074556648731232 | Test loss: 0.477584570646286\n",
            "Epoch: 9760 | Train loss: 0.17000457644462585 | Test loss: 0.4759635925292969\n",
            "Epoch: 9780 | Train loss: 0.1692635864019394 | Test loss: 0.47434258460998535\n",
            "Epoch: 9800 | Train loss: 0.16852258145809174 | Test loss: 0.47272157669067383\n",
            "Epoch: 9820 | Train loss: 0.16778527200222015 | Test loss: 0.47110649943351746\n",
            "Epoch: 9840 | Train loss: 0.1670999825000763 | Test loss: 0.4695446491241455\n",
            "Epoch: 9860 | Train loss: 0.16641466319561005 | Test loss: 0.46798282861709595\n",
            "Epoch: 9880 | Train loss: 0.16572938859462738 | Test loss: 0.4664209485054016\n",
            "Epoch: 9900 | Train loss: 0.16504408419132233 | Test loss: 0.46485909819602966\n",
            "Epoch: 9920 | Train loss: 0.16435880959033966 | Test loss: 0.4632972180843353\n",
            "Epoch: 9940 | Train loss: 0.1636734902858734 | Test loss: 0.461735337972641\n",
            "Epoch: 9960 | Train loss: 0.16303420066833496 | Test loss: 0.46022677421569824\n",
            "Epoch: 9980 | Train loss: 0.1624021828174591 | Test loss: 0.4587240219116211\n",
            "Epoch: 10000 | Train loss: 0.16177015006542206 | Test loss: 0.4572213292121887\n",
            "Epoch: 10020 | Train loss: 0.161138117313385 | Test loss: 0.45571860671043396\n",
            "Epoch: 10040 | Train loss: 0.16050609946250916 | Test loss: 0.4542159140110016\n",
            "Epoch: 10060 | Train loss: 0.1598740667104721 | Test loss: 0.4527132213115692\n",
            "Epoch: 10080 | Train loss: 0.1592632532119751 | Test loss: 0.4512364864349365\n",
            "Epoch: 10100 | Train loss: 0.15868143737316132 | Test loss: 0.4497915804386139\n",
            "Epoch: 10120 | Train loss: 0.15809960663318634 | Test loss: 0.44834667444229126\n",
            "Epoch: 10140 | Train loss: 0.15751777589321136 | Test loss: 0.446901798248291\n",
            "Epoch: 10160 | Train loss: 0.15693597495555878 | Test loss: 0.4454568922519684\n",
            "Epoch: 10180 | Train loss: 0.1563541442155838 | Test loss: 0.44401198625564575\n",
            "Epoch: 10200 | Train loss: 0.15577231347560883 | Test loss: 0.4425671100616455\n",
            "Epoch: 10220 | Train loss: 0.15522773563861847 | Test loss: 0.4411703646183014\n",
            "Epoch: 10240 | Train loss: 0.15469472110271454 | Test loss: 0.4397856891155243\n",
            "Epoch: 10260 | Train loss: 0.1541617065668106 | Test loss: 0.4384009838104248\n",
            "Epoch: 10280 | Train loss: 0.15362870693206787 | Test loss: 0.4370163083076477\n",
            "Epoch: 10300 | Train loss: 0.15309567749500275 | Test loss: 0.43563157320022583\n",
            "Epoch: 10320 | Train loss: 0.15256267786026 | Test loss: 0.4342469274997711\n",
            "Epoch: 10340 | Train loss: 0.15202966332435608 | Test loss: 0.43286219239234924\n",
            "Epoch: 10360 | Train loss: 0.15153460204601288 | Test loss: 0.43152913451194763\n",
            "Epoch: 10380 | Train loss: 0.15104812383651733 | Test loss: 0.43020516633987427\n",
            "Epoch: 10400 | Train loss: 0.1505616456270218 | Test loss: 0.4288812279701233\n",
            "Epoch: 10420 | Train loss: 0.15007516741752625 | Test loss: 0.4275572896003723\n",
            "Epoch: 10440 | Train loss: 0.1495886892080307 | Test loss: 0.42623329162597656\n",
            "Epoch: 10460 | Train loss: 0.14910219609737396 | Test loss: 0.4249093532562256\n",
            "Epoch: 10480 | Train loss: 0.14861571788787842 | Test loss: 0.4235854148864746\n",
            "Epoch: 10500 | Train loss: 0.14815427362918854 | Test loss: 0.4222981929779053\n",
            "Epoch: 10520 | Train loss: 0.14771202206611633 | Test loss: 0.421035498380661\n",
            "Epoch: 10540 | Train loss: 0.14726975560188293 | Test loss: 0.41977283358573914\n",
            "Epoch: 10560 | Train loss: 0.14682750403881073 | Test loss: 0.41851016879081726\n",
            "Epoch: 10580 | Train loss: 0.14638526737689972 | Test loss: 0.4172475039958954\n",
            "Epoch: 10600 | Train loss: 0.14594300091266632 | Test loss: 0.41598477959632874\n",
            "Epoch: 10620 | Train loss: 0.14550073444843292 | Test loss: 0.4147220551967621\n",
            "Epoch: 10640 | Train loss: 0.1450584977865219 | Test loss: 0.4134593904018402\n",
            "Epoch: 10660 | Train loss: 0.1446569710969925 | Test loss: 0.4122585356235504\n",
            "Epoch: 10680 | Train loss: 0.144256591796875 | Test loss: 0.4110576808452606\n",
            "Epoch: 10700 | Train loss: 0.14385627210140228 | Test loss: 0.40985679626464844\n",
            "Epoch: 10720 | Train loss: 0.14345590770244598 | Test loss: 0.40865588188171387\n",
            "Epoch: 10740 | Train loss: 0.14305555820465088 | Test loss: 0.4074550271034241\n",
            "Epoch: 10760 | Train loss: 0.14265520870685577 | Test loss: 0.4062541127204895\n",
            "Epoch: 10780 | Train loss: 0.14225487411022186 | Test loss: 0.4050532281398773\n",
            "Epoch: 10800 | Train loss: 0.14185497164726257 | Test loss: 0.40385547280311584\n",
            "Epoch: 10820 | Train loss: 0.14149418473243713 | Test loss: 0.4027169346809387\n",
            "Epoch: 10840 | Train loss: 0.1411333978176117 | Test loss: 0.4015783965587616\n",
            "Epoch: 10860 | Train loss: 0.14077261090278625 | Test loss: 0.4004398286342621\n",
            "Epoch: 10880 | Train loss: 0.14041180908679962 | Test loss: 0.39930129051208496\n",
            "Epoch: 10900 | Train loss: 0.14005102217197418 | Test loss: 0.39816275238990784\n",
            "Epoch: 10920 | Train loss: 0.13969023525714874 | Test loss: 0.3970242142677307\n",
            "Epoch: 10940 | Train loss: 0.1393294483423233 | Test loss: 0.3958856761455536\n",
            "Epoch: 10960 | Train loss: 0.13896866142749786 | Test loss: 0.3947471082210541\n",
            "Epoch: 10980 | Train loss: 0.1386304646730423 | Test loss: 0.3936491012573242\n",
            "Epoch: 11000 | Train loss: 0.13830667734146118 | Test loss: 0.39257287979125977\n",
            "Epoch: 11020 | Train loss: 0.13798291981220245 | Test loss: 0.3914966881275177\n",
            "Epoch: 11040 | Train loss: 0.13765914738178253 | Test loss: 0.39042049646377563\n",
            "Epoch: 11060 | Train loss: 0.1373353749513626 | Test loss: 0.3893442749977112\n",
            "Epoch: 11080 | Train loss: 0.1370116025209427 | Test loss: 0.3882680833339691\n",
            "Epoch: 11100 | Train loss: 0.13668783009052277 | Test loss: 0.38719186186790466\n",
            "Epoch: 11120 | Train loss: 0.13636405766010284 | Test loss: 0.3861156404018402\n",
            "Epoch: 11140 | Train loss: 0.13604028522968292 | Test loss: 0.38503944873809814\n",
            "Epoch: 11160 | Train loss: 0.13574138283729553 | Test loss: 0.38401079177856445\n",
            "Epoch: 11180 | Train loss: 0.13545241951942444 | Test loss: 0.382997989654541\n",
            "Epoch: 11200 | Train loss: 0.13516345620155334 | Test loss: 0.3819851875305176\n",
            "Epoch: 11220 | Train loss: 0.13487449288368225 | Test loss: 0.38097238540649414\n",
            "Epoch: 11240 | Train loss: 0.13458552956581116 | Test loss: 0.3799595832824707\n",
            "Epoch: 11260 | Train loss: 0.13429656624794006 | Test loss: 0.37894678115844727\n",
            "Epoch: 11280 | Train loss: 0.13400760293006897 | Test loss: 0.37793397903442383\n",
            "Epoch: 11300 | Train loss: 0.13371863961219788 | Test loss: 0.3769211769104004\n",
            "Epoch: 11320 | Train loss: 0.13342967629432678 | Test loss: 0.37590837478637695\n",
            "Epoch: 11340 | Train loss: 0.13314908742904663 | Test loss: 0.37491461634635925\n",
            "Epoch: 11360 | Train loss: 0.13289235532283783 | Test loss: 0.37396520376205444\n",
            "Epoch: 11380 | Train loss: 0.13263562321662903 | Test loss: 0.373015820980072\n",
            "Epoch: 11400 | Train loss: 0.13237889111042023 | Test loss: 0.3720664083957672\n",
            "Epoch: 11420 | Train loss: 0.13212214410305023 | Test loss: 0.3711170256137848\n",
            "Epoch: 11440 | Train loss: 0.13186541199684143 | Test loss: 0.3701675832271576\n",
            "Epoch: 11460 | Train loss: 0.13160867989063263 | Test loss: 0.36921823024749756\n",
            "Epoch: 11480 | Train loss: 0.13135193288326263 | Test loss: 0.36826881766319275\n",
            "Epoch: 11500 | Train loss: 0.13109521567821503 | Test loss: 0.36731940507888794\n",
            "Epoch: 11520 | Train loss: 0.13083846867084503 | Test loss: 0.3663700222969055\n",
            "Epoch: 11540 | Train loss: 0.13058629631996155 | Test loss: 0.36543339490890503\n",
            "Epoch: 11560 | Train loss: 0.13035935163497925 | Test loss: 0.3645479381084442\n",
            "Epoch: 11580 | Train loss: 0.13013242185115814 | Test loss: 0.3636624813079834\n",
            "Epoch: 11600 | Train loss: 0.12990547716617584 | Test loss: 0.3627770245075226\n",
            "Epoch: 11620 | Train loss: 0.12967853248119354 | Test loss: 0.36189156770706177\n",
            "Epoch: 11640 | Train loss: 0.12945158779621124 | Test loss: 0.36100608110427856\n",
            "Epoch: 11660 | Train loss: 0.12922465801239014 | Test loss: 0.36012062430381775\n",
            "Epoch: 11680 | Train loss: 0.12899772822856903 | Test loss: 0.3592351973056793\n",
            "Epoch: 11700 | Train loss: 0.12877078354358673 | Test loss: 0.3583497107028961\n",
            "Epoch: 11720 | Train loss: 0.12854383885860443 | Test loss: 0.3574642539024353\n",
            "Epoch: 11740 | Train loss: 0.12831690907478333 | Test loss: 0.3565788269042969\n",
            "Epoch: 11760 | Train loss: 0.1280970424413681 | Test loss: 0.3557126820087433\n",
            "Epoch: 11780 | Train loss: 0.12789742648601532 | Test loss: 0.35489168763160706\n",
            "Epoch: 11800 | Train loss: 0.12769785523414612 | Test loss: 0.3540707230567932\n",
            "Epoch: 11820 | Train loss: 0.12749825417995453 | Test loss: 0.353249728679657\n",
            "Epoch: 11840 | Train loss: 0.12729863822460175 | Test loss: 0.35242873430252075\n",
            "Epoch: 11860 | Train loss: 0.12709906697273254 | Test loss: 0.3516077399253845\n",
            "Epoch: 11880 | Train loss: 0.12689946591854095 | Test loss: 0.3507867455482483\n",
            "Epoch: 11900 | Train loss: 0.12669984996318817 | Test loss: 0.34996578097343445\n",
            "Epoch: 11920 | Train loss: 0.12650027871131897 | Test loss: 0.3491447865962982\n",
            "Epoch: 11940 | Train loss: 0.12630067765712738 | Test loss: 0.348323792219162\n",
            "Epoch: 11960 | Train loss: 0.1261010617017746 | Test loss: 0.34750279784202576\n",
            "Epoch: 11980 | Train loss: 0.1259014904499054 | Test loss: 0.3466818332672119\n",
            "Epoch: 12000 | Train loss: 0.1257130354642868 | Test loss: 0.34589308500289917\n",
            "Epoch: 12020 | Train loss: 0.1255381554365158 | Test loss: 0.34513652324676514\n",
            "Epoch: 12040 | Train loss: 0.12536326050758362 | Test loss: 0.3443800210952759\n",
            "Epoch: 12060 | Train loss: 0.12518838047981262 | Test loss: 0.3436235189437866\n",
            "Epoch: 12080 | Train loss: 0.12501350045204163 | Test loss: 0.342866986989975\n",
            "Epoch: 12100 | Train loss: 0.12483859062194824 | Test loss: 0.34211045503616333\n",
            "Epoch: 12120 | Train loss: 0.12466371059417725 | Test loss: 0.3413539528846741\n",
            "Epoch: 12140 | Train loss: 0.12448883056640625 | Test loss: 0.3405974507331848\n",
            "Epoch: 12160 | Train loss: 0.12431395053863525 | Test loss: 0.33984094858169556\n",
            "Epoch: 12180 | Train loss: 0.12413904815912247 | Test loss: 0.3390843868255615\n",
            "Epoch: 12200 | Train loss: 0.12396416813135147 | Test loss: 0.33832788467407227\n",
            "Epoch: 12220 | Train loss: 0.12378928810358047 | Test loss: 0.337571382522583\n",
            "Epoch: 12240 | Train loss: 0.12361439317464828 | Test loss: 0.33681485056877136\n",
            "Epoch: 12260 | Train loss: 0.1234508529305458 | Test loss: 0.33609437942504883\n",
            "Epoch: 12280 | Train loss: 0.12329833954572678 | Test loss: 0.3354034125804901\n",
            "Epoch: 12300 | Train loss: 0.12314579635858536 | Test loss: 0.3347124457359314\n",
            "Epoch: 12320 | Train loss: 0.12299325317144394 | Test loss: 0.3340214192867279\n",
            "Epoch: 12340 | Train loss: 0.12284073978662491 | Test loss: 0.3333304524421692\n",
            "Epoch: 12360 | Train loss: 0.12268819659948349 | Test loss: 0.33263951539993286\n",
            "Epoch: 12380 | Train loss: 0.12253566086292267 | Test loss: 0.33194851875305176\n",
            "Epoch: 12400 | Train loss: 0.12238314002752304 | Test loss: 0.33125755190849304\n",
            "Epoch: 12420 | Train loss: 0.12223060429096222 | Test loss: 0.33056655526161194\n",
            "Epoch: 12440 | Train loss: 0.12207808345556259 | Test loss: 0.32987555861473083\n",
            "Epoch: 12460 | Train loss: 0.12192554771900177 | Test loss: 0.3291845917701721\n",
            "Epoch: 12480 | Train loss: 0.12177302688360214 | Test loss: 0.328493595123291\n",
            "Epoch: 12500 | Train loss: 0.12162049114704132 | Test loss: 0.3278026580810547\n",
            "Epoch: 12520 | Train loss: 0.1214679628610611 | Test loss: 0.32711169123649597\n",
            "Epoch: 12540 | Train loss: 0.12131860107183456 | Test loss: 0.32643377780914307\n",
            "Epoch: 12560 | Train loss: 0.12118575721979141 | Test loss: 0.3258083462715149\n",
            "Epoch: 12580 | Train loss: 0.12105291336774826 | Test loss: 0.3251829147338867\n",
            "Epoch: 12600 | Train loss: 0.12092006206512451 | Test loss: 0.32455748319625854\n",
            "Epoch: 12620 | Train loss: 0.12078719586133957 | Test loss: 0.323932021856308\n",
            "Epoch: 12640 | Train loss: 0.12065434455871582 | Test loss: 0.3233065605163574\n",
            "Epoch: 12660 | Train loss: 0.12052150070667267 | Test loss: 0.32268115878105164\n",
            "Epoch: 12680 | Train loss: 0.12038864940404892 | Test loss: 0.3220556974411011\n",
            "Epoch: 12700 | Train loss: 0.12025579065084457 | Test loss: 0.3214302659034729\n",
            "Epoch: 12720 | Train loss: 0.12012294679880142 | Test loss: 0.3208048343658447\n",
            "Epoch: 12740 | Train loss: 0.11999010294675827 | Test loss: 0.32017937302589417\n",
            "Epoch: 12760 | Train loss: 0.11985724419355392 | Test loss: 0.319553941488266\n",
            "Epoch: 12780 | Train loss: 0.11972439289093018 | Test loss: 0.31892848014831543\n",
            "Epoch: 12800 | Train loss: 0.11959153413772583 | Test loss: 0.31830307841300964\n",
            "Epoch: 12820 | Train loss: 0.11945869028568268 | Test loss: 0.3176776170730591\n",
            "Epoch: 12840 | Train loss: 0.11932583898305893 | Test loss: 0.3170521855354309\n",
            "Epoch: 12860 | Train loss: 0.11919331550598145 | Test loss: 0.31643006205558777\n",
            "Epoch: 12880 | Train loss: 0.11907758563756943 | Test loss: 0.3158706724643707\n",
            "Epoch: 12900 | Train loss: 0.118961863219738 | Test loss: 0.3153112828731537\n",
            "Epoch: 12920 | Train loss: 0.11884613335132599 | Test loss: 0.31475192308425903\n",
            "Epoch: 12940 | Train loss: 0.11873040348291397 | Test loss: 0.31419259309768677\n",
            "Epoch: 12960 | Train loss: 0.11861467361450195 | Test loss: 0.3136332035064697\n",
            "Epoch: 12980 | Train loss: 0.11849894374608994 | Test loss: 0.3130738437175751\n",
            "Epoch: 13000 | Train loss: 0.11838322132825851 | Test loss: 0.31251445412635803\n",
            "Epoch: 13020 | Train loss: 0.1182674914598465 | Test loss: 0.311955064535141\n",
            "Epoch: 13040 | Train loss: 0.11815176159143448 | Test loss: 0.31139570474624634\n",
            "Epoch: 13060 | Train loss: 0.11803603172302246 | Test loss: 0.3108363747596741\n",
            "Epoch: 13080 | Train loss: 0.11792030185461044 | Test loss: 0.31027698516845703\n",
            "Epoch: 13100 | Train loss: 0.11780457943677902 | Test loss: 0.3097176253795624\n",
            "Epoch: 13120 | Train loss: 0.117688849568367 | Test loss: 0.3091582655906677\n",
            "Epoch: 13140 | Train loss: 0.11757311969995499 | Test loss: 0.3085988759994507\n",
            "Epoch: 13160 | Train loss: 0.11745740473270416 | Test loss: 0.30803951621055603\n",
            "Epoch: 13180 | Train loss: 0.11734165996313095 | Test loss: 0.3074801564216614\n",
            "Epoch: 13200 | Train loss: 0.11722594499588013 | Test loss: 0.30692076683044434\n",
            "Epoch: 13220 | Train loss: 0.11711020767688751 | Test loss: 0.3063614070415497\n",
            "Epoch: 13240 | Train loss: 0.11700189113616943 | Test loss: 0.3058386743068695\n",
            "Epoch: 13260 | Train loss: 0.11690070480108261 | Test loss: 0.30534589290618896\n",
            "Epoch: 13280 | Train loss: 0.11679952591657639 | Test loss: 0.3048531711101532\n",
            "Epoch: 13300 | Train loss: 0.11669833958148956 | Test loss: 0.30436038970947266\n",
            "Epoch: 13320 | Train loss: 0.11659713834524155 | Test loss: 0.3038676083087921\n",
            "Epoch: 13340 | Train loss: 0.11649594455957413 | Test loss: 0.30337485671043396\n",
            "Epoch: 13360 | Train loss: 0.1163947582244873 | Test loss: 0.3028820753097534\n",
            "Epoch: 13380 | Train loss: 0.11629357188940048 | Test loss: 0.30238935351371765\n",
            "Epoch: 13400 | Train loss: 0.11619239300489426 | Test loss: 0.3018965721130371\n",
            "Epoch: 13420 | Train loss: 0.11609119176864624 | Test loss: 0.30140379071235657\n",
            "Epoch: 13440 | Train loss: 0.11598999798297882 | Test loss: 0.3009110391139984\n",
            "Epoch: 13460 | Train loss: 0.115888811647892 | Test loss: 0.30041825771331787\n",
            "Epoch: 13480 | Train loss: 0.11578762531280518 | Test loss: 0.2999255359172821\n",
            "Epoch: 13500 | Train loss: 0.11568643897771835 | Test loss: 0.29943275451660156\n",
            "Epoch: 13520 | Train loss: 0.11558524519205093 | Test loss: 0.298939973115921\n",
            "Epoch: 13540 | Train loss: 0.11548405140638351 | Test loss: 0.29844722151756287\n",
            "Epoch: 13560 | Train loss: 0.11538286507129669 | Test loss: 0.2979544401168823\n",
            "Epoch: 13580 | Train loss: 0.11528167873620987 | Test loss: 0.29746171832084656\n",
            "Epoch: 13600 | Train loss: 0.11518049240112305 | Test loss: 0.296968936920166\n",
            "Epoch: 13620 | Train loss: 0.11507929861545563 | Test loss: 0.2964761555194855\n",
            "Epoch: 13640 | Train loss: 0.11497809737920761 | Test loss: 0.2959834039211273\n",
            "Epoch: 13660 | Train loss: 0.11487691849470139 | Test loss: 0.2954906225204468\n",
            "Epoch: 13680 | Train loss: 0.1147836223244667 | Test loss: 0.295044869184494\n",
            "Epoch: 13700 | Train loss: 0.11469435691833496 | Test loss: 0.29461923241615295\n",
            "Epoch: 13720 | Train loss: 0.11460509151220322 | Test loss: 0.2941935956478119\n",
            "Epoch: 13740 | Train loss: 0.11451583355665207 | Test loss: 0.2937679886817932\n",
            "Epoch: 13760 | Train loss: 0.11442656815052032 | Test loss: 0.29334238171577454\n",
            "Epoch: 13780 | Train loss: 0.11433728784322739 | Test loss: 0.29291674494743347\n",
            "Epoch: 13800 | Train loss: 0.11424802988767624 | Test loss: 0.2924911081790924\n",
            "Epoch: 13820 | Train loss: 0.1141587644815445 | Test loss: 0.29206547141075134\n",
            "Epoch: 13840 | Train loss: 0.11406948417425156 | Test loss: 0.29163986444473267\n",
            "Epoch: 13860 | Train loss: 0.11398022621870041 | Test loss: 0.291214257478714\n",
            "Epoch: 13880 | Train loss: 0.11389096081256866 | Test loss: 0.2907886207103729\n",
            "Epoch: 13900 | Train loss: 0.11380169540643692 | Test loss: 0.29036298394203186\n",
            "Epoch: 13920 | Train loss: 0.11371243000030518 | Test loss: 0.2899373471736908\n",
            "Epoch: 13940 | Train loss: 0.11362316459417343 | Test loss: 0.2895117402076721\n",
            "Epoch: 13960 | Train loss: 0.11353390663862228 | Test loss: 0.28908613324165344\n",
            "Epoch: 13980 | Train loss: 0.11344462633132935 | Test loss: 0.2886604964733124\n",
            "Epoch: 14000 | Train loss: 0.1133553609251976 | Test loss: 0.2882348597049713\n",
            "Epoch: 14020 | Train loss: 0.11326608806848526 | Test loss: 0.28780922293663025\n",
            "Epoch: 14040 | Train loss: 0.11317682266235352 | Test loss: 0.2873836159706116\n",
            "Epoch: 14060 | Train loss: 0.11308755725622177 | Test loss: 0.2869580090045929\n",
            "Epoch: 14080 | Train loss: 0.11299829930067062 | Test loss: 0.28653237223625183\n",
            "Epoch: 14100 | Train loss: 0.11290903389453888 | Test loss: 0.28610673546791077\n",
            "Epoch: 14120 | Train loss: 0.11281976848840714 | Test loss: 0.2856810986995697\n",
            "Epoch: 14140 | Train loss: 0.11273050308227539 | Test loss: 0.285255491733551\n",
            "Epoch: 14160 | Train loss: 0.11264123767614365 | Test loss: 0.28482988476753235\n",
            "Epoch: 14180 | Train loss: 0.1125519648194313 | Test loss: 0.2844042479991913\n",
            "Epoch: 14200 | Train loss: 0.11246269941329956 | Test loss: 0.2839786112308502\n",
            "Epoch: 14220 | Train loss: 0.11238107830286026 | Test loss: 0.2836105525493622\n",
            "Epoch: 14240 | Train loss: 0.11230108886957169 | Test loss: 0.2832525670528412\n",
            "Epoch: 14260 | Train loss: 0.11222109943628311 | Test loss: 0.2828946113586426\n",
            "Epoch: 14280 | Train loss: 0.11214111000299454 | Test loss: 0.28253665566444397\n",
            "Epoch: 14300 | Train loss: 0.11206112056970596 | Test loss: 0.28217869997024536\n",
            "Epoch: 14320 | Train loss: 0.11198113113641739 | Test loss: 0.28182077407836914\n",
            "Epoch: 14340 | Train loss: 0.11190114170312881 | Test loss: 0.28146281838417053\n",
            "Epoch: 14360 | Train loss: 0.11182115226984024 | Test loss: 0.2811048626899719\n",
            "Epoch: 14380 | Train loss: 0.11174116283655167 | Test loss: 0.2807469069957733\n",
            "Epoch: 14400 | Train loss: 0.11166117340326309 | Test loss: 0.2803889214992523\n",
            "Epoch: 14420 | Train loss: 0.11158118396997452 | Test loss: 0.2800309956073761\n",
            "Epoch: 14440 | Train loss: 0.11150119453668594 | Test loss: 0.2796730697154999\n",
            "Epoch: 14460 | Train loss: 0.11142120510339737 | Test loss: 0.2793150842189789\n",
            "Epoch: 14480 | Train loss: 0.1113412156701088 | Test loss: 0.2789571285247803\n",
            "Epoch: 14500 | Train loss: 0.11126122623682022 | Test loss: 0.27859917283058167\n",
            "Epoch: 14520 | Train loss: 0.11118123680353165 | Test loss: 0.27824124693870544\n",
            "Epoch: 14540 | Train loss: 0.11110124737024307 | Test loss: 0.27788329124450684\n",
            "Epoch: 14560 | Train loss: 0.1110212579369545 | Test loss: 0.27752530574798584\n",
            "Epoch: 14580 | Train loss: 0.11094126850366592 | Test loss: 0.2771673798561096\n",
            "Epoch: 14600 | Train loss: 0.11086127907037735 | Test loss: 0.276809424161911\n",
            "Epoch: 14620 | Train loss: 0.11078128963708878 | Test loss: 0.2764514684677124\n",
            "Epoch: 14640 | Train loss: 0.1107013002038002 | Test loss: 0.2760935425758362\n",
            "Epoch: 14660 | Train loss: 0.11062131077051163 | Test loss: 0.2757355868816376\n",
            "Epoch: 14680 | Train loss: 0.11054132133722305 | Test loss: 0.2753776013851166\n",
            "Epoch: 14700 | Train loss: 0.11046134680509567 | Test loss: 0.27501964569091797\n",
            "Epoch: 14720 | Train loss: 0.1103813424706459 | Test loss: 0.27466171979904175\n",
            "Epoch: 14740 | Train loss: 0.11030135303735733 | Test loss: 0.27430373430252075\n",
            "Epoch: 14760 | Train loss: 0.11022136360406876 | Test loss: 0.27394580841064453\n",
            "Epoch: 14780 | Train loss: 0.11014137417078018 | Test loss: 0.27358782291412354\n",
            "Epoch: 14800 | Train loss: 0.11006138473749161 | Test loss: 0.2732298970222473\n",
            "Epoch: 14820 | Train loss: 0.10998139530420303 | Test loss: 0.2728719413280487\n",
            "Epoch: 14840 | Train loss: 0.10990140587091446 | Test loss: 0.2725140154361725\n",
            "Epoch: 14860 | Train loss: 0.10982141643762589 | Test loss: 0.2721560597419739\n",
            "Epoch: 14880 | Train loss: 0.10974142700433731 | Test loss: 0.27179810404777527\n",
            "Epoch: 14900 | Train loss: 0.10966143757104874 | Test loss: 0.2714401185512543\n",
            "Epoch: 14920 | Train loss: 0.10958607494831085 | Test loss: 0.271133154630661\n",
            "Epoch: 14940 | Train loss: 0.10951262712478638 | Test loss: 0.27084311842918396\n",
            "Epoch: 14960 | Train loss: 0.1094391867518425 | Test loss: 0.2705531120300293\n",
            "Epoch: 14980 | Train loss: 0.10936572402715683 | Test loss: 0.27026307582855225\n",
            "Epoch: 15000 | Train loss: 0.10929228365421295 | Test loss: 0.26997309923171997\n",
            "Epoch: 15020 | Train loss: 0.10921883583068848 | Test loss: 0.2696830928325653\n",
            "Epoch: 15040 | Train loss: 0.1091453805565834 | Test loss: 0.26939305663108826\n",
            "Epoch: 15060 | Train loss: 0.10907196253538132 | Test loss: 0.2691030204296112\n",
            "Epoch: 15080 | Train loss: 0.10899849236011505 | Test loss: 0.26881301403045654\n",
            "Epoch: 15100 | Train loss: 0.10892504453659058 | Test loss: 0.2685229778289795\n",
            "Epoch: 15120 | Train loss: 0.1088516041636467 | Test loss: 0.2682330012321472\n",
            "Epoch: 15140 | Train loss: 0.10877814143896103 | Test loss: 0.26794296503067017\n",
            "Epoch: 15160 | Train loss: 0.10870470106601715 | Test loss: 0.2676529884338379\n",
            "Epoch: 15180 | Train loss: 0.10863125324249268 | Test loss: 0.26736295223236084\n",
            "Epoch: 15200 | Train loss: 0.1085577979683876 | Test loss: 0.2670729458332062\n",
            "Epoch: 15220 | Train loss: 0.10848437994718552 | Test loss: 0.2667829394340515\n",
            "Epoch: 15240 | Train loss: 0.10841090977191925 | Test loss: 0.26649290323257446\n",
            "Epoch: 15260 | Train loss: 0.10833746194839478 | Test loss: 0.2662028968334198\n",
            "Epoch: 15280 | Train loss: 0.1082640215754509 | Test loss: 0.26591286063194275\n",
            "Epoch: 15300 | Train loss: 0.10819055885076523 | Test loss: 0.2656228542327881\n",
            "Epoch: 15320 | Train loss: 0.10811711847782135 | Test loss: 0.2653328776359558\n",
            "Epoch: 15340 | Train loss: 0.10804367065429688 | Test loss: 0.26504284143447876\n",
            "Epoch: 15360 | Train loss: 0.1079702153801918 | Test loss: 0.2647528350353241\n",
            "Epoch: 15380 | Train loss: 0.10789679735898972 | Test loss: 0.26446279883384705\n",
            "Epoch: 15400 | Train loss: 0.10782332718372345 | Test loss: 0.2641727924346924\n",
            "Epoch: 15420 | Train loss: 0.10774987936019897 | Test loss: 0.2638827860355377\n",
            "Epoch: 15440 | Train loss: 0.1076764389872551 | Test loss: 0.26359277963638306\n",
            "Epoch: 15460 | Train loss: 0.10760297626256943 | Test loss: 0.2633027732372284\n",
            "Epoch: 15480 | Train loss: 0.10752953588962555 | Test loss: 0.26301273703575134\n",
            "Epoch: 15500 | Train loss: 0.10745608806610107 | Test loss: 0.2627227306365967\n",
            "Epoch: 15520 | Train loss: 0.107382632791996 | Test loss: 0.262432724237442\n",
            "Epoch: 15540 | Train loss: 0.10730921477079391 | Test loss: 0.26214271783828735\n",
            "Epoch: 15560 | Train loss: 0.10723574459552765 | Test loss: 0.2618527114391327\n",
            "Epoch: 15580 | Train loss: 0.10716229677200317 | Test loss: 0.26156267523765564\n",
            "Epoch: 15600 | Train loss: 0.1070888563990593 | Test loss: 0.261272668838501\n",
            "Epoch: 15620 | Train loss: 0.10701539367437363 | Test loss: 0.2609826326370239\n",
            "Epoch: 15640 | Train loss: 0.10694195330142975 | Test loss: 0.26069265604019165\n",
            "Epoch: 15660 | Train loss: 0.10686850547790527 | Test loss: 0.260402649641037\n",
            "Epoch: 15680 | Train loss: 0.1067950502038002 | Test loss: 0.26011261343955994\n",
            "Epoch: 15700 | Train loss: 0.10672163218259811 | Test loss: 0.2598225772380829\n",
            "Epoch: 15720 | Train loss: 0.10664816200733185 | Test loss: 0.2595325708389282\n",
            "Epoch: 15740 | Train loss: 0.10657471418380737 | Test loss: 0.25924253463745117\n",
            "Epoch: 15760 | Train loss: 0.1065012738108635 | Test loss: 0.2589525580406189\n",
            "Epoch: 15780 | Train loss: 0.10642781108617783 | Test loss: 0.25866252183914185\n",
            "Epoch: 15800 | Train loss: 0.10635437071323395 | Test loss: 0.25837254524230957\n",
            "Epoch: 15820 | Train loss: 0.10628092288970947 | Test loss: 0.2580825090408325\n",
            "Epoch: 15840 | Train loss: 0.1062074676156044 | Test loss: 0.25779250264167786\n",
            "Epoch: 15860 | Train loss: 0.10613404959440231 | Test loss: 0.2575024962425232\n",
            "Epoch: 15880 | Train loss: 0.10606057941913605 | Test loss: 0.25721246004104614\n",
            "Epoch: 15900 | Train loss: 0.10598713159561157 | Test loss: 0.2569224536418915\n",
            "Epoch: 15920 | Train loss: 0.1059136912226677 | Test loss: 0.25663241744041443\n",
            "Epoch: 15940 | Train loss: 0.10584022849798203 | Test loss: 0.25634241104125977\n",
            "Epoch: 15960 | Train loss: 0.10577010363340378 | Test loss: 0.25611403584480286\n",
            "Epoch: 15980 | Train loss: 0.10570049285888672 | Test loss: 0.25589248538017273\n",
            "Epoch: 16000 | Train loss: 0.10563087463378906 | Test loss: 0.255670964717865\n",
            "Epoch: 16020 | Train loss: 0.1055612564086914 | Test loss: 0.2554493844509125\n",
            "Epoch: 16040 | Train loss: 0.10549163818359375 | Test loss: 0.25522786378860474\n",
            "Epoch: 16060 | Train loss: 0.10542204231023788 | Test loss: 0.2550063133239746\n",
            "Epoch: 16080 | Train loss: 0.10535242408514023 | Test loss: 0.25478479266166687\n",
            "Epoch: 16100 | Train loss: 0.10528282076120377 | Test loss: 0.25456324219703674\n",
            "Epoch: 16120 | Train loss: 0.10521320253610611 | Test loss: 0.2543416917324066\n",
            "Epoch: 16140 | Train loss: 0.10514359921216965 | Test loss: 0.2541201710700989\n",
            "Epoch: 16160 | Train loss: 0.10507398098707199 | Test loss: 0.25389862060546875\n",
            "Epoch: 16180 | Train loss: 0.10500437021255493 | Test loss: 0.253677099943161\n",
            "Epoch: 16200 | Train loss: 0.10493476688861847 | Test loss: 0.2534555494785309\n",
            "Epoch: 16220 | Train loss: 0.10486514866352081 | Test loss: 0.25323399901390076\n",
            "Epoch: 16240 | Train loss: 0.10479553043842316 | Test loss: 0.25301241874694824\n",
            "Epoch: 16260 | Train loss: 0.10472593456506729 | Test loss: 0.2527908980846405\n",
            "Epoch: 16280 | Train loss: 0.10465631633996964 | Test loss: 0.25256937742233276\n",
            "Epoch: 16300 | Train loss: 0.10458669811487198 | Test loss: 0.25234782695770264\n",
            "Epoch: 16320 | Train loss: 0.10451709479093552 | Test loss: 0.2521262764930725\n",
            "Epoch: 16340 | Train loss: 0.10444747656583786 | Test loss: 0.2519047260284424\n",
            "Epoch: 16360 | Train loss: 0.1043778657913208 | Test loss: 0.25168320536613464\n",
            "Epoch: 16380 | Train loss: 0.10430826246738434 | Test loss: 0.2514616847038269\n",
            "Epoch: 16400 | Train loss: 0.10423864424228668 | Test loss: 0.2512401044368744\n",
            "Epoch: 16420 | Train loss: 0.10416902601718903 | Test loss: 0.25101858377456665\n",
            "Epoch: 16440 | Train loss: 0.10409939289093018 | Test loss: 0.2507970333099365\n",
            "Epoch: 16460 | Train loss: 0.10402979701757431 | Test loss: 0.2505755126476288\n",
            "Epoch: 16480 | Train loss: 0.10396019369363785 | Test loss: 0.25035396218299866\n",
            "Epoch: 16500 | Train loss: 0.10389059036970139 | Test loss: 0.25013241171836853\n",
            "Epoch: 16520 | Train loss: 0.10382097214460373 | Test loss: 0.2499108761548996\n",
            "Epoch: 16540 | Train loss: 0.10375135391950607 | Test loss: 0.24968932569026947\n",
            "Epoch: 16560 | Train loss: 0.10368174314498901 | Test loss: 0.24946780502796173\n",
            "Epoch: 16580 | Train loss: 0.10361213982105255 | Test loss: 0.2492462396621704\n",
            "Epoch: 16600 | Train loss: 0.1035425215959549 | Test loss: 0.24902473390102386\n",
            "Epoch: 16620 | Train loss: 0.10347290337085724 | Test loss: 0.24880316853523254\n",
            "Epoch: 16640 | Train loss: 0.10340330749750137 | Test loss: 0.24858160316944122\n",
            "Epoch: 16660 | Train loss: 0.10333368927240372 | Test loss: 0.2483600676059723\n",
            "Epoch: 16680 | Train loss: 0.10326407104730606 | Test loss: 0.24813854694366455\n",
            "Epoch: 16700 | Train loss: 0.1031944528222084 | Test loss: 0.24791698157787323\n",
            "Epoch: 16720 | Train loss: 0.10312485694885254 | Test loss: 0.2476954460144043\n",
            "Epoch: 16740 | Train loss: 0.10305523872375488 | Test loss: 0.24747391045093536\n",
            "Epoch: 16760 | Train loss: 0.10298562049865723 | Test loss: 0.24725237488746643\n",
            "Epoch: 16780 | Train loss: 0.10291602462530136 | Test loss: 0.2470308393239975\n",
            "Epoch: 16800 | Train loss: 0.1028464064002037 | Test loss: 0.24680928885936737\n",
            "Epoch: 16820 | Train loss: 0.10277678817510605 | Test loss: 0.24658775329589844\n",
            "Epoch: 16840 | Train loss: 0.10270716995000839 | Test loss: 0.24636618793010712\n",
            "Epoch: 16860 | Train loss: 0.10263758152723312 | Test loss: 0.24614468216896057\n",
            "Epoch: 16880 | Train loss: 0.10256796330213547 | Test loss: 0.24592314660549164\n",
            "Epoch: 16900 | Train loss: 0.10249834507703781 | Test loss: 0.24570158123970032\n",
            "Epoch: 16920 | Train loss: 0.10242873430252075 | Test loss: 0.24548004567623138\n",
            "Epoch: 16940 | Train loss: 0.1023591160774231 | Test loss: 0.24525849521160126\n",
            "Epoch: 16960 | Train loss: 0.10228951275348663 | Test loss: 0.24503694474697113\n",
            "Epoch: 16980 | Train loss: 0.10221989452838898 | Test loss: 0.244815394282341\n",
            "Epoch: 17000 | Train loss: 0.10215029865503311 | Test loss: 0.24459388852119446\n",
            "Epoch: 17020 | Train loss: 0.10208068042993546 | Test loss: 0.24437232315540314\n",
            "Epoch: 17040 | Train loss: 0.1020110622048378 | Test loss: 0.2441507875919342\n",
            "Epoch: 17060 | Train loss: 0.10194144397974014 | Test loss: 0.24392925202846527\n",
            "Epoch: 17080 | Train loss: 0.10187184065580368 | Test loss: 0.24370770156383514\n",
            "Epoch: 17100 | Train loss: 0.10180222243070602 | Test loss: 0.2434861660003662\n",
            "Epoch: 17120 | Train loss: 0.10173261165618896 | Test loss: 0.24326463043689728\n",
            "Epoch: 17140 | Train loss: 0.10166299343109131 | Test loss: 0.24304309487342834\n",
            "Epoch: 17160 | Train loss: 0.10159339010715485 | Test loss: 0.2428215593099594\n",
            "Epoch: 17180 | Train loss: 0.10152377933263779 | Test loss: 0.24260000884532928\n",
            "Epoch: 17200 | Train loss: 0.10145416110754013 | Test loss: 0.24237847328186035\n",
            "Epoch: 17220 | Train loss: 0.10138455778360367 | Test loss: 0.24215690791606903\n",
            "Epoch: 17240 | Train loss: 0.10131492465734482 | Test loss: 0.2419353574514389\n",
            "Epoch: 17260 | Train loss: 0.10124533623456955 | Test loss: 0.24171383678913116\n",
            "Epoch: 17280 | Train loss: 0.1011757180094719 | Test loss: 0.24149230122566223\n",
            "Epoch: 17300 | Train loss: 0.10110610723495483 | Test loss: 0.2412707656621933\n",
            "Epoch: 17320 | Train loss: 0.10103650391101837 | Test loss: 0.24104920029640198\n",
            "Epoch: 17340 | Train loss: 0.10096688568592072 | Test loss: 0.24082766473293304\n",
            "Epoch: 17360 | Train loss: 0.10089726746082306 | Test loss: 0.24060611426830292\n",
            "Epoch: 17380 | Train loss: 0.100827656686306 | Test loss: 0.24038457870483398\n",
            "Epoch: 17400 | Train loss: 0.10075805336236954 | Test loss: 0.24016304314136505\n",
            "Epoch: 17420 | Train loss: 0.10068843513727188 | Test loss: 0.23994150757789612\n",
            "Epoch: 17440 | Train loss: 0.10061883181333542 | Test loss: 0.2397199422121048\n",
            "Epoch: 17460 | Train loss: 0.10054922103881836 | Test loss: 0.23949842154979706\n",
            "Epoch: 17480 | Train loss: 0.1004796028137207 | Test loss: 0.23927688598632812\n",
            "Epoch: 17500 | Train loss: 0.10040998458862305 | Test loss: 0.2390553504228592\n",
            "Epoch: 17520 | Train loss: 0.10034038126468658 | Test loss: 0.23883378505706787\n",
            "Epoch: 17540 | Train loss: 0.10027076303958893 | Test loss: 0.23861224949359894\n",
            "Epoch: 17560 | Train loss: 0.10020115226507187 | Test loss: 0.23839071393013\n",
            "Epoch: 17580 | Train loss: 0.1001315489411354 | Test loss: 0.23816914856433868\n",
            "Epoch: 17600 | Train loss: 0.10006194561719894 | Test loss: 0.23794762790203094\n",
            "Epoch: 17620 | Train loss: 0.09999232739210129 | Test loss: 0.23772607743740082\n",
            "Epoch: 17640 | Train loss: 0.09992269426584244 | Test loss: 0.2375045269727707\n",
            "Epoch: 17660 | Train loss: 0.09985309839248657 | Test loss: 0.23728299140930176\n",
            "Epoch: 17680 | Train loss: 0.09978348016738892 | Test loss: 0.23706145584583282\n",
            "Epoch: 17700 | Train loss: 0.09971386939287186 | Test loss: 0.2368399202823639\n",
            "Epoch: 17720 | Train loss: 0.0996442660689354 | Test loss: 0.23661838471889496\n",
            "Epoch: 17740 | Train loss: 0.09957464784383774 | Test loss: 0.23639683425426483\n",
            "Epoch: 17760 | Train loss: 0.09950503706932068 | Test loss: 0.2361752986907959\n",
            "Epoch: 17780 | Train loss: 0.09943542629480362 | Test loss: 0.23595376312732697\n",
            "Epoch: 17800 | Train loss: 0.09936581552028656 | Test loss: 0.23573222756385803\n",
            "Epoch: 17820 | Train loss: 0.0992961972951889 | Test loss: 0.2355106920003891\n",
            "Epoch: 17840 | Train loss: 0.09922658652067184 | Test loss: 0.2352890968322754\n",
            "Epoch: 17860 | Train loss: 0.09915698319673538 | Test loss: 0.23506759107112885\n",
            "Epoch: 17880 | Train loss: 0.09908737242221832 | Test loss: 0.23484604060649872\n",
            "Epoch: 17900 | Train loss: 0.09901774674654007 | Test loss: 0.23462450504302979\n",
            "Epoch: 17920 | Train loss: 0.09894814342260361 | Test loss: 0.23440293967723846\n",
            "Epoch: 17940 | Train loss: 0.09887852519750595 | Test loss: 0.23418140411376953\n",
            "Epoch: 17960 | Train loss: 0.09880891442298889 | Test loss: 0.2339598685503006\n",
            "Epoch: 17980 | Train loss: 0.09873931854963303 | Test loss: 0.23373833298683167\n",
            "Epoch: 18000 | Train loss: 0.09866970032453537 | Test loss: 0.23351681232452393\n",
            "Epoch: 18020 | Train loss: 0.09860008209943771 | Test loss: 0.2332952469587326\n",
            "Epoch: 18040 | Train loss: 0.09853143990039825 | Test loss: 0.23312869668006897\n",
            "Epoch: 18060 | Train loss: 0.09846290200948715 | Test loss: 0.23296724259853363\n",
            "Epoch: 18080 | Train loss: 0.09839438647031784 | Test loss: 0.23280401527881622\n",
            "Epoch: 18100 | Train loss: 0.09832584112882614 | Test loss: 0.23264256119728088\n",
            "Epoch: 18120 | Train loss: 0.09825731068849564 | Test loss: 0.23248110711574554\n",
            "Epoch: 18140 | Train loss: 0.09818879514932632 | Test loss: 0.23231609165668488\n",
            "Epoch: 18160 | Train loss: 0.09812027215957642 | Test loss: 0.2321564257144928\n",
            "Epoch: 18180 | Train loss: 0.09805172681808472 | Test loss: 0.2319948524236679\n",
            "Epoch: 18200 | Train loss: 0.09798320382833481 | Test loss: 0.2318299561738968\n",
            "Epoch: 18220 | Train loss: 0.0979146808385849 | Test loss: 0.23166851699352264\n",
            "Epoch: 18240 | Train loss: 0.09784616529941559 | Test loss: 0.23150692880153656\n",
            "Epoch: 18260 | Train loss: 0.09777762740850449 | Test loss: 0.2313438206911087\n",
            "Epoch: 18280 | Train loss: 0.09770908206701279 | Test loss: 0.23118403553962708\n",
            "Epoch: 18300 | Train loss: 0.09764056652784348 | Test loss: 0.23102079331874847\n",
            "Epoch: 18320 | Train loss: 0.09757202863693237 | Test loss: 0.23085932433605194\n",
            "Epoch: 18340 | Train loss: 0.09750350564718246 | Test loss: 0.23069611191749573\n",
            "Epoch: 18360 | Train loss: 0.09743498265743256 | Test loss: 0.2305346578359604\n",
            "Epoch: 18380 | Train loss: 0.09736645221710205 | Test loss: 0.23037318885326385\n",
            "Epoch: 18400 | Train loss: 0.09729792922735214 | Test loss: 0.23020818829536438\n",
            "Epoch: 18420 | Train loss: 0.09722938388586044 | Test loss: 0.2300485223531723\n",
            "Epoch: 18440 | Train loss: 0.09716086834669113 | Test loss: 0.22988693416118622\n",
            "Epoch: 18460 | Train loss: 0.09709233045578003 | Test loss: 0.2297220528125763\n",
            "Epoch: 18480 | Train loss: 0.09702380746603012 | Test loss: 0.22956059873104095\n",
            "Epoch: 18500 | Train loss: 0.09695527702569962 | Test loss: 0.22939901053905487\n",
            "Epoch: 18520 | Train loss: 0.0968867614865303 | Test loss: 0.22923579812049866\n",
            "Epoch: 18540 | Train loss: 0.0968182235956192 | Test loss: 0.2290761023759842\n",
            "Epoch: 18560 | Train loss: 0.0967497006058693 | Test loss: 0.2289111167192459\n",
            "Epoch: 18580 | Train loss: 0.09668117761611938 | Test loss: 0.22874963283538818\n",
            "Epoch: 18600 | Train loss: 0.09661263972520828 | Test loss: 0.22858820855617523\n",
            "Epoch: 18620 | Train loss: 0.09654411673545837 | Test loss: 0.22842660546302795\n",
            "Epoch: 18640 | Train loss: 0.09647558629512787 | Test loss: 0.2282634973526001\n",
            "Epoch: 18660 | Train loss: 0.09640704840421677 | Test loss: 0.2281019240617752\n",
            "Epoch: 18680 | Train loss: 0.09633853286504745 | Test loss: 0.22794046998023987\n",
            "Epoch: 18700 | Train loss: 0.09626998752355576 | Test loss: 0.22777901589870453\n",
            "Epoch: 18720 | Train loss: 0.09620147198438644 | Test loss: 0.22761578857898712\n",
            "Epoch: 18740 | Train loss: 0.09613293409347534 | Test loss: 0.22745433449745178\n",
            "Epoch: 18760 | Train loss: 0.09606441110372543 | Test loss: 0.22729110717773438\n",
            "Epoch: 18780 | Train loss: 0.09599589556455612 | Test loss: 0.22712786495685577\n",
            "Epoch: 18800 | Train loss: 0.09592735022306442 | Test loss: 0.2269681990146637\n",
            "Epoch: 18820 | Train loss: 0.09585882723331451 | Test loss: 0.22680316865444183\n",
            "Epoch: 18840 | Train loss: 0.0957903042435646 | Test loss: 0.2266416996717453\n",
            "Epoch: 18860 | Train loss: 0.0957217738032341 | Test loss: 0.22648027539253235\n",
            "Epoch: 18880 | Train loss: 0.09565325081348419 | Test loss: 0.22631870210170746\n",
            "Epoch: 18900 | Train loss: 0.09558471292257309 | Test loss: 0.22615556418895721\n",
            "Epoch: 18920 | Train loss: 0.09551618248224258 | Test loss: 0.2259940207004547\n",
            "Epoch: 18940 | Train loss: 0.09544765204191208 | Test loss: 0.22583256661891937\n",
            "Epoch: 18960 | Train loss: 0.09537911415100098 | Test loss: 0.22567109763622284\n",
            "Epoch: 18980 | Train loss: 0.09531059116125107 | Test loss: 0.22550788521766663\n",
            "Epoch: 19000 | Train loss: 0.09524205327033997 | Test loss: 0.2253464311361313\n",
            "Epoch: 19020 | Train loss: 0.09517353028059006 | Test loss: 0.2251831740140915\n",
            "Epoch: 19040 | Train loss: 0.09510501474142075 | Test loss: 0.2250216007232666\n",
            "Epoch: 19060 | Train loss: 0.09503649175167084 | Test loss: 0.22486017644405365\n",
            "Epoch: 19080 | Train loss: 0.09496796131134033 | Test loss: 0.22469525039196014\n",
            "Epoch: 19100 | Train loss: 0.09489943087100983 | Test loss: 0.22453546524047852\n",
            "Epoch: 19120 | Train loss: 0.09483089298009872 | Test loss: 0.22437401115894318\n",
            "Epoch: 19140 | Train loss: 0.09476237744092941 | Test loss: 0.22421078383922577\n",
            "Epoch: 19160 | Train loss: 0.09469384700059891 | Test loss: 0.22404754161834717\n",
            "Epoch: 19180 | Train loss: 0.0946253091096878 | Test loss: 0.22388608753681183\n",
            "Epoch: 19200 | Train loss: 0.09455680102109909 | Test loss: 0.22372284531593323\n",
            "Epoch: 19220 | Train loss: 0.09448826313018799 | Test loss: 0.22356140613555908\n",
            "Epoch: 19240 | Train loss: 0.09441973268985748 | Test loss: 0.22339992225170135\n",
            "Epoch: 19260 | Train loss: 0.09435120224952698 | Test loss: 0.22323837876319885\n",
            "Epoch: 19280 | Train loss: 0.09428267925977707 | Test loss: 0.2230752557516098\n",
            "Epoch: 19300 | Train loss: 0.09421414881944656 | Test loss: 0.2229136973619461\n",
            "Epoch: 19320 | Train loss: 0.09414561837911606 | Test loss: 0.22275224328041077\n",
            "Epoch: 19340 | Train loss: 0.09407708793878555 | Test loss: 0.22258734703063965\n",
            "Epoch: 19360 | Train loss: 0.09400855749845505 | Test loss: 0.22242753207683563\n",
            "Epoch: 19380 | Train loss: 0.09394001960754395 | Test loss: 0.22226610779762268\n",
            "Epoch: 19400 | Train loss: 0.09387150406837463 | Test loss: 0.2221028357744217\n",
            "Epoch: 19420 | Train loss: 0.09380298107862473 | Test loss: 0.22193960845470428\n",
            "Epoch: 19440 | Train loss: 0.09373444318771362 | Test loss: 0.22177815437316895\n",
            "Epoch: 19460 | Train loss: 0.09366592019796371 | Test loss: 0.22161494195461273\n",
            "Epoch: 19480 | Train loss: 0.09359738975763321 | Test loss: 0.2214534729719162\n",
            "Epoch: 19500 | Train loss: 0.0935288593173027 | Test loss: 0.22129201889038086\n",
            "Epoch: 19520 | Train loss: 0.0934603363275528 | Test loss: 0.22113047540187836\n",
            "Epoch: 19540 | Train loss: 0.0933917984366417 | Test loss: 0.2209673374891281\n",
            "Epoch: 19560 | Train loss: 0.09332328289747238 | Test loss: 0.22080574929714203\n",
            "Epoch: 19580 | Train loss: 0.09325475990772247 | Test loss: 0.22064252197742462\n",
            "Epoch: 19600 | Train loss: 0.09318622201681137 | Test loss: 0.22048108279705048\n",
            "Epoch: 19620 | Train loss: 0.09311768412590027 | Test loss: 0.22031962871551514\n",
            "Epoch: 19640 | Train loss: 0.09304916113615036 | Test loss: 0.22015805542469025\n",
            "Epoch: 19660 | Train loss: 0.09298063069581985 | Test loss: 0.2199949324131012\n",
            "Epoch: 19680 | Train loss: 0.09291210025548935 | Test loss: 0.2198333740234375\n",
            "Epoch: 19700 | Train loss: 0.09284357726573944 | Test loss: 0.21967191994190216\n",
            "Epoch: 19720 | Train loss: 0.09277504682540894 | Test loss: 0.21950702369213104\n",
            "Epoch: 19740 | Train loss: 0.09270652383565903 | Test loss: 0.21934722363948822\n",
            "Epoch: 19760 | Train loss: 0.09263798594474792 | Test loss: 0.2191823571920395\n",
            "Epoch: 19780 | Train loss: 0.09256946295499802 | Test loss: 0.21902254223823547\n",
            "Epoch: 19800 | Train loss: 0.09250093996524811 | Test loss: 0.21885930001735687\n",
            "Epoch: 19820 | Train loss: 0.0924324095249176 | Test loss: 0.21869786083698273\n",
            "Epoch: 19840 | Train loss: 0.0923638865351677 | Test loss: 0.21853461861610413\n",
            "Epoch: 19860 | Train loss: 0.092295341193676 | Test loss: 0.2183731645345688\n",
            "Epoch: 19880 | Train loss: 0.09222681820392609 | Test loss: 0.21821169555187225\n",
            "Epoch: 19900 | Train loss: 0.09215829521417618 | Test loss: 0.21805015206336975\n",
            "Epoch: 19920 | Train loss: 0.09208976477384567 | Test loss: 0.2178870290517807\n",
            "Epoch: 19940 | Train loss: 0.09202124178409576 | Test loss: 0.21772544085979462\n",
            "Epoch: 19960 | Train loss: 0.09195270389318466 | Test loss: 0.21756398677825928\n",
            "Epoch: 19980 | Train loss: 0.09188418835401535 | Test loss: 0.21739912033081055\n",
            "Epoch: 20000 | Train loss: 0.09181564301252365 | Test loss: 0.21723929047584534\n",
            "Epoch: 20020 | Train loss: 0.09174712002277374 | Test loss: 0.2170744240283966\n",
            "Epoch: 20040 | Train loss: 0.09167858958244324 | Test loss: 0.2169146090745926\n",
            "Epoch: 20060 | Train loss: 0.09161006659269333 | Test loss: 0.2167530506849289\n",
            "Epoch: 20080 | Train loss: 0.09154154360294342 | Test loss: 0.21658816933631897\n",
            "Epoch: 20100 | Train loss: 0.09147300571203232 | Test loss: 0.21642671525478363\n",
            "Epoch: 20120 | Train loss: 0.09140447527170181 | Test loss: 0.21626512706279755\n",
            "Epoch: 20140 | Train loss: 0.0913359597325325 | Test loss: 0.21610203385353088\n",
            "Epoch: 20160 | Train loss: 0.0912674218416214 | Test loss: 0.21594221889972687\n",
            "Epoch: 20180 | Train loss: 0.09119889885187149 | Test loss: 0.21577897667884827\n",
            "Epoch: 20200 | Train loss: 0.09113036841154099 | Test loss: 0.21561752259731293\n",
            "Epoch: 20220 | Train loss: 0.09106184542179108 | Test loss: 0.21545429527759552\n",
            "Epoch: 20240 | Train loss: 0.09099330753087997 | Test loss: 0.21529284119606018\n",
            "Epoch: 20260 | Train loss: 0.09092477709054947 | Test loss: 0.21513138711452484\n",
            "Epoch: 20280 | Train loss: 0.09085625410079956 | Test loss: 0.21496640145778656\n",
            "Epoch: 20300 | Train loss: 0.09078773111104965 | Test loss: 0.2148067057132721\n",
            "Epoch: 20320 | Train loss: 0.09071918576955795 | Test loss: 0.2146451324224472\n",
            "Epoch: 20340 | Train loss: 0.09065067023038864 | Test loss: 0.2144802361726761\n",
            "Epoch: 20360 | Train loss: 0.09058213979005814 | Test loss: 0.21431879699230194\n",
            "Epoch: 20380 | Train loss: 0.09051362425088882 | Test loss: 0.21415720880031586\n",
            "Epoch: 20400 | Train loss: 0.09044508635997772 | Test loss: 0.213994100689888\n",
            "Epoch: 20420 | Train loss: 0.09037654846906662 | Test loss: 0.21383431553840637\n",
            "Epoch: 20440 | Train loss: 0.09030802547931671 | Test loss: 0.21367107331752777\n",
            "Epoch: 20460 | Train loss: 0.09023948758840561 | Test loss: 0.21350960433483124\n",
            "Epoch: 20480 | Train loss: 0.0901709720492363 | Test loss: 0.21334639191627502\n",
            "Epoch: 20500 | Train loss: 0.09010244160890579 | Test loss: 0.21318493783473969\n",
            "Epoch: 20520 | Train loss: 0.09003391116857529 | Test loss: 0.21302346885204315\n",
            "Epoch: 20540 | Train loss: 0.08996538817882538 | Test loss: 0.21285846829414368\n",
            "Epoch: 20560 | Train loss: 0.08989685028791428 | Test loss: 0.2126988023519516\n",
            "Epoch: 20580 | Train loss: 0.08982832729816437 | Test loss: 0.21253721415996552\n",
            "Epoch: 20600 | Train loss: 0.08975980430841446 | Test loss: 0.21237397193908691\n",
            "Epoch: 20620 | Train loss: 0.08969126641750336 | Test loss: 0.21221251785755157\n",
            "Epoch: 20640 | Train loss: 0.08962273597717285 | Test loss: 0.21204929053783417\n",
            "Epoch: 20660 | Train loss: 0.08955422788858414 | Test loss: 0.21188607811927795\n",
            "Epoch: 20680 | Train loss: 0.08948568254709244 | Test loss: 0.2117263823747635\n",
            "Epoch: 20700 | Train loss: 0.08941715955734253 | Test loss: 0.2115613967180252\n",
            "Epoch: 20720 | Train loss: 0.08934862911701202 | Test loss: 0.21139991283416748\n",
            "Epoch: 20740 | Train loss: 0.08928009867668152 | Test loss: 0.21123848855495453\n",
            "Epoch: 20760 | Train loss: 0.08921157568693161 | Test loss: 0.21107688546180725\n",
            "Epoch: 20780 | Train loss: 0.0891430452466011 | Test loss: 0.2109137773513794\n",
            "Epoch: 20800 | Train loss: 0.08907450735569 | Test loss: 0.2107522040605545\n",
            "Epoch: 20820 | Train loss: 0.08900599926710129 | Test loss: 0.21059074997901917\n",
            "Epoch: 20840 | Train loss: 0.08893745392560959 | Test loss: 0.21042929589748383\n",
            "Epoch: 20860 | Train loss: 0.08886893838644028 | Test loss: 0.21026606857776642\n",
            "Epoch: 20880 | Train loss: 0.08880039304494858 | Test loss: 0.21010461449623108\n",
            "Epoch: 20900 | Train loss: 0.08873187005519867 | Test loss: 0.20994138717651367\n",
            "Epoch: 20920 | Train loss: 0.08866335451602936 | Test loss: 0.20977814495563507\n",
            "Epoch: 20940 | Train loss: 0.08859480917453766 | Test loss: 0.209618479013443\n",
            "Epoch: 20960 | Train loss: 0.08852628618478775 | Test loss: 0.20945344865322113\n",
            "Epoch: 20980 | Train loss: 0.08845777064561844 | Test loss: 0.2092919796705246\n",
            "Epoch: 21000 | Train loss: 0.08838923275470734 | Test loss: 0.20913055539131165\n",
            "Epoch: 21020 | Train loss: 0.08832070976495743 | Test loss: 0.20896898210048676\n",
            "Epoch: 21040 | Train loss: 0.08825217932462692 | Test loss: 0.2088058441877365\n",
            "Epoch: 21060 | Train loss: 0.08818364143371582 | Test loss: 0.208644300699234\n",
            "Epoch: 21080 | Train loss: 0.08811511844396591 | Test loss: 0.20848284661769867\n",
            "Epoch: 21100 | Train loss: 0.08804657310247421 | Test loss: 0.20832137763500214\n",
            "Epoch: 21120 | Train loss: 0.0879780575633049 | Test loss: 0.20815816521644592\n",
            "Epoch: 21140 | Train loss: 0.0879095271229744 | Test loss: 0.20799656212329865\n",
            "Epoch: 21160 | Train loss: 0.08784100413322449 | Test loss: 0.2078334540128708\n",
            "Epoch: 21180 | Train loss: 0.08777248114347458 | Test loss: 0.2076718807220459\n",
            "Epoch: 21200 | Train loss: 0.08770394325256348 | Test loss: 0.20751045644283295\n",
            "Epoch: 21220 | Train loss: 0.08763542026281357 | Test loss: 0.20734553039073944\n",
            "Epoch: 21240 | Train loss: 0.08756688982248306 | Test loss: 0.2071857452392578\n",
            "Epoch: 21260 | Train loss: 0.08749835938215256 | Test loss: 0.20702429115772247\n",
            "Epoch: 21280 | Train loss: 0.08742983639240265 | Test loss: 0.20686106383800507\n",
            "Epoch: 21300 | Train loss: 0.08736130595207214 | Test loss: 0.20669782161712646\n",
            "Epoch: 21320 | Train loss: 0.08729276806116104 | Test loss: 0.20653636753559113\n",
            "Epoch: 21340 | Train loss: 0.08722425252199173 | Test loss: 0.20637312531471252\n",
            "Epoch: 21360 | Train loss: 0.08715572208166122 | Test loss: 0.20621168613433838\n",
            "Epoch: 21380 | Train loss: 0.08708719164133072 | Test loss: 0.20605020225048065\n",
            "Epoch: 21400 | Train loss: 0.08701866120100021 | Test loss: 0.20588865876197815\n",
            "Epoch: 21420 | Train loss: 0.0869501382112503 | Test loss: 0.2057255357503891\n",
            "Epoch: 21440 | Train loss: 0.0868816077709198 | Test loss: 0.2055639773607254\n",
            "Epoch: 21460 | Train loss: 0.08681308478116989 | Test loss: 0.20540252327919006\n",
            "Epoch: 21480 | Train loss: 0.08674454689025879 | Test loss: 0.20523762702941895\n",
            "Epoch: 21500 | Train loss: 0.08667601644992828 | Test loss: 0.20507781207561493\n",
            "Epoch: 21520 | Train loss: 0.08660747855901718 | Test loss: 0.20491638779640198\n",
            "Epoch: 21540 | Train loss: 0.08653896301984787 | Test loss: 0.204753115773201\n",
            "Epoch: 21560 | Train loss: 0.08647044003009796 | Test loss: 0.20458988845348358\n",
            "Epoch: 21580 | Train loss: 0.08640190213918686 | Test loss: 0.20442843437194824\n",
            "Epoch: 21600 | Train loss: 0.08633337914943695 | Test loss: 0.20426522195339203\n",
            "Epoch: 21620 | Train loss: 0.08626484870910645 | Test loss: 0.2041037529706955\n",
            "Epoch: 21640 | Train loss: 0.08619631826877594 | Test loss: 0.20394229888916016\n",
            "Epoch: 21660 | Train loss: 0.08612779527902603 | Test loss: 0.20378075540065765\n",
            "Epoch: 21680 | Train loss: 0.08605927228927612 | Test loss: 0.20361749827861786\n",
            "Epoch: 21700 | Train loss: 0.08599074184894562 | Test loss: 0.20345602929592133\n",
            "Epoch: 21720 | Train loss: 0.08592221885919571 | Test loss: 0.20329280197620392\n",
            "Epoch: 21740 | Train loss: 0.0858536809682846 | Test loss: 0.20313136279582977\n",
            "Epoch: 21760 | Train loss: 0.0857851430773735 | Test loss: 0.20296990871429443\n",
            "Epoch: 21780 | Train loss: 0.0857166275382042 | Test loss: 0.20280833542346954\n",
            "Epoch: 21800 | Train loss: 0.08564808964729309 | Test loss: 0.2026452124118805\n",
            "Epoch: 21820 | Train loss: 0.08557956665754318 | Test loss: 0.2024836540222168\n",
            "Epoch: 21840 | Train loss: 0.08551102876663208 | Test loss: 0.20232219994068146\n",
            "Epoch: 21860 | Train loss: 0.08544250577688217 | Test loss: 0.20215730369091034\n",
            "Epoch: 21880 | Train loss: 0.08537397533655167 | Test loss: 0.20199750363826752\n",
            "Epoch: 21900 | Train loss: 0.08530544489622116 | Test loss: 0.2018326073884964\n",
            "Epoch: 21920 | Train loss: 0.08523691445589066 | Test loss: 0.20167279243469238\n",
            "Epoch: 21940 | Train loss: 0.08516839146614075 | Test loss: 0.2015112191438675\n",
            "Epoch: 21960 | Train loss: 0.08509986847639084 | Test loss: 0.2013479769229889\n",
            "Epoch: 21980 | Train loss: 0.08503134548664093 | Test loss: 0.201186403632164\n",
            "Epoch: 22000 | Train loss: 0.08496280759572983 | Test loss: 0.20102326571941376\n",
            "Epoch: 22020 | Train loss: 0.08489428460597992 | Test loss: 0.20086169242858887\n",
            "Epoch: 22040 | Train loss: 0.08482575416564941 | Test loss: 0.20069842040538788\n",
            "Epoch: 22060 | Train loss: 0.08475722372531891 | Test loss: 0.20053687691688538\n",
            "Epoch: 22080 | Train loss: 0.0846887156367302 | Test loss: 0.2003752738237381\n",
            "Epoch: 22100 | Train loss: 0.08462017774581909 | Test loss: 0.2002120316028595\n",
            "Epoch: 22120 | Train loss: 0.08455164730548859 | Test loss: 0.2000504583120346\n",
            "Epoch: 22140 | Train loss: 0.08448310941457748 | Test loss: 0.19988732039928436\n",
            "Epoch: 22160 | Train loss: 0.08441459387540817 | Test loss: 0.19972752034664154\n",
            "Epoch: 22180 | Train loss: 0.08434606343507767 | Test loss: 0.19956248998641968\n",
            "Epoch: 22200 | Train loss: 0.08427753299474716 | Test loss: 0.19940268993377686\n",
            "Epoch: 22220 | Train loss: 0.08420900255441666 | Test loss: 0.19924111664295197\n",
            "Epoch: 22240 | Train loss: 0.08414048701524734 | Test loss: 0.1990761011838913\n",
            "Epoch: 22260 | Train loss: 0.08407195657491684 | Test loss: 0.19891463220119476\n",
            "Epoch: 22280 | Train loss: 0.08400342613458633 | Test loss: 0.19875316321849823\n",
            "Epoch: 22300 | Train loss: 0.08393489569425583 | Test loss: 0.19859157502651215\n",
            "Epoch: 22320 | Train loss: 0.08386636525392532 | Test loss: 0.19842998683452606\n",
            "Epoch: 22340 | Train loss: 0.08379783481359482 | Test loss: 0.19826674461364746\n",
            "Epoch: 22360 | Train loss: 0.0837293192744255 | Test loss: 0.19810515642166138\n",
            "Epoch: 22380 | Train loss: 0.0836607962846756 | Test loss: 0.1979435831308365\n",
            "Epoch: 22400 | Train loss: 0.0835922583937645 | Test loss: 0.19778211414813995\n",
            "Epoch: 22420 | Train loss: 0.0835237205028534 | Test loss: 0.19761720299720764\n",
            "Epoch: 22440 | Train loss: 0.08345520496368408 | Test loss: 0.19745740294456482\n",
            "Epoch: 22460 | Train loss: 0.08338667452335358 | Test loss: 0.19729404151439667\n",
            "Epoch: 22480 | Train loss: 0.08331815153360367 | Test loss: 0.19713079929351807\n",
            "Epoch: 22500 | Train loss: 0.08324962109327316 | Test loss: 0.19697098433971405\n",
            "Epoch: 22520 | Train loss: 0.08318110555410385 | Test loss: 0.1968076527118683\n",
            "Epoch: 22540 | Train loss: 0.08311256021261215 | Test loss: 0.19664616882801056\n",
            "Epoch: 22560 | Train loss: 0.08304403722286224 | Test loss: 0.19648471474647522\n",
            "Epoch: 22580 | Train loss: 0.08297550678253174 | Test loss: 0.19632144272327423\n",
            "Epoch: 22600 | Train loss: 0.08290698379278183 | Test loss: 0.19615988433361053\n",
            "Epoch: 22620 | Train loss: 0.08283846080303192 | Test loss: 0.19599829614162445\n",
            "Epoch: 22640 | Train loss: 0.08276993036270142 | Test loss: 0.19583503901958466\n",
            "Epoch: 22660 | Train loss: 0.08270140737295151 | Test loss: 0.19567346572875977\n",
            "Epoch: 22680 | Train loss: 0.082632876932621 | Test loss: 0.19551022350788116\n",
            "Epoch: 22700 | Train loss: 0.0825643464922905 | Test loss: 0.19534876942634583\n",
            "Epoch: 22720 | Train loss: 0.08249582350254059 | Test loss: 0.19518551230430603\n",
            "Epoch: 22740 | Train loss: 0.08242727816104889 | Test loss: 0.19502393901348114\n",
            "Epoch: 22760 | Train loss: 0.08235877007246017 | Test loss: 0.19486235082149506\n",
            "Epoch: 22780 | Train loss: 0.08229024708271027 | Test loss: 0.19469909369945526\n",
            "Epoch: 22800 | Train loss: 0.08222170919179916 | Test loss: 0.19453752040863037\n",
            "Epoch: 22820 | Train loss: 0.08215317875146866 | Test loss: 0.19437439739704132\n",
            "Epoch: 22840 | Train loss: 0.08208464831113815 | Test loss: 0.1942145824432373\n",
            "Epoch: 22860 | Train loss: 0.08201612532138824 | Test loss: 0.19404956698417664\n",
            "Epoch: 22880 | Train loss: 0.08194758743047714 | Test loss: 0.1938897669315338\n",
            "Epoch: 22900 | Train loss: 0.08187907189130783 | Test loss: 0.19372819364070892\n",
            "Epoch: 22920 | Train loss: 0.08181054890155792 | Test loss: 0.19356314837932587\n",
            "Epoch: 22940 | Train loss: 0.08174201101064682 | Test loss: 0.19340334832668304\n",
            "Epoch: 22960 | Train loss: 0.08167348802089691 | Test loss: 0.1932418793439865\n",
            "Epoch: 22980 | Train loss: 0.0816049575805664 | Test loss: 0.1930786371231079\n",
            "Epoch: 23000 | Train loss: 0.0815364345908165 | Test loss: 0.19291706383228302\n",
            "Epoch: 23020 | Train loss: 0.08146790415048599 | Test loss: 0.19275382161140442\n",
            "Epoch: 23040 | Train loss: 0.08139938116073608 | Test loss: 0.19259224832057953\n",
            "Epoch: 23060 | Train loss: 0.08133085072040558 | Test loss: 0.19243066012859344\n",
            "Epoch: 23080 | Train loss: 0.08126232773065567 | Test loss: 0.19226740300655365\n",
            "Epoch: 23100 | Train loss: 0.08119379729032516 | Test loss: 0.19210593402385712\n",
            "Epoch: 23120 | Train loss: 0.08112525939941406 | Test loss: 0.19194446504116058\n",
            "Epoch: 23140 | Train loss: 0.08105672895908356 | Test loss: 0.19178111851215363\n",
            "Epoch: 23160 | Train loss: 0.08098822087049484 | Test loss: 0.19161787629127502\n",
            "Epoch: 23180 | Train loss: 0.08091968297958374 | Test loss: 0.1914580762386322\n",
            "Epoch: 23200 | Train loss: 0.08085115998983383 | Test loss: 0.19129471480846405\n",
            "Epoch: 23220 | Train loss: 0.08078264445066452 | Test loss: 0.19113147258758545\n",
            "Epoch: 23240 | Train loss: 0.08071409910917282 | Test loss: 0.19097000360488892\n",
            "Epoch: 23260 | Train loss: 0.08064556121826172 | Test loss: 0.1908085197210312\n",
            "Epoch: 23280 | Train loss: 0.08057703822851181 | Test loss: 0.1906469464302063\n",
            "Epoch: 23300 | Train loss: 0.0805085226893425 | Test loss: 0.19048535823822021\n",
            "Epoch: 23320 | Train loss: 0.080439992249012 | Test loss: 0.1903221160173416\n",
            "Epoch: 23340 | Train loss: 0.08037146925926208 | Test loss: 0.19016054272651672\n",
            "Epoch: 23360 | Train loss: 0.08030294626951218 | Test loss: 0.18999730050563812\n",
            "Epoch: 23380 | Train loss: 0.08023440837860107 | Test loss: 0.1898358315229416\n",
            "Epoch: 23400 | Train loss: 0.08016588538885117 | Test loss: 0.1896725744009018\n",
            "Epoch: 23420 | Train loss: 0.08009734004735947 | Test loss: 0.1895109862089157\n",
            "Epoch: 23440 | Train loss: 0.08002882450819016 | Test loss: 0.18934941291809082\n",
            "Epoch: 23460 | Train loss: 0.07996030151844025 | Test loss: 0.18918617069721222\n",
            "Epoch: 23480 | Train loss: 0.07989176362752914 | Test loss: 0.18902459740638733\n",
            "Epoch: 23500 | Train loss: 0.07982324808835983 | Test loss: 0.18886135518550873\n",
            "Epoch: 23520 | Train loss: 0.07975471019744873 | Test loss: 0.18870165944099426\n",
            "Epoch: 23540 | Train loss: 0.07968618720769882 | Test loss: 0.1885366290807724\n",
            "Epoch: 23560 | Train loss: 0.07961765676736832 | Test loss: 0.18837682902812958\n",
            "Epoch: 23580 | Train loss: 0.07954912632703781 | Test loss: 0.1882152557373047\n",
            "Epoch: 23600 | Train loss: 0.0794806107878685 | Test loss: 0.18805022537708282\n",
            "Epoch: 23620 | Train loss: 0.079412080347538 | Test loss: 0.1878904402256012\n",
            "Epoch: 23640 | Train loss: 0.07934354990720749 | Test loss: 0.18772883713245392\n",
            "Epoch: 23660 | Train loss: 0.07927502691745758 | Test loss: 0.18756571412086487\n",
            "Epoch: 23680 | Train loss: 0.07920649647712708 | Test loss: 0.18740412592887878\n",
            "Epoch: 23700 | Train loss: 0.07913795858621597 | Test loss: 0.18724088370800018\n",
            "Epoch: 23720 | Train loss: 0.07906944304704666 | Test loss: 0.1870792955160141\n",
            "Epoch: 23740 | Train loss: 0.07900092005729675 | Test loss: 0.1869177222251892\n",
            "Epoch: 23760 | Train loss: 0.07893239706754684 | Test loss: 0.1867544800043106\n",
            "Epoch: 23780 | Train loss: 0.07886386662721634 | Test loss: 0.18659289181232452\n",
            "Epoch: 23800 | Train loss: 0.07879532873630524 | Test loss: 0.18642976880073547\n",
            "Epoch: 23820 | Train loss: 0.07872679084539413 | Test loss: 0.18626819550991058\n",
            "Epoch: 23840 | Train loss: 0.07865827530622482 | Test loss: 0.1861049383878708\n",
            "Epoch: 23860 | Train loss: 0.07858973741531372 | Test loss: 0.18594513833522797\n",
            "Epoch: 23880 | Train loss: 0.07852122187614441 | Test loss: 0.185781791806221\n",
            "Epoch: 23900 | Train loss: 0.0784526988863945 | Test loss: 0.18561851978302002\n",
            "Epoch: 23920 | Train loss: 0.0783841609954834 | Test loss: 0.18545706570148468\n",
            "Epoch: 23940 | Train loss: 0.0783156305551529 | Test loss: 0.18529558181762695\n",
            "Epoch: 23960 | Train loss: 0.07824710756540298 | Test loss: 0.18513402342796326\n",
            "Epoch: 23980 | Train loss: 0.07817857712507248 | Test loss: 0.18497243523597717\n",
            "Epoch: 24000 | Train loss: 0.07811006158590317 | Test loss: 0.18480919301509857\n",
            "Epoch: 24020 | Train loss: 0.07804153114557266 | Test loss: 0.1846476048231125\n",
            "Epoch: 24040 | Train loss: 0.07797300070524216 | Test loss: 0.1844860315322876\n",
            "Epoch: 24060 | Train loss: 0.07790447771549225 | Test loss: 0.18432112038135529\n",
            "Epoch: 24080 | Train loss: 0.07783593982458115 | Test loss: 0.18415965139865875\n",
            "Epoch: 24100 | Train loss: 0.07776740938425064 | Test loss: 0.18399807810783386\n",
            "Epoch: 24120 | Train loss: 0.07769889384508133 | Test loss: 0.18383648991584778\n",
            "Epoch: 24140 | Train loss: 0.07763036340475082 | Test loss: 0.18367324769496918\n",
            "Epoch: 24160 | Train loss: 0.07756183296442032 | Test loss: 0.1835116595029831\n",
            "Epoch: 24180 | Train loss: 0.07749331742525101 | Test loss: 0.183350071310997\n",
            "Epoch: 24200 | Train loss: 0.0774247795343399 | Test loss: 0.18318860232830048\n",
            "Epoch: 24220 | Train loss: 0.0773562416434288 | Test loss: 0.18302370607852936\n",
            "Epoch: 24240 | Train loss: 0.07728772610425949 | Test loss: 0.18286390602588654\n",
            "Epoch: 24260 | Train loss: 0.07721918821334839 | Test loss: 0.18270233273506165\n",
            "Epoch: 24280 | Train loss: 0.07715067267417908 | Test loss: 0.18253730237483978\n",
            "Epoch: 24300 | Train loss: 0.07708214968442917 | Test loss: 0.18237750232219696\n",
            "Epoch: 24320 | Train loss: 0.07701361924409866 | Test loss: 0.18221591413021088\n",
            "Epoch: 24340 | Train loss: 0.07694508880376816 | Test loss: 0.18205265700817108\n",
            "Epoch: 24360 | Train loss: 0.07687655091285706 | Test loss: 0.18189120292663574\n",
            "Epoch: 24380 | Train loss: 0.07680803537368774 | Test loss: 0.18172796070575714\n",
            "Epoch: 24400 | Train loss: 0.07673950493335724 | Test loss: 0.18156638741493225\n",
            "Epoch: 24420 | Train loss: 0.07667098194360733 | Test loss: 0.18140479922294617\n",
            "Epoch: 24440 | Train loss: 0.07660245895385742 | Test loss: 0.18124155700206757\n",
            "Epoch: 24460 | Train loss: 0.07653392851352692 | Test loss: 0.18107996881008148\n",
            "Epoch: 24480 | Train loss: 0.07646539062261581 | Test loss: 0.18091683089733124\n",
            "Epoch: 24500 | Train loss: 0.07639684528112411 | Test loss: 0.18075525760650635\n",
            "Epoch: 24520 | Train loss: 0.0763283371925354 | Test loss: 0.18059201538562775\n",
            "Epoch: 24540 | Train loss: 0.0762598067522049 | Test loss: 0.18043220043182373\n",
            "Epoch: 24560 | Train loss: 0.07619128376245499 | Test loss: 0.18026885390281677\n",
            "Epoch: 24580 | Train loss: 0.07612277567386627 | Test loss: 0.18010561168193817\n",
            "Epoch: 24600 | Train loss: 0.07605423033237457 | Test loss: 0.17994581162929535\n",
            "Epoch: 24620 | Train loss: 0.07598569244146347 | Test loss: 0.17978088557720184\n",
            "Epoch: 24640 | Train loss: 0.07591717690229416 | Test loss: 0.17962108552455902\n",
            "Epoch: 24660 | Train loss: 0.07584863901138306 | Test loss: 0.17945949733257294\n",
            "Epoch: 24680 | Train loss: 0.07578011602163315 | Test loss: 0.17929625511169434\n",
            "Epoch: 24700 | Train loss: 0.07571159303188324 | Test loss: 0.17913468182086945\n",
            "Epoch: 24720 | Train loss: 0.07564306259155273 | Test loss: 0.17897310853004456\n",
            "Epoch: 24740 | Train loss: 0.07557453960180283 | Test loss: 0.17880986630916595\n",
            "Epoch: 24760 | Train loss: 0.07550600916147232 | Test loss: 0.17864839732646942\n",
            "Epoch: 24780 | Train loss: 0.07543747127056122 | Test loss: 0.17848514020442963\n",
            "Epoch: 24800 | Train loss: 0.0753689557313919 | Test loss: 0.17832355201244354\n",
            "Epoch: 24820 | Train loss: 0.0753004252910614 | Test loss: 0.17816029489040375\n",
            "Epoch: 24840 | Train loss: 0.07523190230131149 | Test loss: 0.17799873650074005\n",
            "Epoch: 24860 | Train loss: 0.07516337931156158 | Test loss: 0.17783714830875397\n",
            "Epoch: 24880 | Train loss: 0.07509484142065048 | Test loss: 0.17767392098903656\n",
            "Epoch: 24900 | Train loss: 0.07502631098031998 | Test loss: 0.17751245200634003\n",
            "Epoch: 24920 | Train loss: 0.07495778053998947 | Test loss: 0.1773509681224823\n",
            "Epoch: 24940 | Train loss: 0.07488925755023956 | Test loss: 0.1771893948316574\n",
            "Epoch: 24960 | Train loss: 0.07482073456048965 | Test loss: 0.17702437937259674\n",
            "Epoch: 24980 | Train loss: 0.07475220412015915 | Test loss: 0.17686454951763153\n",
            "Epoch: 25000 | Train loss: 0.07468368113040924 | Test loss: 0.17670297622680664\n",
            "Epoch: 25020 | Train loss: 0.07461515814065933 | Test loss: 0.17653973400592804\n",
            "Epoch: 25040 | Train loss: 0.07454662024974823 | Test loss: 0.1763782799243927\n",
            "Epoch: 25060 | Train loss: 0.07447808980941772 | Test loss: 0.1762150377035141\n",
            "Epoch: 25080 | Train loss: 0.07440956681966782 | Test loss: 0.17605344951152802\n",
            "Epoch: 25100 | Train loss: 0.07434103637933731 | Test loss: 0.17589186131954193\n",
            "Epoch: 25120 | Train loss: 0.0742725133895874 | Test loss: 0.17572860419750214\n",
            "Epoch: 25140 | Train loss: 0.0742039903998375 | Test loss: 0.17556703090667725\n",
            "Epoch: 25160 | Train loss: 0.07413546741008759 | Test loss: 0.17540380358695984\n",
            "Epoch: 25180 | Train loss: 0.07406691461801529 | Test loss: 0.1752423346042633\n",
            "Epoch: 25200 | Train loss: 0.07399839907884598 | Test loss: 0.1750790774822235\n",
            "Epoch: 25220 | Train loss: 0.07392986863851547 | Test loss: 0.1749192774295807\n",
            "Epoch: 25240 | Train loss: 0.07386134564876556 | Test loss: 0.17475591599941254\n",
            "Epoch: 25260 | Train loss: 0.07379283010959625 | Test loss: 0.17459265887737274\n",
            "Epoch: 25280 | Train loss: 0.07372429221868515 | Test loss: 0.1744328737258911\n",
            "Epoch: 25300 | Train loss: 0.07365577667951584 | Test loss: 0.17426784336566925\n",
            "Epoch: 25320 | Train loss: 0.07358723133802414 | Test loss: 0.17410814762115479\n",
            "Epoch: 25340 | Train loss: 0.07351870834827423 | Test loss: 0.1739465594291687\n",
            "Epoch: 25360 | Train loss: 0.07345017790794373 | Test loss: 0.1737833321094513\n",
            "Epoch: 25380 | Train loss: 0.07338165491819382 | Test loss: 0.1736217588186264\n",
            "Epoch: 25400 | Train loss: 0.07331313192844391 | Test loss: 0.17346017062664032\n",
            "Epoch: 25420 | Train loss: 0.0732446014881134 | Test loss: 0.17329692840576172\n",
            "Epoch: 25440 | Train loss: 0.0731760784983635 | Test loss: 0.17313534021377563\n",
            "Epoch: 25460 | Train loss: 0.07310754060745239 | Test loss: 0.1729722023010254\n",
            "Epoch: 25480 | Train loss: 0.07303901761770248 | Test loss: 0.1728106290102005\n",
            "Epoch: 25500 | Train loss: 0.07297049462795258 | Test loss: 0.1726473867893219\n",
            "Epoch: 25520 | Train loss: 0.07290195673704147 | Test loss: 0.172485813498497\n",
            "Epoch: 25540 | Train loss: 0.07283344119787216 | Test loss: 0.17232422530651093\n",
            "Epoch: 25560 | Train loss: 0.07276491820812225 | Test loss: 0.17216099798679352\n",
            "Epoch: 25580 | Train loss: 0.07269640266895294 | Test loss: 0.1719994843006134\n",
            "Epoch: 25600 | Train loss: 0.07262788712978363 | Test loss: 0.1718362718820572\n",
            "Epoch: 25620 | Train loss: 0.07255935668945312 | Test loss: 0.17167487740516663\n",
            "Epoch: 25640 | Train loss: 0.07249084115028381 | Test loss: 0.17151348292827606\n",
            "Epoch: 25660 | Train loss: 0.0724223256111145 | Test loss: 0.17135195434093475\n",
            "Epoch: 25680 | Train loss: 0.0723538026213646 | Test loss: 0.17118875682353973\n",
            "Epoch: 25700 | Train loss: 0.07228529453277588 | Test loss: 0.17102722823619843\n",
            "Epoch: 25720 | Train loss: 0.07221677154302597 | Test loss: 0.17086583375930786\n",
            "Epoch: 25740 | Train loss: 0.07214825600385666 | Test loss: 0.1707027703523636\n",
            "Epoch: 25760 | Train loss: 0.07207973301410675 | Test loss: 0.17054124176502228\n",
            "Epoch: 25780 | Train loss: 0.07201122492551804 | Test loss: 0.17037805914878845\n",
            "Epoch: 25800 | Train loss: 0.07194270938634872 | Test loss: 0.17021651566028595\n",
            "Epoch: 25820 | Train loss: 0.07187419384717941 | Test loss: 0.17005334794521332\n",
            "Epoch: 25840 | Train loss: 0.0718056708574295 | Test loss: 0.16989028453826904\n",
            "Epoch: 25860 | Train loss: 0.0717371478676796 | Test loss: 0.169730544090271\n",
            "Epoch: 25880 | Train loss: 0.07166863232851028 | Test loss: 0.1695690155029297\n",
            "Epoch: 25900 | Train loss: 0.07160011678934097 | Test loss: 0.16940581798553467\n",
            "Epoch: 25920 | Train loss: 0.07153160870075226 | Test loss: 0.16924431920051575\n",
            "Epoch: 25940 | Train loss: 0.07146308571100235 | Test loss: 0.16908124089241028\n",
            "Epoch: 25960 | Train loss: 0.07139457017183304 | Test loss: 0.16891804337501526\n",
            "Epoch: 25980 | Train loss: 0.07132603973150253 | Test loss: 0.1687583178281784\n",
            "Epoch: 26000 | Train loss: 0.07125753164291382 | Test loss: 0.16859333217144012\n",
            "Epoch: 26020 | Train loss: 0.07118900865316391 | Test loss: 0.16843359172344208\n",
            "Epoch: 26040 | Train loss: 0.0711204931139946 | Test loss: 0.16827218234539032\n",
            "Epoch: 26060 | Train loss: 0.07105197757482529 | Test loss: 0.1681090146303177\n",
            "Epoch: 26080 | Train loss: 0.07098345458507538 | Test loss: 0.16794583201408386\n",
            "Epoch: 26100 | Train loss: 0.07091493904590607 | Test loss: 0.1677844226360321\n",
            "Epoch: 26120 | Train loss: 0.07084641605615616 | Test loss: 0.16762302815914154\n",
            "Epoch: 26140 | Train loss: 0.07077790051698685 | Test loss: 0.16746149957180023\n",
            "Epoch: 26160 | Train loss: 0.07070937752723694 | Test loss: 0.1672983020544052\n",
            "Epoch: 26180 | Train loss: 0.07064086943864822 | Test loss: 0.1671367734670639\n",
            "Epoch: 26200 | Train loss: 0.07057233899831772 | Test loss: 0.16697537899017334\n",
            "Epoch: 26220 | Train loss: 0.07050382345914841 | Test loss: 0.16681230068206787\n",
            "Epoch: 26240 | Train loss: 0.0704353004693985 | Test loss: 0.16665080189704895\n",
            "Epoch: 26260 | Train loss: 0.07036679238080978 | Test loss: 0.16648760437965393\n",
            "Epoch: 26280 | Train loss: 0.07029827684164047 | Test loss: 0.16632607579231262\n",
            "Epoch: 26300 | Train loss: 0.07022976875305176 | Test loss: 0.1661628931760788\n",
            "Epoch: 26320 | Train loss: 0.07016123831272125 | Test loss: 0.16599981486797333\n",
            "Epoch: 26340 | Train loss: 0.07009271532297134 | Test loss: 0.16584008932113647\n",
            "Epoch: 26360 | Train loss: 0.07002420723438263 | Test loss: 0.1656751185655594\n",
            "Epoch: 26380 | Train loss: 0.06995568424463272 | Test loss: 0.16551537811756134\n",
            "Epoch: 26400 | Train loss: 0.069887176156044 | Test loss: 0.16535383462905884\n",
            "Epoch: 26420 | Train loss: 0.0698186382651329 | Test loss: 0.16519078612327576\n",
            "Epoch: 26440 | Train loss: 0.06975014507770538 | Test loss: 0.16502760350704193\n",
            "Epoch: 26460 | Train loss: 0.06968160718679428 | Test loss: 0.16486607491970062\n",
            "Epoch: 26480 | Train loss: 0.06961309909820557 | Test loss: 0.16470301151275635\n",
            "Epoch: 26500 | Train loss: 0.06954457610845566 | Test loss: 0.1645432710647583\n",
            "Epoch: 26520 | Train loss: 0.06947606056928635 | Test loss: 0.16438007354736328\n",
            "Epoch: 26540 | Train loss: 0.06940754503011703 | Test loss: 0.16421855986118317\n",
            "Epoch: 26560 | Train loss: 0.06933902949094772 | Test loss: 0.16405537724494934\n",
            "Epoch: 26580 | Train loss: 0.06927050650119781 | Test loss: 0.16389396786689758\n",
            "Epoch: 26600 | Train loss: 0.0692019835114479 | Test loss: 0.16373255848884583\n",
            "Epoch: 26620 | Train loss: 0.06913347542285919 | Test loss: 0.16356761753559113\n",
            "Epoch: 26640 | Train loss: 0.06906495243310928 | Test loss: 0.1634078472852707\n",
            "Epoch: 26660 | Train loss: 0.06899642944335938 | Test loss: 0.16324634850025177\n",
            "Epoch: 26680 | Train loss: 0.06892792135477066 | Test loss: 0.16308148205280304\n",
            "Epoch: 26700 | Train loss: 0.06885939836502075 | Test loss: 0.16292186081409454\n",
            "Epoch: 26720 | Train loss: 0.06879087537527084 | Test loss: 0.16275855898857117\n",
            "Epoch: 26740 | Train loss: 0.06872235983610153 | Test loss: 0.1625971496105194\n",
            "Epoch: 26760 | Train loss: 0.06865384429693222 | Test loss: 0.1624356210231781\n",
            "Epoch: 26780 | Train loss: 0.0685853362083435 | Test loss: 0.16227243840694427\n",
            "Epoch: 26800 | Train loss: 0.0685168132185936 | Test loss: 0.162109375\n",
            "Epoch: 26820 | Train loss: 0.06844829022884369 | Test loss: 0.1619478464126587\n",
            "Epoch: 26840 | Train loss: 0.06837977468967438 | Test loss: 0.16178464889526367\n",
            "Epoch: 26860 | Train loss: 0.06831125915050507 | Test loss: 0.16162490844726562\n",
            "Epoch: 26880 | Train loss: 0.06824275106191635 | Test loss: 0.16145995259284973\n",
            "Epoch: 26900 | Train loss: 0.06817422062158585 | Test loss: 0.16130033135414124\n",
            "Epoch: 26920 | Train loss: 0.06810569763183594 | Test loss: 0.1611371487379074\n",
            "Epoch: 26940 | Train loss: 0.06803719699382782 | Test loss: 0.1609756201505661\n",
            "Epoch: 26960 | Train loss: 0.06796866655349731 | Test loss: 0.16081254184246063\n",
            "Epoch: 26980 | Train loss: 0.0679001435637474 | Test loss: 0.1606493592262268\n",
            "Epoch: 27000 | Train loss: 0.0678316131234169 | Test loss: 0.16048961877822876\n",
            "Epoch: 27020 | Train loss: 0.06776311248540878 | Test loss: 0.16032810509204865\n",
            "Epoch: 27040 | Train loss: 0.06769460439682007 | Test loss: 0.16016492247581482\n",
            "Epoch: 27060 | Train loss: 0.06762608140707016 | Test loss: 0.16000352799892426\n",
            "Epoch: 27080 | Train loss: 0.06755753606557846 | Test loss: 0.15984033048152924\n",
            "Epoch: 27100 | Train loss: 0.06748905032873154 | Test loss: 0.1596771478652954\n",
            "Epoch: 27120 | Train loss: 0.06742053478956223 | Test loss: 0.15951739251613617\n",
            "Epoch: 27140 | Train loss: 0.06735201179981232 | Test loss: 0.15935243666172028\n",
            "Epoch: 27160 | Train loss: 0.0672835037112236 | Test loss: 0.1591910570859909\n",
            "Epoch: 27180 | Train loss: 0.0672149658203125 | Test loss: 0.15902963280677795\n",
            "Epoch: 27200 | Train loss: 0.06714645773172379 | Test loss: 0.15886810421943665\n",
            "Epoch: 27220 | Train loss: 0.06707793474197388 | Test loss: 0.1587066948413849\n",
            "Epoch: 27240 | Train loss: 0.06700941175222397 | Test loss: 0.1585433930158615\n",
            "Epoch: 27260 | Train loss: 0.06694090366363525 | Test loss: 0.15838198363780975\n",
            "Epoch: 27280 | Train loss: 0.06687238067388535 | Test loss: 0.15821890532970428\n",
            "Epoch: 27300 | Train loss: 0.06680387258529663 | Test loss: 0.15805739164352417\n",
            "Epoch: 27320 | Train loss: 0.06673534959554672 | Test loss: 0.15789420902729034\n",
            "Epoch: 27340 | Train loss: 0.06666683405637741 | Test loss: 0.15773268043994904\n",
            "Epoch: 27360 | Train loss: 0.0665983334183693 | Test loss: 0.1575695127248764\n",
            "Epoch: 27380 | Train loss: 0.0665297880768776 | Test loss: 0.1574098765850067\n",
            "Epoch: 27400 | Train loss: 0.06646127998828888 | Test loss: 0.1572466790676117\n",
            "Epoch: 27420 | Train loss: 0.06639274954795837 | Test loss: 0.1570851355791092\n",
            "Epoch: 27440 | Train loss: 0.06632423400878906 | Test loss: 0.15692190825939178\n",
            "Epoch: 27460 | Train loss: 0.06625571101903915 | Test loss: 0.15676048398017883\n",
            "Epoch: 27480 | Train loss: 0.06618718057870865 | Test loss: 0.1565990447998047\n",
            "Epoch: 27500 | Train loss: 0.06611865013837814 | Test loss: 0.15643595159053802\n",
            "Epoch: 27520 | Train loss: 0.06605012714862823 | Test loss: 0.15627285838127136\n",
            "Epoch: 27540 | Train loss: 0.06598158925771713 | Test loss: 0.1561097502708435\n",
            "Epoch: 27560 | Train loss: 0.06591305881738663 | Test loss: 0.15595011413097382\n",
            "Epoch: 27580 | Train loss: 0.06584452837705612 | Test loss: 0.15578702092170715\n",
            "Epoch: 27600 | Train loss: 0.06577599793672562 | Test loss: 0.1556239128112793\n",
            "Epoch: 27620 | Train loss: 0.06570746749639511 | Test loss: 0.15546415746212006\n",
            "Epoch: 27640 | Train loss: 0.0656389370560646 | Test loss: 0.1553010493516922\n",
            "Epoch: 27660 | Train loss: 0.0655704066157341 | Test loss: 0.15513797104358673\n",
            "Epoch: 27680 | Train loss: 0.06550188362598419 | Test loss: 0.15497486293315887\n",
            "Epoch: 27700 | Train loss: 0.06543334573507309 | Test loss: 0.15481522679328918\n",
            "Epoch: 27720 | Train loss: 0.06536481529474258 | Test loss: 0.15465211868286133\n",
            "Epoch: 27740 | Train loss: 0.06529629230499268 | Test loss: 0.15448902547359467\n",
            "Epoch: 27760 | Train loss: 0.06522776186466217 | Test loss: 0.1543276160955429\n",
            "Epoch: 27780 | Train loss: 0.06515922397375107 | Test loss: 0.15416614711284637\n",
            "Epoch: 27800 | Train loss: 0.06509070098400116 | Test loss: 0.1540030539035797\n",
            "Epoch: 27820 | Train loss: 0.06502217054367065 | Test loss: 0.15383997559547424\n",
            "Epoch: 27840 | Train loss: 0.06495364010334015 | Test loss: 0.15368030965328217\n",
            "Epoch: 27860 | Train loss: 0.06488510221242905 | Test loss: 0.1535172164440155\n",
            "Epoch: 27880 | Train loss: 0.06481657177209854 | Test loss: 0.15335410833358765\n",
            "Epoch: 27900 | Train loss: 0.06474806368350983 | Test loss: 0.1531926840543747\n",
            "Epoch: 27920 | Train loss: 0.06467951834201813 | Test loss: 0.15303124487400055\n",
            "Epoch: 27940 | Train loss: 0.06461098790168762 | Test loss: 0.1528681516647339\n",
            "Epoch: 27960 | Train loss: 0.06454246491193771 | Test loss: 0.15270505845546722\n",
            "Epoch: 27980 | Train loss: 0.06447392702102661 | Test loss: 0.15254540741443634\n",
            "Epoch: 28000 | Train loss: 0.0644053965806961 | Test loss: 0.15238229930400848\n",
            "Epoch: 28020 | Train loss: 0.0643368661403656 | Test loss: 0.15221922099590302\n",
            "Epoch: 28040 | Train loss: 0.0642683282494545 | Test loss: 0.15205612778663635\n",
            "Epoch: 28060 | Train loss: 0.06419980525970459 | Test loss: 0.15189634263515472\n",
            "Epoch: 28080 | Train loss: 0.06413128226995468 | Test loss: 0.15173326432704926\n",
            "Epoch: 28100 | Train loss: 0.06406275182962418 | Test loss: 0.1515701711177826\n",
            "Epoch: 28120 | Train loss: 0.06399420648813248 | Test loss: 0.15141050517559052\n",
            "Epoch: 28140 | Train loss: 0.06392568349838257 | Test loss: 0.15124742686748505\n",
            "Epoch: 28160 | Train loss: 0.06385715305805206 | Test loss: 0.15108433365821838\n",
            "Epoch: 28180 | Train loss: 0.06378863006830215 | Test loss: 0.15092122554779053\n",
            "Epoch: 28200 | Train loss: 0.06372009962797165 | Test loss: 0.15075980126857758\n",
            "Epoch: 28220 | Train loss: 0.06365156918764114 | Test loss: 0.15059836208820343\n",
            "Epoch: 28240 | Train loss: 0.06358303874731064 | Test loss: 0.15043526887893677\n",
            "Epoch: 28260 | Train loss: 0.06351450085639954 | Test loss: 0.15027561783790588\n",
            "Epoch: 28280 | Train loss: 0.06344597786664963 | Test loss: 0.15011250972747803\n",
            "Epoch: 28300 | Train loss: 0.06337743997573853 | Test loss: 0.14994941651821136\n",
            "Epoch: 28320 | Train loss: 0.06330891698598862 | Test loss: 0.1497863382101059\n",
            "Epoch: 28340 | Train loss: 0.06324037164449692 | Test loss: 0.1496267020702362\n",
            "Epoch: 28360 | Train loss: 0.0631718561053276 | Test loss: 0.14946362376213074\n",
            "Epoch: 28380 | Train loss: 0.0631033256649971 | Test loss: 0.14930053055286407\n",
            "Epoch: 28400 | Train loss: 0.06303480267524719 | Test loss: 0.14913733303546906\n",
            "Epoch: 28420 | Train loss: 0.06296628713607788 | Test loss: 0.14897756278514862\n",
            "Epoch: 28440 | Train loss: 0.06289776414632797 | Test loss: 0.1488160341978073\n",
            "Epoch: 28460 | Train loss: 0.06282924115657806 | Test loss: 0.14865295588970184\n",
            "Epoch: 28480 | Train loss: 0.06276071071624756 | Test loss: 0.14848974347114563\n",
            "Epoch: 28500 | Train loss: 0.06269218772649765 | Test loss: 0.14832821488380432\n",
            "Epoch: 28520 | Train loss: 0.06262367963790894 | Test loss: 0.1481650173664093\n",
            "Epoch: 28540 | Train loss: 0.06255515664815903 | Test loss: 0.14800535142421722\n",
            "Epoch: 28560 | Train loss: 0.06248664855957031 | Test loss: 0.14784227311611176\n",
            "Epoch: 28580 | Train loss: 0.06241810321807861 | Test loss: 0.14767907559871674\n",
            "Epoch: 28600 | Train loss: 0.062349583953619 | Test loss: 0.14751754701137543\n",
            "Epoch: 28620 | Train loss: 0.06228107213973999 | Test loss: 0.1473543494939804\n",
            "Epoch: 28640 | Train loss: 0.062212515622377396 | Test loss: 0.14719468355178833\n",
            "Epoch: 28660 | Train loss: 0.062144018709659576 | Test loss: 0.1470315009355545\n",
            "Epoch: 28680 | Train loss: 0.062075503170490265 | Test loss: 0.1468699425458908\n",
            "Epoch: 28700 | Train loss: 0.06200698763132095 | Test loss: 0.1467067450284958\n",
            "Epoch: 28720 | Train loss: 0.06193845346570015 | Test loss: 0.14654366672039032\n",
            "Epoch: 28740 | Train loss: 0.061869919300079346 | Test loss: 0.14638403058052063\n",
            "Epoch: 28760 | Train loss: 0.061801400035619736 | Test loss: 0.1462208330631256\n",
            "Epoch: 28780 | Train loss: 0.06173288822174072 | Test loss: 0.1460592895746231\n",
            "Epoch: 28800 | Train loss: 0.06166436895728111 | Test loss: 0.14589612185955048\n",
            "Epoch: 28820 | Train loss: 0.06159583851695061 | Test loss: 0.14573299884796143\n",
            "Epoch: 28840 | Train loss: 0.0615273118019104 | Test loss: 0.14557324349880219\n",
            "Epoch: 28860 | Train loss: 0.06145879626274109 | Test loss: 0.14540992677211761\n",
            "Epoch: 28880 | Train loss: 0.06139028072357178 | Test loss: 0.14524850249290466\n",
            "Epoch: 28900 | Train loss: 0.06132175400853157 | Test loss: 0.145085409283638\n",
            "Epoch: 28920 | Train loss: 0.06125323846936226 | Test loss: 0.1449238806962967\n",
            "Epoch: 28940 | Train loss: 0.06118472293019295 | Test loss: 0.1447623372077942\n",
            "Epoch: 28960 | Train loss: 0.061116188764572144 | Test loss: 0.14460091292858124\n",
            "Epoch: 28980 | Train loss: 0.06104766204953194 | Test loss: 0.14443783462047577\n",
            "Epoch: 29000 | Train loss: 0.060979146510362625 | Test loss: 0.14427629113197327\n",
            "Epoch: 29020 | Train loss: 0.060910630971193314 | Test loss: 0.1441112607717514\n",
            "Epoch: 29040 | Train loss: 0.060842107981443405 | Test loss: 0.14394989609718323\n",
            "Epoch: 29060 | Train loss: 0.0607735775411129 | Test loss: 0.14378848671913147\n",
            "Epoch: 29080 | Train loss: 0.06070506572723389 | Test loss: 0.14362692832946777\n",
            "Epoch: 29100 | Train loss: 0.06063655763864517 | Test loss: 0.14346371591091156\n",
            "Epoch: 29120 | Train loss: 0.06056802347302437 | Test loss: 0.1433023065328598\n",
            "Epoch: 29140 | Train loss: 0.06049949675798416 | Test loss: 0.14314089715480804\n",
            "Epoch: 29160 | Train loss: 0.06043098121881485 | Test loss: 0.14297933876514435\n",
            "Epoch: 29180 | Train loss: 0.06036245822906494 | Test loss: 0.14281611144542694\n",
            "Epoch: 29200 | Train loss: 0.06029393896460533 | Test loss: 0.14265473186969757\n",
            "Epoch: 29220 | Train loss: 0.06022540479898453 | Test loss: 0.1424916386604309\n",
            "Epoch: 29240 | Train loss: 0.06015688180923462 | Test loss: 0.14233021438121796\n",
            "Epoch: 29260 | Train loss: 0.06008836254477501 | Test loss: 0.14216868579387665\n",
            "Epoch: 29280 | Train loss: 0.0600198470056057 | Test loss: 0.14200548827648163\n",
            "Epoch: 29300 | Train loss: 0.05995131656527519 | Test loss: 0.14184406399726868\n",
            "Epoch: 29320 | Train loss: 0.05988279730081558 | Test loss: 0.14168097078800201\n",
            "Epoch: 29340 | Train loss: 0.059814270585775375 | Test loss: 0.14151956140995026\n",
            "Epoch: 29360 | Train loss: 0.05974574759602547 | Test loss: 0.14135800302028656\n",
            "Epoch: 29380 | Train loss: 0.059677232056856155 | Test loss: 0.14119480550289154\n",
            "Epoch: 29400 | Train loss: 0.05960870906710625 | Test loss: 0.14103339612483978\n",
            "Epoch: 29420 | Train loss: 0.05954017862677574 | Test loss: 0.14087019860744476\n",
            "Epoch: 29440 | Train loss: 0.05947166681289673 | Test loss: 0.14070700109004974\n",
            "Epoch: 29460 | Train loss: 0.05940314754843712 | Test loss: 0.1405472308397293\n",
            "Epoch: 29480 | Train loss: 0.05933462455868721 | Test loss: 0.14038236439228058\n",
            "Epoch: 29500 | Train loss: 0.0592661015689373 | Test loss: 0.14022259414196014\n",
            "Epoch: 29520 | Train loss: 0.05919758230447769 | Test loss: 0.14005939662456512\n",
            "Epoch: 29540 | Train loss: 0.05912909656763077 | Test loss: 0.13989786803722382\n",
            "Epoch: 29560 | Train loss: 0.05906054005026817 | Test loss: 0.13973478972911835\n",
            "Epoch: 29580 | Train loss: 0.058992017060518265 | Test loss: 0.13957157731056213\n",
            "Epoch: 29600 | Train loss: 0.058923494070768356 | Test loss: 0.1394118368625641\n",
            "Epoch: 29620 | Train loss: 0.05885496363043785 | Test loss: 0.1392502784729004\n",
            "Epoch: 29640 | Train loss: 0.05878645181655884 | Test loss: 0.13908720016479492\n",
            "Epoch: 29660 | Train loss: 0.05871793255209923 | Test loss: 0.1389240026473999\n",
            "Epoch: 29680 | Train loss: 0.05864940956234932 | Test loss: 0.1387624591588974\n",
            "Epoch: 29700 | Train loss: 0.058580901473760605 | Test loss: 0.13859926164150238\n",
            "Epoch: 29720 | Train loss: 0.058512359857559204 | Test loss: 0.1384396106004715\n",
            "Epoch: 29740 | Train loss: 0.05844384431838989 | Test loss: 0.13827641308307648\n",
            "Epoch: 29760 | Train loss: 0.05837532877922058 | Test loss: 0.13811486959457397\n",
            "Epoch: 29780 | Train loss: 0.05830680951476097 | Test loss: 0.13795171678066254\n",
            "Epoch: 29800 | Train loss: 0.058238279074430466 | Test loss: 0.1377885937690735\n",
            "Epoch: 29820 | Train loss: 0.05816974863409996 | Test loss: 0.1376289576292038\n",
            "Epoch: 29840 | Train loss: 0.05810122564435005 | Test loss: 0.13746574521064758\n",
            "Epoch: 29860 | Train loss: 0.05803271010518074 | Test loss: 0.13730421662330627\n",
            "Epoch: 29880 | Train loss: 0.05796419456601143 | Test loss: 0.13714098930358887\n",
            "Epoch: 29900 | Train loss: 0.05789567157626152 | Test loss: 0.1369779109954834\n",
            "Epoch: 29920 | Train loss: 0.057827141135931015 | Test loss: 0.13681815564632416\n",
            "Epoch: 29940 | Train loss: 0.057758629322052 | Test loss: 0.1366548389196396\n",
            "Epoch: 29960 | Train loss: 0.05769011005759239 | Test loss: 0.13649342954158783\n",
            "Epoch: 29980 | Train loss: 0.05762157961726189 | Test loss: 0.13633033633232117\n",
            "Epoch: 30000 | Train loss: 0.05755305290222168 | Test loss: 0.13616721332073212\n",
            "Epoch: 30020 | Train loss: 0.05748453363776207 | Test loss: 0.13600750267505646\n",
            "Epoch: 30040 | Train loss: 0.05741601064801216 | Test loss: 0.1358441710472107\n",
            "Epoch: 30060 | Train loss: 0.05734748765826225 | Test loss: 0.13568274676799774\n",
            "Epoch: 30080 | Train loss: 0.05727896839380264 | Test loss: 0.13551966845989227\n",
            "Epoch: 30100 | Train loss: 0.057210445404052734 | Test loss: 0.13535812497138977\n",
            "Epoch: 30120 | Train loss: 0.057141926139593124 | Test loss: 0.13519658148288727\n",
            "Epoch: 30140 | Train loss: 0.057073403149843216 | Test loss: 0.13503515720367432\n",
            "Epoch: 30160 | Train loss: 0.05700486898422241 | Test loss: 0.13487206399440765\n",
            "Epoch: 30180 | Train loss: 0.0569363497197628 | Test loss: 0.134708970785141\n",
            "Epoch: 30200 | Train loss: 0.056867815554142 | Test loss: 0.13454586267471313\n",
            "Epoch: 30220 | Train loss: 0.0567992702126503 | Test loss: 0.13438622653484344\n",
            "Epoch: 30240 | Train loss: 0.05673074722290039 | Test loss: 0.13422313332557678\n",
            "Epoch: 30260 | Train loss: 0.05666223168373108 | Test loss: 0.13405990600585938\n",
            "Epoch: 30280 | Train loss: 0.05659369379281998 | Test loss: 0.1338968127965927\n",
            "Epoch: 30300 | Train loss: 0.05652515962719917 | Test loss: 0.13373716175556183\n",
            "Epoch: 30320 | Train loss: 0.056456636637449265 | Test loss: 0.13357405364513397\n",
            "Epoch: 30340 | Train loss: 0.05638810619711876 | Test loss: 0.1334109753370285\n",
            "Epoch: 30360 | Train loss: 0.056319572031497955 | Test loss: 0.13325132429599762\n",
            "Epoch: 30380 | Train loss: 0.05625104531645775 | Test loss: 0.1330881267786026\n",
            "Epoch: 30400 | Train loss: 0.05618251487612724 | Test loss: 0.13292501866817474\n",
            "Epoch: 30420 | Train loss: 0.05611398443579674 | Test loss: 0.13276192545890808\n",
            "Epoch: 30440 | Train loss: 0.056045450270175934 | Test loss: 0.132602259516716\n",
            "Epoch: 30460 | Train loss: 0.05597692355513573 | Test loss: 0.13243916630744934\n",
            "Epoch: 30480 | Train loss: 0.05590839311480522 | Test loss: 0.13227607309818268\n",
            "Epoch: 30500 | Train loss: 0.055839862674474716 | Test loss: 0.1321164220571518\n",
            "Epoch: 30520 | Train loss: 0.05577133968472481 | Test loss: 0.13195155560970306\n",
            "Epoch: 30540 | Train loss: 0.055702805519104004 | Test loss: 0.13179011642932892\n",
            "Epoch: 30560 | Train loss: 0.0556342788040638 | Test loss: 0.13162700831890106\n",
            "Epoch: 30580 | Train loss: 0.055565740913152695 | Test loss: 0.13146737217903137\n",
            "Epoch: 30600 | Train loss: 0.05549721047282219 | Test loss: 0.13130424916744232\n",
            "Epoch: 30620 | Train loss: 0.05542868375778198 | Test loss: 0.13114118576049805\n",
            "Epoch: 30640 | Train loss: 0.05536014959216118 | Test loss: 0.13097809255123138\n",
            "Epoch: 30660 | Train loss: 0.05529163032770157 | Test loss: 0.1308184117078781\n",
            "Epoch: 30680 | Train loss: 0.055223096162080765 | Test loss: 0.1306552141904831\n",
            "Epoch: 30700 | Train loss: 0.05515456944704056 | Test loss: 0.13049213588237762\n",
            "Epoch: 30720 | Train loss: 0.05508602783083916 | Test loss: 0.13033246994018555\n",
            "Epoch: 30740 | Train loss: 0.05501749739050865 | Test loss: 0.13016937673091888\n",
            "Epoch: 30760 | Train loss: 0.05494897440075874 | Test loss: 0.13000628352165222\n",
            "Epoch: 30780 | Train loss: 0.05488044023513794 | Test loss: 0.12984319031238556\n",
            "Epoch: 30800 | Train loss: 0.054811906069517136 | Test loss: 0.12968353927135468\n",
            "Epoch: 30820 | Train loss: 0.05474337562918663 | Test loss: 0.12952034175395966\n",
            "Epoch: 30840 | Train loss: 0.05467486009001732 | Test loss: 0.1293572187423706\n",
            "Epoch: 30860 | Train loss: 0.05460631847381592 | Test loss: 0.12919758260250092\n",
            "Epoch: 30880 | Train loss: 0.05453779175877571 | Test loss: 0.12903448939323425\n",
            "Epoch: 30900 | Train loss: 0.054469265043735504 | Test loss: 0.1288713812828064\n",
            "Epoch: 30920 | Train loss: 0.0544007308781147 | Test loss: 0.12870828807353973\n",
            "Epoch: 30940 | Train loss: 0.054332196712493896 | Test loss: 0.12854862213134766\n",
            "Epoch: 30960 | Train loss: 0.054263677448034286 | Test loss: 0.12838375568389893\n",
            "Epoch: 30980 | Train loss: 0.05419514328241348 | Test loss: 0.12822233140468597\n",
            "Epoch: 31000 | Train loss: 0.05412660166621208 | Test loss: 0.1280626803636551\n",
            "Epoch: 31020 | Train loss: 0.05405808240175247 | Test loss: 0.12789957225322723\n",
            "Epoch: 31040 | Train loss: 0.053989555686712265 | Test loss: 0.12773647904396057\n",
            "Epoch: 31060 | Train loss: 0.05392102152109146 | Test loss: 0.1275734007358551\n",
            "Epoch: 31080 | Train loss: 0.053852494806051254 | Test loss: 0.1274137794971466\n",
            "Epoch: 31100 | Train loss: 0.05378397926688194 | Test loss: 0.12725059688091278\n",
            "Epoch: 31120 | Train loss: 0.05371546372771263 | Test loss: 0.12708906829357147\n",
            "Epoch: 31140 | Train loss: 0.05364694818854332 | Test loss: 0.12692587077617645\n",
            "Epoch: 31160 | Train loss: 0.05357843637466431 | Test loss: 0.12676434218883514\n",
            "Epoch: 31180 | Train loss: 0.0535099096596241 | Test loss: 0.12660129368305206\n",
            "Epoch: 31200 | Train loss: 0.05344138666987419 | Test loss: 0.12643809616565704\n",
            "Epoch: 31220 | Train loss: 0.05337287113070488 | Test loss: 0.126278355717659\n",
            "Epoch: 31240 | Train loss: 0.053304363042116165 | Test loss: 0.12611684203147888\n",
            "Epoch: 31260 | Train loss: 0.05323584005236626 | Test loss: 0.12595365941524506\n",
            "Epoch: 31280 | Train loss: 0.05316732078790665 | Test loss: 0.1257922500371933\n",
            "Epoch: 31300 | Train loss: 0.05309879779815674 | Test loss: 0.12562905251979828\n",
            "Epoch: 31320 | Train loss: 0.053030289709568024 | Test loss: 0.12546586990356445\n",
            "Epoch: 31340 | Train loss: 0.052961766719818115 | Test loss: 0.1253044754266739\n",
            "Epoch: 31360 | Train loss: 0.052893247455358505 | Test loss: 0.12514306604862213\n",
            "Epoch: 31380 | Train loss: 0.0528247244656086 | Test loss: 0.12498153746128082\n",
            "Epoch: 31400 | Train loss: 0.05275621637701988 | Test loss: 0.1248183622956276\n",
            "Epoch: 31420 | Train loss: 0.05268770456314087 | Test loss: 0.12465681880712509\n",
            "Epoch: 31440 | Train loss: 0.052619170397520065 | Test loss: 0.12449543923139572\n",
            "Epoch: 31460 | Train loss: 0.052550654858350754 | Test loss: 0.12433236837387085\n",
            "Epoch: 31480 | Train loss: 0.052482135593891144 | Test loss: 0.12417083233594894\n",
            "Epoch: 31500 | Train loss: 0.05241362378001213 | Test loss: 0.12400765717029572\n",
            "Epoch: 31520 | Train loss: 0.05234510824084282 | Test loss: 0.1238461285829544\n",
            "Epoch: 31540 | Train loss: 0.05227659270167351 | Test loss: 0.12368293106555939\n",
            "Epoch: 31560 | Train loss: 0.0522080659866333 | Test loss: 0.12351987510919571\n",
            "Epoch: 31580 | Train loss: 0.05213954672217369 | Test loss: 0.12336013466119766\n",
            "Epoch: 31600 | Train loss: 0.05207103490829468 | Test loss: 0.12319862097501755\n",
            "Epoch: 31620 | Train loss: 0.052002519369125366 | Test loss: 0.12303543090820312\n",
            "Epoch: 31640 | Train loss: 0.051934003829956055 | Test loss: 0.12287390232086182\n",
            "Epoch: 31660 | Train loss: 0.051865484565496445 | Test loss: 0.12271082401275635\n",
            "Epoch: 31680 | Train loss: 0.05179697275161743 | Test loss: 0.12254764884710312\n",
            "Epoch: 31700 | Train loss: 0.05172843858599663 | Test loss: 0.12238790839910507\n",
            "Epoch: 31720 | Train loss: 0.05165993049740791 | Test loss: 0.12222293764352798\n",
            "Epoch: 31740 | Train loss: 0.051591407507658005 | Test loss: 0.12206319719552994\n",
            "Epoch: 31760 | Train loss: 0.05152289196848869 | Test loss: 0.12190180271863937\n",
            "Epoch: 31780 | Train loss: 0.05145437642931938 | Test loss: 0.12173861265182495\n",
            "Epoch: 31800 | Train loss: 0.05138586089015007 | Test loss: 0.12157543003559113\n",
            "Epoch: 31820 | Train loss: 0.05131734162569046 | Test loss: 0.12141402065753937\n",
            "Epoch: 31840 | Train loss: 0.05124881491065025 | Test loss: 0.12125261127948761\n",
            "Epoch: 31860 | Train loss: 0.05118029937148094 | Test loss: 0.1210910901427269\n",
            "Epoch: 31880 | Train loss: 0.05111178383231163 | Test loss: 0.12092790752649307\n",
            "Epoch: 31900 | Train loss: 0.05104327201843262 | Test loss: 0.12076637893915176\n",
            "Epoch: 31920 | Train loss: 0.05097474530339241 | Test loss: 0.1206049695611\n",
            "Epoch: 31940 | Train loss: 0.0509062297642231 | Test loss: 0.12044190615415573\n",
            "Epoch: 31960 | Train loss: 0.05083770677447319 | Test loss: 0.12028038501739502\n",
            "Epoch: 31980 | Train loss: 0.050769198685884476 | Test loss: 0.1201172024011612\n",
            "Epoch: 32000 | Train loss: 0.050700683146715164 | Test loss: 0.11995568126440048\n",
            "Epoch: 32020 | Train loss: 0.05063216760754585 | Test loss: 0.11979251354932785\n",
            "Epoch: 32040 | Train loss: 0.050563644617795944 | Test loss: 0.11962943524122238\n",
            "Epoch: 32060 | Train loss: 0.050495125353336334 | Test loss: 0.11946969479322433\n",
            "Epoch: 32080 | Train loss: 0.050426602363586426 | Test loss: 0.11930470913648605\n",
            "Epoch: 32100 | Train loss: 0.05035809427499771 | Test loss: 0.1191449761390686\n",
            "Epoch: 32120 | Train loss: 0.0502895824611187 | Test loss: 0.1189834401011467\n",
            "Epoch: 32140 | Train loss: 0.05022105202078819 | Test loss: 0.11882038414478302\n",
            "Epoch: 32160 | Train loss: 0.05015253648161888 | Test loss: 0.11865720897912979\n",
            "Epoch: 32180 | Train loss: 0.05008402094244957 | Test loss: 0.11849568039178848\n",
            "Epoch: 32200 | Train loss: 0.05001549795269966 | Test loss: 0.11833260208368301\n",
            "Epoch: 32220 | Train loss: 0.04994697868824005 | Test loss: 0.11817286163568497\n",
            "Epoch: 32240 | Train loss: 0.04987845942378044 | Test loss: 0.11800967901945114\n",
            "Epoch: 32260 | Train loss: 0.04980995133519173 | Test loss: 0.11784813553094864\n",
            "Epoch: 32280 | Train loss: 0.049741439521312714 | Test loss: 0.11768496036529541\n",
            "Epoch: 32300 | Train loss: 0.04967290908098221 | Test loss: 0.11752355098724365\n",
            "Epoch: 32320 | Train loss: 0.049604382365942 | Test loss: 0.1173621416091919\n",
            "Epoch: 32340 | Train loss: 0.04953587427735329 | Test loss: 0.1171971783041954\n",
            "Epoch: 32360 | Train loss: 0.049467358738183975 | Test loss: 0.11703746765851974\n",
            "Epoch: 32380 | Train loss: 0.049398843199014664 | Test loss: 0.11687592417001724\n",
            "Epoch: 32400 | Train loss: 0.049330323934555054 | Test loss: 0.1167110726237297\n",
            "Epoch: 32420 | Train loss: 0.04926179349422455 | Test loss: 0.11655145138502121\n",
            "Epoch: 32440 | Train loss: 0.049193285405635834 | Test loss: 0.11638815701007843\n",
            "Epoch: 32460 | Train loss: 0.049124766141176224 | Test loss: 0.11622674763202667\n",
            "Epoch: 32480 | Train loss: 0.04905625060200691 | Test loss: 0.11606521904468536\n",
            "Epoch: 32500 | Train loss: 0.0489877425134182 | Test loss: 0.11590204387903214\n",
            "Epoch: 32520 | Train loss: 0.04891921579837799 | Test loss: 0.11573896557092667\n",
            "Epoch: 32540 | Train loss: 0.048850689083337784 | Test loss: 0.11557743698358536\n",
            "Epoch: 32560 | Train loss: 0.048782188445329666 | Test loss: 0.11541423946619034\n",
            "Epoch: 32580 | Train loss: 0.04871366545557976 | Test loss: 0.11525452136993408\n",
            "Epoch: 32600 | Train loss: 0.048645149916410446 | Test loss: 0.115089550614357\n",
            "Epoch: 32620 | Train loss: 0.04857662692666054 | Test loss: 0.1149299144744873\n",
            "Epoch: 32640 | Train loss: 0.04850810393691063 | Test loss: 0.11476673930883408\n",
            "Epoch: 32660 | Train loss: 0.048439595848321915 | Test loss: 0.11460521072149277\n",
            "Epoch: 32680 | Train loss: 0.048371072858572006 | Test loss: 0.11444216221570969\n",
            "Epoch: 32700 | Train loss: 0.0483025461435318 | Test loss: 0.11427896469831467\n",
            "Epoch: 32720 | Train loss: 0.048234034329652786 | Test loss: 0.11411922425031662\n",
            "Epoch: 32740 | Train loss: 0.048165515065193176 | Test loss: 0.11395769566297531\n",
            "Epoch: 32760 | Train loss: 0.04809700697660446 | Test loss: 0.11379452049732208\n",
            "Epoch: 32780 | Train loss: 0.048028480261564255 | Test loss: 0.11363311111927032\n",
            "Epoch: 32800 | Train loss: 0.047959960997104645 | Test loss: 0.1134699136018753\n",
            "Epoch: 32820 | Train loss: 0.04789144918322563 | Test loss: 0.11330673843622208\n",
            "Epoch: 32840 | Train loss: 0.047822922468185425 | Test loss: 0.11314699798822403\n",
            "Epoch: 32860 | Train loss: 0.04775441065430641 | Test loss: 0.11298387497663498\n",
            "Epoch: 32880 | Train loss: 0.0476858876645565 | Test loss: 0.11282080411911011\n",
            "Epoch: 32900 | Train loss: 0.0476173460483551 | Test loss: 0.11265768110752106\n",
            "Epoch: 32920 | Train loss: 0.0475488044321537 | Test loss: 0.11249800026416779\n",
            "Epoch: 32940 | Train loss: 0.0474802665412426 | Test loss: 0.11233661323785782\n",
            "Epoch: 32960 | Train loss: 0.04741174355149269 | Test loss: 0.1121717318892479\n",
            "Epoch: 32980 | Train loss: 0.047343190759420395 | Test loss: 0.11201205104589462\n",
            "Epoch: 33000 | Train loss: 0.04727466031908989 | Test loss: 0.11184892803430557\n",
            "Epoch: 33020 | Train loss: 0.04720611497759819 | Test loss: 0.11168751865625381\n",
            "Epoch: 33040 | Train loss: 0.047137584537267685 | Test loss: 0.11152439564466476\n",
            "Epoch: 33060 | Train loss: 0.04706904664635658 | Test loss: 0.11136126518249512\n",
            "Epoch: 33080 | Train loss: 0.04700050875544548 | Test loss: 0.11119814217090607\n",
            "Epoch: 33100 | Train loss: 0.04693197086453438 | Test loss: 0.1110384464263916\n",
            "Epoch: 33120 | Train loss: 0.046863432973623276 | Test loss: 0.11087532341480255\n",
            "Epoch: 33140 | Train loss: 0.04679490253329277 | Test loss: 0.1107121929526329\n",
            "Epoch: 33160 | Train loss: 0.04672635719180107 | Test loss: 0.11055248230695724\n",
            "Epoch: 33180 | Train loss: 0.046657826751470566 | Test loss: 0.11038937419652939\n",
            "Epoch: 33200 | Train loss: 0.046589285135269165 | Test loss: 0.11022796481847763\n",
            "Epoch: 33220 | Train loss: 0.04652075096964836 | Test loss: 0.11006485670804977\n",
            "Epoch: 33240 | Train loss: 0.04645221307873726 | Test loss: 0.10990171879529953\n",
            "Epoch: 33260 | Train loss: 0.04638366773724556 | Test loss: 0.10974031686782837\n",
            "Epoch: 33280 | Train loss: 0.046315133571624756 | Test loss: 0.10957890003919601\n",
            "Epoch: 33300 | Train loss: 0.04624659940600395 | Test loss: 0.10941578447818756\n",
            "Epoch: 33320 | Train loss: 0.04617806151509285 | Test loss: 0.10925266891717911\n",
            "Epoch: 33340 | Train loss: 0.04610952362418175 | Test loss: 0.10909297317266464\n",
            "Epoch: 33360 | Train loss: 0.046040989458560944 | Test loss: 0.10892985016107559\n",
            "Epoch: 33380 | Train loss: 0.04597244784235954 | Test loss: 0.10876842588186264\n",
            "Epoch: 33400 | Train loss: 0.04590391367673874 | Test loss: 0.10860531777143478\n",
            "Epoch: 33420 | Train loss: 0.04583537206053734 | Test loss: 0.10844390839338303\n",
            "Epoch: 33440 | Train loss: 0.04576682671904564 | Test loss: 0.10828075557947159\n",
            "Epoch: 33460 | Train loss: 0.045698296278715134 | Test loss: 0.10811936855316162\n",
            "Epoch: 33480 | Train loss: 0.04562976211309433 | Test loss: 0.10795623064041138\n",
            "Epoch: 33500 | Train loss: 0.045561227947473526 | Test loss: 0.10779311507940292\n",
            "Epoch: 33520 | Train loss: 0.045492686331272125 | Test loss: 0.10763342678546906\n",
            "Epoch: 33540 | Train loss: 0.04542415216565132 | Test loss: 0.10747029632329941\n",
            "Epoch: 33560 | Train loss: 0.04535561427474022 | Test loss: 0.10730717331171036\n",
            "Epoch: 33580 | Train loss: 0.04528704658150673 | Test loss: 0.10714405030012131\n",
            "Epoch: 33600 | Train loss: 0.04521853104233742 | Test loss: 0.10698435455560684\n",
            "Epoch: 33620 | Train loss: 0.045149993151426315 | Test loss: 0.10682123154401779\n",
            "Epoch: 33640 | Train loss: 0.04508145898580551 | Test loss: 0.10665982216596603\n",
            "Epoch: 33660 | Train loss: 0.04501292109489441 | Test loss: 0.10649670660495758\n",
            "Epoch: 33680 | Train loss: 0.044944390654563904 | Test loss: 0.10633356869220734\n",
            "Epoch: 33700 | Train loss: 0.044875841587781906 | Test loss: 0.10617389529943466\n",
            "Epoch: 33720 | Train loss: 0.0448073074221611 | Test loss: 0.10601074993610382\n",
            "Epoch: 33740 | Train loss: 0.0447387732565403 | Test loss: 0.10584761947393417\n",
            "Epoch: 33760 | Train loss: 0.044670235365629196 | Test loss: 0.10568449646234512\n",
            "Epoch: 33780 | Train loss: 0.04460170120000839 | Test loss: 0.10552483052015305\n",
            "Epoch: 33800 | Train loss: 0.04453315958380699 | Test loss: 0.10536167770624161\n",
            "Epoch: 33820 | Train loss: 0.04446462541818619 | Test loss: 0.10520028322935104\n",
            "Epoch: 33840 | Train loss: 0.044396087527275085 | Test loss: 0.10503716766834259\n",
            "Epoch: 33860 | Train loss: 0.04432755336165428 | Test loss: 0.10487402975559235\n",
            "Epoch: 33880 | Train loss: 0.044259004294872284 | Test loss: 0.10471434891223907\n",
            "Epoch: 33900 | Train loss: 0.04419047757983208 | Test loss: 0.10455121845006943\n",
            "Epoch: 33920 | Train loss: 0.044121935963630676 | Test loss: 0.10438808053731918\n",
            "Epoch: 33940 | Train loss: 0.044053398072719574 | Test loss: 0.10422497242689133\n",
            "Epoch: 33960 | Train loss: 0.04398486018180847 | Test loss: 0.10406527668237686\n",
            "Epoch: 33980 | Train loss: 0.04391631856560707 | Test loss: 0.10390215367078781\n",
            "Epoch: 34000 | Train loss: 0.043847788125276566 | Test loss: 0.10374074429273605\n",
            "Epoch: 34020 | Train loss: 0.043779246509075165 | Test loss: 0.1035793349146843\n",
            "Epoch: 34040 | Train loss: 0.04371071234345436 | Test loss: 0.10341620445251465\n",
            "Epoch: 34060 | Train loss: 0.04364217072725296 | Test loss: 0.10325479507446289\n",
            "Epoch: 34080 | Train loss: 0.043573636561632156 | Test loss: 0.10309167206287384\n",
            "Epoch: 34100 | Train loss: 0.043505098670721054 | Test loss: 0.1029285416007042\n",
            "Epoch: 34120 | Train loss: 0.04343656077980995 | Test loss: 0.10276541858911514\n",
            "Epoch: 34140 | Train loss: 0.04336802288889885 | Test loss: 0.10260572284460068\n",
            "Epoch: 34160 | Train loss: 0.04329947754740715 | Test loss: 0.10244262218475342\n",
            "Epoch: 34180 | Train loss: 0.04323095455765724 | Test loss: 0.10227947682142258\n",
            "Epoch: 34200 | Train loss: 0.043162405490875244 | Test loss: 0.1021197959780693\n",
            "Epoch: 34220 | Train loss: 0.043093882501125336 | Test loss: 0.10195668041706085\n",
            "Epoch: 34240 | Train loss: 0.043025337159633636 | Test loss: 0.1017952710390091\n",
            "Epoch: 34260 | Train loss: 0.042956795543432236 | Test loss: 0.10163213312625885\n",
            "Epoch: 34280 | Train loss: 0.04288826510310173 | Test loss: 0.1014690026640892\n",
            "Epoch: 34300 | Train loss: 0.04281972348690033 | Test loss: 0.10130587965250015\n",
            "Epoch: 34320 | Train loss: 0.04275118559598923 | Test loss: 0.10114619880914688\n",
            "Epoch: 34340 | Train loss: 0.042682647705078125 | Test loss: 0.10098307579755783\n",
            "Epoch: 34360 | Train loss: 0.04261411726474762 | Test loss: 0.10081993788480759\n",
            "Epoch: 34380 | Train loss: 0.04254556819796562 | Test loss: 0.10066025704145432\n",
            "Epoch: 34400 | Train loss: 0.04247703775763512 | Test loss: 0.10049712657928467\n",
            "Epoch: 34420 | Train loss: 0.042408499866724014 | Test loss: 0.10033571720123291\n",
            "Epoch: 34440 | Train loss: 0.04233996197581291 | Test loss: 0.10017260164022446\n",
            "Epoch: 34460 | Train loss: 0.04227142781019211 | Test loss: 0.10000946372747421\n",
            "Epoch: 34480 | Train loss: 0.042202889919281006 | Test loss: 0.09984805434942245\n",
            "Epoch: 34500 | Train loss: 0.042134348303079605 | Test loss: 0.0996866449713707\n",
            "Epoch: 34520 | Train loss: 0.0420658104121685 | Test loss: 0.09952352195978165\n",
            "Epoch: 34540 | Train loss: 0.041997279971838 | Test loss: 0.0993603989481926\n",
            "Epoch: 34560 | Train loss: 0.0419287346303463 | Test loss: 0.09920071810483932\n",
            "Epoch: 34580 | Train loss: 0.04186020791530609 | Test loss: 0.09903758019208908\n",
            "Epoch: 34600 | Train loss: 0.041791658848524094 | Test loss: 0.09887617826461792\n",
            "Epoch: 34620 | Train loss: 0.04172312840819359 | Test loss: 0.09871305525302887\n",
            "Epoch: 34640 | Train loss: 0.04165458306670189 | Test loss: 0.09855163097381592\n",
            "Epoch: 34660 | Train loss: 0.041586048901081085 | Test loss: 0.09838847815990448\n",
            "Epoch: 34680 | Train loss: 0.041517507284879684 | Test loss: 0.0982271060347557\n",
            "Epoch: 34700 | Train loss: 0.04144897311925888 | Test loss: 0.09806398302316666\n",
            "Epoch: 34720 | Train loss: 0.041380442678928375 | Test loss: 0.0979008600115776\n",
            "Epoch: 34740 | Train loss: 0.041311897337436676 | Test loss: 0.09774117171764374\n",
            "Epoch: 34760 | Train loss: 0.04124336689710617 | Test loss: 0.09757804125547409\n",
            "Epoch: 34780 | Train loss: 0.04117482155561447 | Test loss: 0.09741663187742233\n",
            "Epoch: 34800 | Train loss: 0.04110627621412277 | Test loss: 0.09725179523229599\n",
            "Epoch: 34820 | Train loss: 0.04103774577379227 | Test loss: 0.09709209948778152\n",
            "Epoch: 34840 | Train loss: 0.040969207882881165 | Test loss: 0.09692896902561188\n",
            "Epoch: 34860 | Train loss: 0.04090067371726036 | Test loss: 0.09676756709814072\n",
            "Epoch: 34880 | Train loss: 0.040832143276929855 | Test loss: 0.09660445153713226\n",
            "Epoch: 34900 | Train loss: 0.040763597935438156 | Test loss: 0.09644131362438202\n",
            "Epoch: 34920 | Train loss: 0.04069506376981735 | Test loss: 0.09628163278102875\n",
            "Epoch: 34940 | Train loss: 0.04062653332948685 | Test loss: 0.09611676633358002\n",
            "Epoch: 34960 | Train loss: 0.04055798798799515 | Test loss: 0.09595536440610886\n",
            "Epoch: 34980 | Train loss: 0.04048944637179375 | Test loss: 0.0957922488451004\n",
            "Epoch: 35000 | Train loss: 0.040420908480882645 | Test loss: 0.09563255310058594\n",
            "Epoch: 35020 | Train loss: 0.04035237058997154 | Test loss: 0.09546944499015808\n",
            "Epoch: 35040 | Train loss: 0.04028383642435074 | Test loss: 0.09530802816152573\n",
            "Epoch: 35060 | Train loss: 0.040215298533439636 | Test loss: 0.09514489769935608\n",
            "Epoch: 35080 | Train loss: 0.04014676436781883 | Test loss: 0.09498176723718643\n",
            "Epoch: 35100 | Train loss: 0.04007822647690773 | Test loss: 0.09482036530971527\n",
            "Epoch: 35120 | Train loss: 0.04000968486070633 | Test loss: 0.09465895593166351\n",
            "Epoch: 35140 | Train loss: 0.03994115814566612 | Test loss: 0.09449583292007446\n",
            "Epoch: 35160 | Train loss: 0.03987261280417442 | Test loss: 0.09433271735906601\n",
            "Epoch: 35180 | Train loss: 0.03980407118797302 | Test loss: 0.09417301416397095\n",
            "Epoch: 35200 | Train loss: 0.03973553329706192 | Test loss: 0.0940098986029625\n",
            "Epoch: 35220 | Train loss: 0.039666999131441116 | Test loss: 0.09384848922491074\n",
            "Epoch: 35240 | Train loss: 0.03959846496582031 | Test loss: 0.09368535876274109\n",
            "Epoch: 35260 | Train loss: 0.039529915899038315 | Test loss: 0.09352394193410873\n",
            "Epoch: 35280 | Train loss: 0.03946138545870781 | Test loss: 0.09336254000663757\n",
            "Epoch: 35300 | Train loss: 0.039392851293087006 | Test loss: 0.09319941699504852\n",
            "Epoch: 35320 | Train loss: 0.0393243134021759 | Test loss: 0.09303628653287888\n",
            "Epoch: 35340 | Train loss: 0.0392557717859745 | Test loss: 0.09287316352128983\n",
            "Epoch: 35360 | Train loss: 0.0391872301697731 | Test loss: 0.09271348267793655\n",
            "Epoch: 35380 | Train loss: 0.0391186960041523 | Test loss: 0.0925503820180893\n",
            "Epoch: 35400 | Train loss: 0.039050161838531494 | Test loss: 0.09238895028829575\n",
            "Epoch: 35420 | Train loss: 0.03898162022233009 | Test loss: 0.09222753345966339\n",
            "Epoch: 35440 | Train loss: 0.03891308605670929 | Test loss: 0.09206441789865494\n",
            "Epoch: 35460 | Train loss: 0.03884454071521759 | Test loss: 0.09190299361944199\n",
            "Epoch: 35480 | Train loss: 0.038776010274887085 | Test loss: 0.09173987805843353\n",
            "Epoch: 35500 | Train loss: 0.03870747610926628 | Test loss: 0.09157674759626389\n",
            "Epoch: 35520 | Train loss: 0.03863894194364548 | Test loss: 0.09141362458467484\n",
            "Epoch: 35540 | Train loss: 0.03857040032744408 | Test loss: 0.09125393629074097\n",
            "Epoch: 35560 | Train loss: 0.038501858711242676 | Test loss: 0.09109252691268921\n",
            "Epoch: 35580 | Train loss: 0.038433339446783066 | Test loss: 0.09092768281698227\n",
            "Epoch: 35600 | Train loss: 0.03836478292942047 | Test loss: 0.09076802432537079\n",
            "Epoch: 35620 | Train loss: 0.03829624876379967 | Test loss: 0.09060486406087875\n",
            "Epoch: 35640 | Train loss: 0.038227710872888565 | Test loss: 0.090443454682827\n",
            "Epoch: 35660 | Train loss: 0.03815917298197746 | Test loss: 0.09028033912181854\n",
            "Epoch: 35680 | Train loss: 0.03809063881635666 | Test loss: 0.0901172086596489\n",
            "Epoch: 35700 | Train loss: 0.03802209720015526 | Test loss: 0.08995407819747925\n",
            "Epoch: 35720 | Train loss: 0.03795357421040535 | Test loss: 0.08979266881942749\n",
            "Epoch: 35740 | Train loss: 0.03788502514362335 | Test loss: 0.08963125944137573\n",
            "Epoch: 35760 | Train loss: 0.03781649097800255 | Test loss: 0.08946814388036728\n",
            "Epoch: 35780 | Train loss: 0.03774794563651085 | Test loss: 0.08930845558643341\n",
            "Epoch: 35800 | Train loss: 0.03767943009734154 | Test loss: 0.08914532512426376\n",
            "Epoch: 35820 | Train loss: 0.03761087357997894 | Test loss: 0.088983915746212\n",
            "Epoch: 35840 | Train loss: 0.03754233941435814 | Test loss: 0.08882079273462296\n",
            "Epoch: 35860 | Train loss: 0.037473805248737335 | Test loss: 0.0886576697230339\n",
            "Epoch: 35880 | Train loss: 0.03740525618195534 | Test loss: 0.08849631249904633\n",
            "Epoch: 35900 | Train loss: 0.037336718291044235 | Test loss: 0.08833485096693039\n",
            "Epoch: 35920 | Train loss: 0.03726818785071373 | Test loss: 0.08817172795534134\n",
            "Epoch: 35940 | Train loss: 0.037199653685092926 | Test loss: 0.0880085900425911\n",
            "Epoch: 35960 | Train loss: 0.03713110834360123 | Test loss: 0.08784891664981842\n",
            "Epoch: 35980 | Train loss: 0.03706257417798042 | Test loss: 0.08768578618764877\n",
            "Epoch: 36000 | Train loss: 0.03699403628706932 | Test loss: 0.08752437680959702\n",
            "Epoch: 36020 | Train loss: 0.03692550212144852 | Test loss: 0.08736125379800797\n",
            "Epoch: 36040 | Train loss: 0.03685695305466652 | Test loss: 0.08719984441995621\n",
            "Epoch: 36060 | Train loss: 0.036788418889045715 | Test loss: 0.08703671395778656\n",
            "Epoch: 36080 | Train loss: 0.03671988844871521 | Test loss: 0.0868753120303154\n",
            "Epoch: 36100 | Train loss: 0.03665135055780411 | Test loss: 0.08671216666698456\n",
            "Epoch: 36120 | Train loss: 0.036582816392183304 | Test loss: 0.0865490511059761\n",
            "Epoch: 36140 | Train loss: 0.0365142747759819 | Test loss: 0.08638937026262283\n",
            "Epoch: 36160 | Train loss: 0.0364457368850708 | Test loss: 0.08622623980045319\n",
            "Epoch: 36180 | Train loss: 0.0363771989941597 | Test loss: 0.08606483787298203\n",
            "Epoch: 36200 | Train loss: 0.036308664828538895 | Test loss: 0.08589997887611389\n",
            "Epoch: 36220 | Train loss: 0.036240119487047195 | Test loss: 0.08574030548334122\n",
            "Epoch: 36240 | Train loss: 0.03617158532142639 | Test loss: 0.08557717502117157\n",
            "Epoch: 36260 | Train loss: 0.03610304743051529 | Test loss: 0.08541577309370041\n",
            "Epoch: 36280 | Train loss: 0.036034513264894485 | Test loss: 0.08525264263153076\n",
            "Epoch: 36300 | Train loss: 0.035965971648693085 | Test loss: 0.08508951961994171\n",
            "Epoch: 36320 | Train loss: 0.035897430032491684 | Test loss: 0.08492980897426605\n",
            "Epoch: 36340 | Train loss: 0.03582889959216118 | Test loss: 0.0847666934132576\n",
            "Epoch: 36360 | Train loss: 0.03576036915183067 | Test loss: 0.08460357040166855\n",
            "Epoch: 36380 | Train loss: 0.035691823810338974 | Test loss: 0.0844404548406601\n",
            "Epoch: 36400 | Train loss: 0.03562328591942787 | Test loss: 0.08428075909614563\n",
            "Epoch: 36420 | Train loss: 0.03555474430322647 | Test loss: 0.08411763608455658\n",
            "Epoch: 36440 | Train loss: 0.035486213862895966 | Test loss: 0.08395622670650482\n",
            "Epoch: 36460 | Train loss: 0.03541768342256546 | Test loss: 0.08379309624433517\n",
            "Epoch: 36480 | Train loss: 0.03534914180636406 | Test loss: 0.08362997323274612\n",
            "Epoch: 36500 | Train loss: 0.03528059646487236 | Test loss: 0.08347028493881226\n",
            "Epoch: 36520 | Train loss: 0.035212062299251556 | Test loss: 0.08330715447664261\n",
            "Epoch: 36540 | Train loss: 0.03514352813363075 | Test loss: 0.08314402401447296\n",
            "Epoch: 36560 | Train loss: 0.03507499024271965 | Test loss: 0.0829809159040451\n",
            "Epoch: 36580 | Train loss: 0.035006456077098846 | Test loss: 0.08282124251127243\n",
            "Epoch: 36600 | Train loss: 0.034937914460897446 | Test loss: 0.08265809714794159\n",
            "Epoch: 36620 | Train loss: 0.03486938402056694 | Test loss: 0.08249673247337341\n",
            "Epoch: 36640 | Train loss: 0.034800849854946136 | Test loss: 0.08233361691236496\n",
            "Epoch: 36660 | Train loss: 0.03473231941461563 | Test loss: 0.0821705088019371\n",
            "Epoch: 36680 | Train loss: 0.03466377779841423 | Test loss: 0.08201082795858383\n",
            "Epoch: 36700 | Train loss: 0.034595247358083725 | Test loss: 0.08184771239757538\n",
            "Epoch: 36720 | Train loss: 0.03452670946717262 | Test loss: 0.08168461173772812\n",
            "Epoch: 36740 | Train loss: 0.03445814922451973 | Test loss: 0.08152149617671967\n",
            "Epoch: 36760 | Train loss: 0.03438963741064072 | Test loss: 0.0813618153333664\n",
            "Epoch: 36780 | Train loss: 0.034321099519729614 | Test loss: 0.08119869977235794\n",
            "Epoch: 36800 | Train loss: 0.03425256907939911 | Test loss: 0.08103730529546738\n",
            "Epoch: 36820 | Train loss: 0.034184034913778305 | Test loss: 0.08087418973445892\n",
            "Epoch: 36840 | Train loss: 0.0341155044734478 | Test loss: 0.08071107417345047\n",
            "Epoch: 36860 | Train loss: 0.0340469591319561 | Test loss: 0.0805514007806778\n",
            "Epoch: 36880 | Train loss: 0.033978428691625595 | Test loss: 0.08038828521966934\n",
            "Epoch: 36900 | Train loss: 0.03390989080071449 | Test loss: 0.08022689074277878\n",
            "Epoch: 36920 | Train loss: 0.03384136036038399 | Test loss: 0.08006377518177032\n",
            "Epoch: 36940 | Train loss: 0.03377282992005348 | Test loss: 0.07990066707134247\n",
            "Epoch: 36960 | Train loss: 0.033704277127981186 | Test loss: 0.07973925769329071\n",
            "Epoch: 36980 | Train loss: 0.03363574668765068 | Test loss: 0.07957787811756134\n",
            "Epoch: 37000 | Train loss: 0.033567219972610474 | Test loss: 0.07941476255655289\n",
            "Epoch: 37020 | Train loss: 0.03349869325757027 | Test loss: 0.07925165444612503\n",
            "Epoch: 37040 | Train loss: 0.03343014046549797 | Test loss: 0.07909195125102997\n",
            "Epoch: 37060 | Train loss: 0.033361610025167465 | Test loss: 0.0789288654923439\n",
            "Epoch: 37080 | Train loss: 0.03329307958483696 | Test loss: 0.07876747101545334\n",
            "Epoch: 37100 | Train loss: 0.033224545419216156 | Test loss: 0.07860436290502548\n",
            "Epoch: 37120 | Train loss: 0.03315601125359535 | Test loss: 0.07844123989343643\n",
            "Epoch: 37140 | Train loss: 0.03308746963739395 | Test loss: 0.07827812433242798\n",
            "Epoch: 37160 | Train loss: 0.03301894664764404 | Test loss: 0.07811672985553741\n",
            "Epoch: 37180 | Train loss: 0.032950397580862045 | Test loss: 0.07795705646276474\n",
            "Epoch: 37200 | Train loss: 0.03288187086582184 | Test loss: 0.0777922198176384\n",
            "Epoch: 37220 | Train loss: 0.03281332924962044 | Test loss: 0.07763254642486572\n",
            "Epoch: 37240 | Train loss: 0.032744795083999634 | Test loss: 0.07746943086385727\n",
            "Epoch: 37260 | Train loss: 0.03267625719308853 | Test loss: 0.0773080438375473\n",
            "Epoch: 37280 | Train loss: 0.032607726752758026 | Test loss: 0.07714492082595825\n",
            "Epoch: 37300 | Train loss: 0.03253919631242752 | Test loss: 0.0769818127155304\n",
            "Epoch: 37320 | Train loss: 0.03247065469622612 | Test loss: 0.07681868970394135\n",
            "Epoch: 37340 | Train loss: 0.032402120530605316 | Test loss: 0.07665901631116867\n",
            "Epoch: 37360 | Train loss: 0.032333582639694214 | Test loss: 0.07649590820074081\n",
            "Epoch: 37380 | Train loss: 0.03226504847407341 | Test loss: 0.07633452117443085\n",
            "Epoch: 37400 | Train loss: 0.032196514308452606 | Test loss: 0.07617311179637909\n",
            "Epoch: 37420 | Train loss: 0.0321279801428318 | Test loss: 0.07601000368595123\n",
            "Epoch: 37440 | Train loss: 0.0320594422519207 | Test loss: 0.07584860920906067\n",
            "Epoch: 37460 | Train loss: 0.031990911811590195 | Test loss: 0.07568550109863281\n",
            "Epoch: 37480 | Train loss: 0.03192238137125969 | Test loss: 0.07552237808704376\n",
            "Epoch: 37500 | Train loss: 0.03185383975505829 | Test loss: 0.0753592699766159\n",
            "Epoch: 37520 | Train loss: 0.031785305589437485 | Test loss: 0.07519959658384323\n",
            "Epoch: 37540 | Train loss: 0.03171676769852638 | Test loss: 0.07503645122051239\n",
            "Epoch: 37560 | Train loss: 0.03164823725819588 | Test loss: 0.07487508654594421\n",
            "Epoch: 37580 | Train loss: 0.031579699367284775 | Test loss: 0.07471197098493576\n",
            "Epoch: 37600 | Train loss: 0.03151117265224457 | Test loss: 0.0745488628745079\n",
            "Epoch: 37620 | Train loss: 0.03144262731075287 | Test loss: 0.07438918203115463\n",
            "Epoch: 37640 | Train loss: 0.03137409687042236 | Test loss: 0.07422606647014618\n",
            "Epoch: 37660 | Train loss: 0.03130556270480156 | Test loss: 0.07406296581029892\n",
            "Epoch: 37680 | Train loss: 0.031237000599503517 | Test loss: 0.07389985024929047\n",
            "Epoch: 37700 | Train loss: 0.031168488785624504 | Test loss: 0.0737401694059372\n",
            "Epoch: 37720 | Train loss: 0.031099950894713402 | Test loss: 0.07357705384492874\n",
            "Epoch: 37740 | Train loss: 0.031031420454382896 | Test loss: 0.07341565936803818\n",
            "Epoch: 37760 | Train loss: 0.030962888151407242 | Test loss: 0.07325254380702972\n",
            "Epoch: 37780 | Train loss: 0.030894353985786438 | Test loss: 0.07308942824602127\n",
            "Epoch: 37800 | Train loss: 0.030825812369585037 | Test loss: 0.0729297548532486\n",
            "Epoch: 37820 | Train loss: 0.030757281929254532 | Test loss: 0.07276663929224014\n",
            "Epoch: 37840 | Train loss: 0.03068874217569828 | Test loss: 0.07260524481534958\n",
            "Epoch: 37860 | Train loss: 0.030620211735367775 | Test loss: 0.07244212925434113\n",
            "Epoch: 37880 | Train loss: 0.03055167756974697 | Test loss: 0.07227902114391327\n",
            "Epoch: 37900 | Train loss: 0.030483130365610123 | Test loss: 0.07211761176586151\n",
            "Epoch: 37920 | Train loss: 0.030414599925279617 | Test loss: 0.07195623219013214\n",
            "Epoch: 37940 | Train loss: 0.03034607134759426 | Test loss: 0.07179311662912369\n",
            "Epoch: 37960 | Train loss: 0.030277540907263756 | Test loss: 0.07163000851869583\n",
            "Epoch: 37980 | Train loss: 0.030208995565772057 | Test loss: 0.07147030532360077\n",
            "Epoch: 38000 | Train loss: 0.03014046512544155 | Test loss: 0.0713072195649147\n",
            "Epoch: 38020 | Train loss: 0.030071929097175598 | Test loss: 0.07114582508802414\n",
            "Epoch: 38040 | Train loss: 0.030003396794199944 | Test loss: 0.07098271697759628\n",
            "Epoch: 38060 | Train loss: 0.02993486262857914 | Test loss: 0.07081959396600723\n",
            "Epoch: 38080 | Train loss: 0.029866322875022888 | Test loss: 0.07065647840499878\n",
            "Epoch: 38100 | Train loss: 0.02979779615998268 | Test loss: 0.07049508392810822\n",
            "Epoch: 38120 | Train loss: 0.029729247093200684 | Test loss: 0.07033541053533554\n",
            "Epoch: 38140 | Train loss: 0.029660722240805626 | Test loss: 0.0701705738902092\n",
            "Epoch: 38160 | Train loss: 0.029592180624604225 | Test loss: 0.07001090049743652\n",
            "Epoch: 38180 | Train loss: 0.02952364645898342 | Test loss: 0.06984778493642807\n",
            "Epoch: 38200 | Train loss: 0.02945510856807232 | Test loss: 0.0696863979101181\n",
            "Epoch: 38220 | Train loss: 0.029386578127741814 | Test loss: 0.06952327489852905\n",
            "Epoch: 38240 | Train loss: 0.02931804768741131 | Test loss: 0.0693601667881012\n",
            "Epoch: 38260 | Train loss: 0.02924950420856476 | Test loss: 0.06919704377651215\n",
            "Epoch: 38280 | Train loss: 0.029180971905589104 | Test loss: 0.06903737038373947\n",
            "Epoch: 38300 | Train loss: 0.029112434014678 | Test loss: 0.06887426227331161\n",
            "Epoch: 38320 | Train loss: 0.029043901711702347 | Test loss: 0.06871287524700165\n",
            "Epoch: 38340 | Train loss: 0.028975365683436394 | Test loss: 0.06855146586894989\n",
            "Epoch: 38360 | Train loss: 0.02890683151781559 | Test loss: 0.06838835775852203\n",
            "Epoch: 38380 | Train loss: 0.028838295489549637 | Test loss: 0.06822696328163147\n",
            "Epoch: 38400 | Train loss: 0.028769761323928833 | Test loss: 0.06806385517120361\n",
            "Epoch: 38420 | Train loss: 0.028701230883598328 | Test loss: 0.06790073215961456\n",
            "Epoch: 38440 | Train loss: 0.028632691130042076 | Test loss: 0.0677376240491867\n",
            "Epoch: 38460 | Train loss: 0.02856415882706642 | Test loss: 0.06757795065641403\n",
            "Epoch: 38480 | Train loss: 0.02849561907351017 | Test loss: 0.06741480529308319\n",
            "Epoch: 38500 | Train loss: 0.028427088633179665 | Test loss: 0.06725344061851501\n",
            "Epoch: 38520 | Train loss: 0.02835855260491371 | Test loss: 0.06709032505750656\n",
            "Epoch: 38540 | Train loss: 0.028290024027228355 | Test loss: 0.0669272169470787\n",
            "Epoch: 38560 | Train loss: 0.028221478685736656 | Test loss: 0.06676753610372543\n",
            "Epoch: 38580 | Train loss: 0.02815294824540615 | Test loss: 0.06660442054271698\n",
            "Epoch: 38600 | Train loss: 0.028084412217140198 | Test loss: 0.06644131988286972\n",
            "Epoch: 38620 | Train loss: 0.028015855699777603 | Test loss: 0.06627820432186127\n",
            "Epoch: 38640 | Train loss: 0.02794734016060829 | Test loss: 0.066118523478508\n",
            "Epoch: 38660 | Train loss: 0.02787880413234234 | Test loss: 0.06595540791749954\n",
            "Epoch: 38680 | Train loss: 0.027810273692011833 | Test loss: 0.06579400599002838\n",
            "Epoch: 38700 | Train loss: 0.02774173952639103 | Test loss: 0.06563089787960052\n",
            "Epoch: 38720 | Train loss: 0.027673203498125076 | Test loss: 0.06546778231859207\n",
            "Epoch: 38740 | Train loss: 0.027604663744568825 | Test loss: 0.0653081089258194\n",
            "Epoch: 38760 | Train loss: 0.027536137029528618 | Test loss: 0.06514499336481094\n",
            "Epoch: 38780 | Train loss: 0.027467593550682068 | Test loss: 0.06498359888792038\n",
            "Epoch: 38800 | Train loss: 0.027399063110351562 | Test loss: 0.06482048332691193\n",
            "Epoch: 38820 | Train loss: 0.027330530807375908 | Test loss: 0.06465737521648407\n",
            "Epoch: 38840 | Train loss: 0.02726198174059391 | Test loss: 0.06449596583843231\n",
            "Epoch: 38860 | Train loss: 0.027193451300263405 | Test loss: 0.06433458626270294\n",
            "Epoch: 38880 | Train loss: 0.027124924585223198 | Test loss: 0.06417147070169449\n",
            "Epoch: 38900 | Train loss: 0.027056390419602394 | Test loss: 0.06400836259126663\n",
            "Epoch: 38920 | Train loss: 0.026987845078110695 | Test loss: 0.06384865939617157\n",
            "Epoch: 38940 | Train loss: 0.02691931463778019 | Test loss: 0.0636855736374855\n",
            "Epoch: 38960 | Train loss: 0.026850780472159386 | Test loss: 0.06352417916059494\n",
            "Epoch: 38980 | Train loss: 0.02678224816918373 | Test loss: 0.06336107105016708\n",
            "Epoch: 39000 | Train loss: 0.026713714003562927 | Test loss: 0.06319794803857803\n",
            "Epoch: 39020 | Train loss: 0.026645174250006676 | Test loss: 0.06303483247756958\n",
            "Epoch: 39040 | Train loss: 0.02657664753496647 | Test loss: 0.06287343800067902\n",
            "Epoch: 39060 | Train loss: 0.02650809846818447 | Test loss: 0.06271376460790634\n",
            "Epoch: 39080 | Train loss: 0.026439575478434563 | Test loss: 0.06254892796278\n",
            "Epoch: 39100 | Train loss: 0.026371031999588013 | Test loss: 0.062389254570007324\n",
            "Epoch: 39120 | Train loss: 0.026302499696612358 | Test loss: 0.06222614273428917\n",
            "Epoch: 39140 | Train loss: 0.026233959943056107 | Test loss: 0.062064748257398605\n",
            "Epoch: 39160 | Train loss: 0.02616543136537075 | Test loss: 0.061901628971099854\n",
            "Epoch: 39180 | Train loss: 0.026096899062395096 | Test loss: 0.061738520860672\n",
            "Epoch: 39200 | Train loss: 0.026028355583548546 | Test loss: 0.06157539412379265\n",
            "Epoch: 39220 | Train loss: 0.02595982328057289 | Test loss: 0.06141572818160057\n",
            "Epoch: 39240 | Train loss: 0.025891287252306938 | Test loss: 0.061252620071172714\n",
            "Epoch: 39260 | Train loss: 0.025822753086686134 | Test loss: 0.06109122559428215\n",
            "Epoch: 39280 | Train loss: 0.02575421705842018 | Test loss: 0.06092981621623039\n",
            "Epoch: 39300 | Train loss: 0.025685682892799377 | Test loss: 0.060766708105802536\n",
            "Epoch: 39320 | Train loss: 0.025617146864533424 | Test loss: 0.06060531735420227\n",
            "Epoch: 39340 | Train loss: 0.02554861642420292 | Test loss: 0.060442209243774414\n",
            "Epoch: 39360 | Train loss: 0.025480082258582115 | Test loss: 0.060279082506895065\n",
            "Epoch: 39380 | Train loss: 0.025411544367671013 | Test loss: 0.06011597439646721\n",
            "Epoch: 39400 | Train loss: 0.02534300647675991 | Test loss: 0.059956301003694534\n",
            "Epoch: 39420 | Train loss: 0.025274470448493958 | Test loss: 0.05979315564036369\n",
            "Epoch: 39440 | Train loss: 0.025205940008163452 | Test loss: 0.059631794691085815\n",
            "Epoch: 39460 | Train loss: 0.0251374039798975 | Test loss: 0.05946868285536766\n",
            "Epoch: 39480 | Train loss: 0.025068877264857292 | Test loss: 0.059305574744939804\n",
            "Epoch: 39500 | Train loss: 0.025000331923365593 | Test loss: 0.059145886451005936\n",
            "Epoch: 39520 | Train loss: 0.024931801483035088 | Test loss: 0.05898277088999748\n",
            "Epoch: 39540 | Train loss: 0.024863265454769135 | Test loss: 0.05881967023015022\n",
            "Epoch: 39560 | Train loss: 0.02479470707476139 | Test loss: 0.05865655466914177\n",
            "Epoch: 39580 | Train loss: 0.02472619153559208 | Test loss: 0.0584968738257885\n",
            "Epoch: 39600 | Train loss: 0.024657655507326126 | Test loss: 0.05833376199007034\n",
            "Epoch: 39620 | Train loss: 0.02458912692964077 | Test loss: 0.058172356337308884\n",
            "Epoch: 39640 | Train loss: 0.024520590901374817 | Test loss: 0.05800924822688103\n",
            "Epoch: 39660 | Train loss: 0.024452056735754013 | Test loss: 0.05784613639116287\n",
            "Epoch: 39680 | Train loss: 0.024383513256907463 | Test loss: 0.0576864592730999\n",
            "Epoch: 39700 | Train loss: 0.024314984679222107 | Test loss: 0.057523347437381744\n",
            "Epoch: 39720 | Train loss: 0.024246444925665855 | Test loss: 0.05736195668578148\n",
            "Epoch: 39740 | Train loss: 0.02417791448533535 | Test loss: 0.05719883367419243\n",
            "Epoch: 39760 | Train loss: 0.024109384045004845 | Test loss: 0.05703572556376457\n",
            "Epoch: 39780 | Train loss: 0.024040833115577698 | Test loss: 0.056874316185712814\n",
            "Epoch: 39800 | Train loss: 0.023972302675247192 | Test loss: 0.056712936609983444\n",
            "Epoch: 39820 | Train loss: 0.023903775960206985 | Test loss: 0.05654982477426529\n",
            "Epoch: 39840 | Train loss: 0.02383524179458618 | Test loss: 0.05638671666383743\n",
            "Epoch: 39860 | Train loss: 0.02376669831573963 | Test loss: 0.05622700974345207\n",
            "Epoch: 39880 | Train loss: 0.023698166012763977 | Test loss: 0.056063927710056305\n",
            "Epoch: 39900 | Train loss: 0.023629633709788322 | Test loss: 0.05590252950787544\n",
            "Epoch: 39920 | Train loss: 0.02356109954416752 | Test loss: 0.055739421397447586\n",
            "Epoch: 39940 | Train loss: 0.023492565378546715 | Test loss: 0.055576302111148834\n",
            "Epoch: 39960 | Train loss: 0.023424025624990463 | Test loss: 0.05541318655014038\n",
            "Epoch: 39980 | Train loss: 0.023355502635240555 | Test loss: 0.05525178834795952\n",
            "Epoch: 40000 | Train loss: 0.02328694798052311 | Test loss: 0.055092114955186844\n",
            "Epoch: 40020 | Train loss: 0.02321842685341835 | Test loss: 0.0549272857606411\n",
            "Epoch: 40040 | Train loss: 0.02314988523721695 | Test loss: 0.054767608642578125\n",
            "Epoch: 40060 | Train loss: 0.023081347346305847 | Test loss: 0.05460449680685997\n",
            "Epoch: 40080 | Train loss: 0.023012811318039894 | Test loss: 0.054443102329969406\n",
            "Epoch: 40100 | Train loss: 0.02294428087770939 | Test loss: 0.054279983043670654\n",
            "Epoch: 40120 | Train loss: 0.022875748574733734 | Test loss: 0.0541168749332428\n",
            "Epoch: 40140 | Train loss: 0.022807206958532333 | Test loss: 0.05395374819636345\n",
            "Epoch: 40160 | Train loss: 0.02273867465555668 | Test loss: 0.05379408225417137\n",
            "Epoch: 40180 | Train loss: 0.022670138627290726 | Test loss: 0.053630974143743515\n",
            "Epoch: 40200 | Train loss: 0.022601602599024773 | Test loss: 0.05346957594156265\n",
            "Epoch: 40220 | Train loss: 0.022533070296049118 | Test loss: 0.05330817028880119\n",
            "Epoch: 40240 | Train loss: 0.022464534267783165 | Test loss: 0.05314506217837334\n",
            "Epoch: 40260 | Train loss: 0.022395998239517212 | Test loss: 0.05298367142677307\n",
            "Epoch: 40280 | Train loss: 0.022327467799186707 | Test loss: 0.052820563316345215\n",
            "Epoch: 40300 | Train loss: 0.022258935496211052 | Test loss: 0.052657436579465866\n",
            "Epoch: 40320 | Train loss: 0.0221903957426548 | Test loss: 0.05249432846903801\n",
            "Epoch: 40340 | Train loss: 0.022121859714388847 | Test loss: 0.052334655076265335\n",
            "Epoch: 40360 | Train loss: 0.022053321823477745 | Test loss: 0.052171509712934494\n",
            "Epoch: 40380 | Train loss: 0.02198479138314724 | Test loss: 0.052010148763656616\n",
            "Epoch: 40400 | Train loss: 0.021916253492236137 | Test loss: 0.05184703692793846\n",
            "Epoch: 40420 | Train loss: 0.02184772863984108 | Test loss: 0.051683928817510605\n",
            "Epoch: 40440 | Train loss: 0.02177918516099453 | Test loss: 0.051524240523576736\n",
            "Epoch: 40460 | Train loss: 0.021710650995373726 | Test loss: 0.05136112496256828\n",
            "Epoch: 40480 | Train loss: 0.02164211869239807 | Test loss: 0.051198024302721024\n",
            "Epoch: 40500 | Train loss: 0.021573560312390327 | Test loss: 0.05103490874171257\n",
            "Epoch: 40520 | Train loss: 0.021505046635866165 | Test loss: 0.0508752278983593\n",
            "Epoch: 40540 | Train loss: 0.021436506882309914 | Test loss: 0.050712116062641144\n",
            "Epoch: 40560 | Train loss: 0.021367980167269707 | Test loss: 0.050550710409879684\n",
            "Epoch: 40580 | Train loss: 0.021299442276358604 | Test loss: 0.05038760229945183\n",
            "Epoch: 40600 | Train loss: 0.02123090997338295 | Test loss: 0.05022449046373367\n",
            "Epoch: 40620 | Train loss: 0.0211623664945364 | Test loss: 0.0500648133456707\n",
            "Epoch: 40640 | Train loss: 0.021093836054205894 | Test loss: 0.049901701509952545\n",
            "Epoch: 40660 | Train loss: 0.021025298163294792 | Test loss: 0.04974031075835228\n",
            "Epoch: 40680 | Train loss: 0.020956767722964287 | Test loss: 0.04957718774676323\n",
            "Epoch: 40700 | Train loss: 0.020888235419988632 | Test loss: 0.04941407963633537\n",
            "Epoch: 40720 | Train loss: 0.020819684490561485 | Test loss: 0.049252670258283615\n",
            "Epoch: 40740 | Train loss: 0.02075115405023098 | Test loss: 0.049091290682554245\n",
            "Epoch: 40760 | Train loss: 0.020682627335190773 | Test loss: 0.04892817884683609\n",
            "Epoch: 40780 | Train loss: 0.02061409316956997 | Test loss: 0.048765070736408234\n",
            "Epoch: 40800 | Train loss: 0.02054554969072342 | Test loss: 0.04860536381602287\n",
            "Epoch: 40820 | Train loss: 0.020477019250392914 | Test loss: 0.048442281782627106\n",
            "Epoch: 40840 | Train loss: 0.02040848508477211 | Test loss: 0.04828088358044624\n",
            "Epoch: 40860 | Train loss: 0.020339949056506157 | Test loss: 0.04811777547001839\n",
            "Epoch: 40880 | Train loss: 0.02027141861617565 | Test loss: 0.047954656183719635\n",
            "Epoch: 40900 | Train loss: 0.0202028788626194 | Test loss: 0.04779154062271118\n",
            "Epoch: 40920 | Train loss: 0.020134354010224342 | Test loss: 0.04763014242053032\n",
            "Epoch: 40940 | Train loss: 0.020065799355506897 | Test loss: 0.047470469027757645\n",
            "Epoch: 40960 | Train loss: 0.019997278228402138 | Test loss: 0.0473056398332119\n",
            "Epoch: 40980 | Train loss: 0.019928736612200737 | Test loss: 0.047145962715148926\n",
            "Epoch: 41000 | Train loss: 0.019860198721289635 | Test loss: 0.04698285087943077\n",
            "Epoch: 41020 | Train loss: 0.01979166269302368 | Test loss: 0.04682145640254021\n",
            "Epoch: 41040 | Train loss: 0.019723134115338326 | Test loss: 0.046658337116241455\n",
            "Epoch: 41060 | Train loss: 0.01965459994971752 | Test loss: 0.0464952290058136\n",
            "Epoch: 41080 | Train loss: 0.01958605833351612 | Test loss: 0.04633210226893425\n",
            "Epoch: 41100 | Train loss: 0.019517527893185616 | Test loss: 0.04617243632674217\n",
            "Epoch: 41120 | Train loss: 0.019448988139629364 | Test loss: 0.046009328216314316\n",
            "Epoch: 41140 | Train loss: 0.01938045583665371 | Test loss: 0.04584793001413345\n",
            "Epoch: 41160 | Train loss: 0.019311919808387756 | Test loss: 0.045686524361371994\n",
            "Epoch: 41180 | Train loss: 0.019243387505412102 | Test loss: 0.04552341625094414\n",
            "Epoch: 41200 | Train loss: 0.019174849614501 | Test loss: 0.04536202549934387\n",
            "Epoch: 41220 | Train loss: 0.019106321036815643 | Test loss: 0.045198917388916016\n",
            "Epoch: 41240 | Train loss: 0.01903778500854969 | Test loss: 0.04503579065203667\n",
            "Epoch: 41260 | Train loss: 0.018969247117638588 | Test loss: 0.04487268254160881\n",
            "Epoch: 41280 | Train loss: 0.018900711089372635 | Test loss: 0.044713009148836136\n",
            "Epoch: 41300 | Train loss: 0.018832175061106682 | Test loss: 0.044549863785505295\n",
            "Epoch: 41320 | Train loss: 0.018763644620776176 | Test loss: 0.044388510286808014\n",
            "Epoch: 41340 | Train loss: 0.018695106729865074 | Test loss: 0.04422539100050926\n",
            "Epoch: 41360 | Train loss: 0.018626580014824867 | Test loss: 0.044062282890081406\n",
            "Epoch: 41380 | Train loss: 0.018558036535978317 | Test loss: 0.04390259459614754\n",
            "Epoch: 41400 | Train loss: 0.018489500507712364 | Test loss: 0.043739479035139084\n",
            "Epoch: 41420 | Train loss: 0.01842097006738186 | Test loss: 0.043576378375291824\n",
            "Epoch: 41440 | Train loss: 0.018352413550019264 | Test loss: 0.04341326281428337\n",
            "Epoch: 41460 | Train loss: 0.018283894285559654 | Test loss: 0.0432535819709301\n",
            "Epoch: 41480 | Train loss: 0.0182153582572937 | Test loss: 0.043090470135211945\n",
            "Epoch: 41500 | Train loss: 0.018146831542253494 | Test loss: 0.042929064482450485\n",
            "Epoch: 41520 | Train loss: 0.01807829551398754 | Test loss: 0.04276595637202263\n",
            "Epoch: 41540 | Train loss: 0.018009759485721588 | Test loss: 0.042602844536304474\n",
            "Epoch: 41560 | Train loss: 0.017941219732165337 | Test loss: 0.0424431674182415\n",
            "Epoch: 41580 | Train loss: 0.01787268929183483 | Test loss: 0.042280055582523346\n",
            "Epoch: 41600 | Train loss: 0.01780414953827858 | Test loss: 0.04211866483092308\n",
            "Epoch: 41620 | Train loss: 0.017735619097948074 | Test loss: 0.04195554181933403\n",
            "Epoch: 41640 | Train loss: 0.01766708865761757 | Test loss: 0.041792433708906174\n",
            "Epoch: 41660 | Train loss: 0.017598537728190422 | Test loss: 0.041631024330854416\n",
            "Epoch: 41680 | Train loss: 0.017530007287859917 | Test loss: 0.041469644755125046\n",
            "Epoch: 41700 | Train loss: 0.01746147871017456 | Test loss: 0.04130653291940689\n",
            "Epoch: 41720 | Train loss: 0.017392946407198906 | Test loss: 0.041143424808979034\n",
            "Epoch: 41740 | Train loss: 0.017324402928352356 | Test loss: 0.040983717888593674\n",
            "Epoch: 41760 | Train loss: 0.0172558706253767 | Test loss: 0.040820635855197906\n",
            "Epoch: 41780 | Train loss: 0.017187336459755898 | Test loss: 0.040659237653017044\n",
            "Epoch: 41800 | Train loss: 0.017118802294135094 | Test loss: 0.04049612954258919\n",
            "Epoch: 41820 | Train loss: 0.01705026999115944 | Test loss: 0.040333010256290436\n",
            "Epoch: 41840 | Train loss: 0.016981730237603188 | Test loss: 0.04016989469528198\n",
            "Epoch: 41860 | Train loss: 0.01691320352256298 | Test loss: 0.04000849649310112\n",
            "Epoch: 41880 | Train loss: 0.01684466376900673 | Test loss: 0.039847105741500854\n",
            "Epoch: 41900 | Train loss: 0.016776129603385925 | Test loss: 0.0396839939057827\n",
            "Epoch: 41920 | Train loss: 0.016707587987184525 | Test loss: 0.03952431678771973\n",
            "Epoch: 41940 | Train loss: 0.016639048233628273 | Test loss: 0.03936120495200157\n",
            "Epoch: 41960 | Train loss: 0.01657051406800747 | Test loss: 0.03919981047511101\n",
            "Epoch: 41980 | Train loss: 0.016501983627676964 | Test loss: 0.039036691188812256\n",
            "Epoch: 42000 | Train loss: 0.01643345318734646 | Test loss: 0.0388735830783844\n",
            "Epoch: 42020 | Train loss: 0.01636490784585476 | Test loss: 0.03871045634150505\n",
            "Epoch: 42040 | Train loss: 0.016296377405524254 | Test loss: 0.03855079039931297\n",
            "Epoch: 42060 | Train loss: 0.01622783951461315 | Test loss: 0.03838768228888512\n",
            "Epoch: 42080 | Train loss: 0.016159305348992348 | Test loss: 0.038226284086704254\n",
            "Epoch: 42100 | Train loss: 0.016090771183371544 | Test loss: 0.038064878433942795\n",
            "Epoch: 42120 | Train loss: 0.01602223888039589 | Test loss: 0.03790177032351494\n",
            "Epoch: 42140 | Train loss: 0.015953702852129936 | Test loss: 0.03774037957191467\n",
            "Epoch: 42160 | Train loss: 0.01588517427444458 | Test loss: 0.037577271461486816\n",
            "Epoch: 42180 | Train loss: 0.015816636383533478 | Test loss: 0.03741414472460747\n",
            "Epoch: 42200 | Train loss: 0.015748098492622375 | Test loss: 0.03725103661417961\n",
            "Epoch: 42220 | Train loss: 0.01567956432700157 | Test loss: 0.03709136322140694\n",
            "Epoch: 42240 | Train loss: 0.015611027367413044 | Test loss: 0.036928217858076096\n",
            "Epoch: 42260 | Train loss: 0.01554249506443739 | Test loss: 0.036766864359378815\n",
            "Epoch: 42280 | Train loss: 0.015473959036171436 | Test loss: 0.03660374507308006\n",
            "Epoch: 42300 | Train loss: 0.01540543045848608 | Test loss: 0.036440636962652206\n",
            "Epoch: 42320 | Train loss: 0.01533688884228468 | Test loss: 0.03628094866871834\n",
            "Epoch: 42340 | Train loss: 0.015268352814018726 | Test loss: 0.036117833107709885\n",
            "Epoch: 42360 | Train loss: 0.015199819579720497 | Test loss: 0.035954732447862625\n",
            "Epoch: 42380 | Train loss: 0.015131264925003052 | Test loss: 0.03579161688685417\n",
            "Epoch: 42400 | Train loss: 0.015062746591866016 | Test loss: 0.0356319360435009\n",
            "Epoch: 42420 | Train loss: 0.014994210563600063 | Test loss: 0.035468824207782745\n",
            "Epoch: 42440 | Train loss: 0.014925682917237282 | Test loss: 0.035307418555021286\n",
            "Epoch: 42460 | Train loss: 0.014857145957648754 | Test loss: 0.03514431044459343\n",
            "Epoch: 42480 | Train loss: 0.01478861179202795 | Test loss: 0.034981198608875275\n",
            "Epoch: 42500 | Train loss: 0.01472007017582655 | Test loss: 0.0348215214908123\n",
            "Epoch: 42520 | Train loss: 0.014651539735496044 | Test loss: 0.03465840965509415\n",
            "Epoch: 42540 | Train loss: 0.014583000913262367 | Test loss: 0.03449701890349388\n",
            "Epoch: 42560 | Train loss: 0.014514470472931862 | Test loss: 0.03433389589190483\n",
            "Epoch: 42580 | Train loss: 0.01444593071937561 | Test loss: 0.034172505140304565\n",
            "Epoch: 42600 | Train loss: 0.014377388171851635 | Test loss: 0.03400937840342522\n",
            "Epoch: 42620 | Train loss: 0.01430885773152113 | Test loss: 0.03384799882769585\n",
            "Epoch: 42640 | Train loss: 0.014240331016480923 | Test loss: 0.03368488699197769\n",
            "Epoch: 42660 | Train loss: 0.014171796850860119 | Test loss: 0.033521778881549835\n",
            "Epoch: 42680 | Train loss: 0.014103255234658718 | Test loss: 0.033362071961164474\n",
            "Epoch: 42700 | Train loss: 0.014034722931683064 | Test loss: 0.03319898992776871\n",
            "Epoch: 42720 | Train loss: 0.013966187834739685 | Test loss: 0.033037591725587845\n",
            "Epoch: 42740 | Train loss: 0.013897652737796307 | Test loss: 0.03287448361515999\n",
            "Epoch: 42760 | Train loss: 0.013829121366143227 | Test loss: 0.03271136432886124\n",
            "Epoch: 42780 | Train loss: 0.013760581612586975 | Test loss: 0.03254824876785278\n",
            "Epoch: 42800 | Train loss: 0.013692053966224194 | Test loss: 0.03238685056567192\n",
            "Epoch: 42820 | Train loss: 0.013623515143990517 | Test loss: 0.032225459814071655\n",
            "Epoch: 42840 | Train loss: 0.013554980047047138 | Test loss: 0.0320623479783535\n",
            "Epoch: 42860 | Train loss: 0.013486439362168312 | Test loss: 0.03190267086029053\n",
            "Epoch: 42880 | Train loss: 0.01341789960861206 | Test loss: 0.03173955902457237\n",
            "Epoch: 42900 | Train loss: 0.013349366374313831 | Test loss: 0.03157816454768181\n",
            "Epoch: 42920 | Train loss: 0.013280835933983326 | Test loss: 0.03141504526138306\n",
            "Epoch: 42940 | Train loss: 0.013212303631007671 | Test loss: 0.0312519371509552\n",
            "Epoch: 42960 | Train loss: 0.013143758289515972 | Test loss: 0.031088812276721\n",
            "Epoch: 42980 | Train loss: 0.013075229711830616 | Test loss: 0.030929142609238625\n",
            "Epoch: 43000 | Train loss: 0.013006689958274364 | Test loss: 0.030766034498810768\n",
            "Epoch: 43020 | Train loss: 0.012938156723976135 | Test loss: 0.030604636296629906\n",
            "Epoch: 43040 | Train loss: 0.01286962628364563 | Test loss: 0.0304415225982666\n",
            "Epoch: 43060 | Train loss: 0.012801090255379677 | Test loss: 0.03028012625873089\n",
            "Epoch: 43080 | Train loss: 0.01273255329579115 | Test loss: 0.030118733644485474\n",
            "Epoch: 43100 | Train loss: 0.012664026580750942 | Test loss: 0.029955631121993065\n",
            "Epoch: 43120 | Train loss: 0.01259548868983984 | Test loss: 0.029792500659823418\n",
            "Epoch: 43140 | Train loss: 0.012526950798928738 | Test loss: 0.02962939254939556\n",
            "Epoch: 43160 | Train loss: 0.012458416633307934 | Test loss: 0.029469717293977737\n",
            "Epoch: 43180 | Train loss: 0.012389878742396832 | Test loss: 0.029306573793292046\n",
            "Epoch: 43200 | Train loss: 0.012321346439421177 | Test loss: 0.029145216569304466\n",
            "Epoch: 43220 | Train loss: 0.012252810411155224 | Test loss: 0.028982097283005714\n",
            "Epoch: 43240 | Train loss: 0.012184280902147293 | Test loss: 0.028818989172577858\n",
            "Epoch: 43260 | Train loss: 0.012115742079913616 | Test loss: 0.0286575797945261\n",
            "Epoch: 43280 | Train loss: 0.012047208845615387 | Test loss: 0.028494471684098244\n",
            "Epoch: 43300 | Train loss: 0.011978670954704285 | Test loss: 0.028333086520433426\n",
            "Epoch: 43320 | Train loss: 0.01191011629998684 | Test loss: 0.028169972822070122\n",
            "Epoch: 43340 | Train loss: 0.011841598898172379 | Test loss: 0.02801029197871685\n",
            "Epoch: 43360 | Train loss: 0.011773061007261276 | Test loss: 0.027847176417708397\n",
            "Epoch: 43380 | Train loss: 0.011704533360898495 | Test loss: 0.027685774490237236\n",
            "Epoch: 43400 | Train loss: 0.011635997332632542 | Test loss: 0.02752266637980938\n",
            "Epoch: 43420 | Train loss: 0.011567461304366589 | Test loss: 0.027359550818800926\n",
            "Epoch: 43440 | Train loss: 0.011498921550810337 | Test loss: 0.02719987742602825\n",
            "Epoch: 43460 | Train loss: 0.011430390179157257 | Test loss: 0.0270367618650198\n",
            "Epoch: 43480 | Train loss: 0.01136185321956873 | Test loss: 0.026875371113419533\n",
            "Epoch: 43500 | Train loss: 0.011293319053947926 | Test loss: 0.026710540056228638\n",
            "Epoch: 43520 | Train loss: 0.011224783025681973 | Test loss: 0.026550859212875366\n",
            "Epoch: 43540 | Train loss: 0.011156241409480572 | Test loss: 0.026387734338641167\n",
            "Epoch: 43560 | Train loss: 0.011087710037827492 | Test loss: 0.026226354762911797\n",
            "Epoch: 43580 | Train loss: 0.011019181460142136 | Test loss: 0.026063239201903343\n",
            "Epoch: 43600 | Train loss: 0.010950648225843906 | Test loss: 0.025900131091475487\n",
            "Epoch: 43620 | Train loss: 0.010882105678319931 | Test loss: 0.025740427896380424\n",
            "Epoch: 43640 | Train loss: 0.010813576169312 | Test loss: 0.02557734213769436\n",
            "Epoch: 43660 | Train loss: 0.010745040141046047 | Test loss: 0.025415945798158646\n",
            "Epoch: 43680 | Train loss: 0.010676504112780094 | Test loss: 0.02525283768773079\n",
            "Epoch: 43700 | Train loss: 0.010607972741127014 | Test loss: 0.025089716538786888\n",
            "Epoch: 43720 | Train loss: 0.010539430193603039 | Test loss: 0.024926602840423584\n",
            "Epoch: 43740 | Train loss: 0.010470896027982235 | Test loss: 0.02476692758500576\n",
            "Epoch: 43760 | Train loss: 0.010402366518974304 | Test loss: 0.024603813886642456\n",
            "Epoch: 43780 | Train loss: 0.0103338323533535 | Test loss: 0.024440700188279152\n",
            "Epoch: 43800 | Train loss: 0.010265291668474674 | Test loss: 0.024281024932861328\n",
            "Epoch: 43820 | Train loss: 0.010196751914918423 | Test loss: 0.024117911234498024\n",
            "Epoch: 43840 | Train loss: 0.010128217749297619 | Test loss: 0.02395652048289776\n",
            "Epoch: 43860 | Train loss: 0.010059688240289688 | Test loss: 0.023793399333953857\n",
            "Epoch: 43880 | Train loss: 0.009991156868636608 | Test loss: 0.023630285635590553\n",
            "Epoch: 43900 | Train loss: 0.009922610595822334 | Test loss: 0.0234671663492918\n",
            "Epoch: 43920 | Train loss: 0.009854072704911232 | Test loss: 0.023307442665100098\n",
            "Epoch: 43940 | Train loss: 0.009785535745322704 | Test loss: 0.02314433455467224\n",
            "Epoch: 43960 | Train loss: 0.009717002511024475 | Test loss: 0.022981196641921997\n",
            "Epoch: 43980 | Train loss: 0.009648451581597328 | Test loss: 0.02282148040831089\n",
            "Epoch: 44000 | Train loss: 0.00957991648465395 | Test loss: 0.022658348083496094\n",
            "Epoch: 44020 | Train loss: 0.009511376731097698 | Test loss: 0.022496944293379784\n",
            "Epoch: 44040 | Train loss: 0.009442835114896297 | Test loss: 0.022333800792694092\n",
            "Epoch: 44060 | Train loss: 0.009374300949275494 | Test loss: 0.022172391414642334\n",
            "Epoch: 44080 | Train loss: 0.009305761195719242 | Test loss: 0.022009259089827538\n",
            "Epoch: 44100 | Train loss: 0.009237229824066162 | Test loss: 0.021847844123840332\n",
            "Epoch: 44120 | Train loss: 0.009168682619929314 | Test loss: 0.021684689447283745\n",
            "Epoch: 44140 | Train loss: 0.009100145660340786 | Test loss: 0.021521562710404396\n",
            "Epoch: 44160 | Train loss: 0.009031600318849087 | Test loss: 0.021361863240599632\n",
            "Epoch: 44180 | Train loss: 0.008963067084550858 | Test loss: 0.0211970042437315\n",
            "Epoch: 44200 | Train loss: 0.008894531987607479 | Test loss: 0.021035606041550636\n",
            "Epoch: 44220 | Train loss: 0.008825990371406078 | Test loss: 0.02087247371673584\n",
            "Epoch: 44240 | Train loss: 0.008757445029914379 | Test loss: 0.02071276307106018\n",
            "Epoch: 44260 | Train loss: 0.008688909932971 | Test loss: 0.020549625158309937\n",
            "Epoch: 44280 | Train loss: 0.0086203683167696 | Test loss: 0.02038821578025818\n",
            "Epoch: 44300 | Train loss: 0.00855183508247137 | Test loss: 0.020225077867507935\n",
            "Epoch: 44320 | Train loss: 0.008483292534947395 | Test loss: 0.020063668489456177\n",
            "Epoch: 44340 | Train loss: 0.00841475185006857 | Test loss: 0.01990225911140442\n",
            "Epoch: 44360 | Train loss: 0.008346215821802616 | Test loss: 0.019739126786589622\n",
            "Epoch: 44380 | Train loss: 0.008277677930891514 | Test loss: 0.019575977697968483\n",
            "Epoch: 44400 | Train loss: 0.00820913165807724 | Test loss: 0.01941285841166973\n",
            "Epoch: 44420 | Train loss: 0.008140593767166138 | Test loss: 0.019253134727478027\n",
            "Epoch: 44440 | Train loss: 0.008072057738900185 | Test loss: 0.01909002661705017\n",
            "Epoch: 44460 | Train loss: 0.00800352357327938 | Test loss: 0.018926888704299927\n",
            "Epoch: 44480 | Train loss: 0.007934972643852234 | Test loss: 0.01876717247068882\n",
            "Epoch: 44500 | Train loss: 0.00786643847823143 | Test loss: 0.018604040145874023\n",
            "Epoch: 44520 | Train loss: 0.007797897793352604 | Test loss: 0.018442636355757713\n",
            "Epoch: 44540 | Train loss: 0.007729356177151203 | Test loss: 0.01827949285507202\n",
            "Epoch: 44560 | Train loss: 0.007660822477191687 | Test loss: 0.018118083477020264\n",
            "Epoch: 44580 | Train loss: 0.0075922817923128605 | Test loss: 0.017954951152205467\n",
            "Epoch: 44600 | Train loss: 0.007523751351982355 | Test loss: 0.01779353618621826\n",
            "Epoch: 44620 | Train loss: 0.007455203682184219 | Test loss: 0.017630381509661674\n",
            "Epoch: 44640 | Train loss: 0.007386666722595692 | Test loss: 0.017467254772782326\n",
            "Epoch: 44660 | Train loss: 0.00731812184676528 | Test loss: 0.017307555302977562\n",
            "Epoch: 44680 | Train loss: 0.007249589543789625 | Test loss: 0.01714269630610943\n",
            "Epoch: 44700 | Train loss: 0.007181053515523672 | Test loss: 0.016981298103928566\n",
            "Epoch: 44720 | Train loss: 0.007112511433660984 | Test loss: 0.01681816577911377\n",
            "Epoch: 44740 | Train loss: 0.007043966557830572 | Test loss: 0.01665845513343811\n",
            "Epoch: 44760 | Train loss: 0.006975430529564619 | Test loss: 0.016495317220687866\n",
            "Epoch: 44780 | Train loss: 0.006906889379024506 | Test loss: 0.01633390784263611\n",
            "Epoch: 44800 | Train loss: 0.006838356610387564 | Test loss: 0.016170769929885864\n",
            "Epoch: 44820 | Train loss: 0.006769814528524876 | Test loss: 0.016009360551834106\n",
            "Epoch: 44840 | Train loss: 0.006701273377984762 | Test loss: 0.01584795117378235\n",
            "Epoch: 44860 | Train loss: 0.006632737349718809 | Test loss: 0.015684818848967552\n",
            "Epoch: 44880 | Train loss: 0.006564199924468994 | Test loss: 0.015521669760346413\n",
            "Epoch: 44900 | Train loss: 0.006495653185993433 | Test loss: 0.015358549542725086\n",
            "Epoch: 44920 | Train loss: 0.006427115295082331 | Test loss: 0.015198826789855957\n",
            "Epoch: 44940 | Train loss: 0.006358579732477665 | Test loss: 0.0150357186794281\n",
            "Epoch: 44960 | Train loss: 0.0062900446355342865 | Test loss: 0.014872580766677856\n",
            "Epoch: 44980 | Train loss: 0.006221494171768427 | Test loss: 0.01471286453306675\n",
            "Epoch: 45000 | Train loss: 0.006152960006147623 | Test loss: 0.014549732208251953\n",
            "Epoch: 45020 | Train loss: 0.006084419321268797 | Test loss: 0.014388329349458218\n",
            "Epoch: 45040 | Train loss: 0.006015877705067396 | Test loss: 0.014225184917449951\n",
            "Epoch: 45060 | Train loss: 0.00594734400510788 | Test loss: 0.014063775539398193\n",
            "Epoch: 45080 | Train loss: 0.0058788033202290535 | Test loss: 0.013900644145905972\n",
            "Epoch: 45100 | Train loss: 0.005810272879898548 | Test loss: 0.013739228248596191\n",
            "Epoch: 45120 | Train loss: 0.005741725210100412 | Test loss: 0.01357607264071703\n",
            "Epoch: 45140 | Train loss: 0.005673188250511885 | Test loss: 0.013412946835160255\n",
            "Epoch: 45160 | Train loss: 0.005604643374681473 | Test loss: 0.013253248296678066\n",
            "Epoch: 45180 | Train loss: 0.005536110606044531 | Test loss: 0.013088387437164783\n",
            "Epoch: 45200 | Train loss: 0.005467575043439865 | Test loss: 0.012926990166306496\n",
            "Epoch: 45220 | Train loss: 0.005399032961577177 | Test loss: 0.0127638578414917\n",
            "Epoch: 45240 | Train loss: 0.005330488085746765 | Test loss: 0.01260414719581604\n",
            "Epoch: 45260 | Train loss: 0.005261951591819525 | Test loss: 0.012441009283065796\n",
            "Epoch: 45280 | Train loss: 0.005193410906940699 | Test loss: 0.012279599905014038\n",
            "Epoch: 45300 | Train loss: 0.005124878138303757 | Test loss: 0.012116461992263794\n",
            "Epoch: 45320 | Train loss: 0.005056336056441069 | Test loss: 0.011955052614212036\n",
            "Epoch: 45340 | Train loss: 0.004987794905900955 | Test loss: 0.011793643236160278\n",
            "Epoch: 45360 | Train loss: 0.004919258411973715 | Test loss: 0.011630511842668056\n",
            "Epoch: 45380 | Train loss: 0.004850721452385187 | Test loss: 0.011467361822724342\n",
            "Epoch: 45400 | Train loss: 0.004782174713909626 | Test loss: 0.011304241605103016\n",
            "Epoch: 45420 | Train loss: 0.004713636822998524 | Test loss: 0.011144518852233887\n",
            "Epoch: 45440 | Train loss: 0.004645101260393858 | Test loss: 0.01098141074180603\n",
            "Epoch: 45460 | Train loss: 0.0045765661634504795 | Test loss: 0.010818272829055786\n",
            "Epoch: 45480 | Train loss: 0.00450801569968462 | Test loss: 0.01065855659544468\n",
            "Epoch: 45500 | Train loss: 0.004439481534063816 | Test loss: 0.010495424270629883\n",
            "Epoch: 45520 | Train loss: 0.004370940383523703 | Test loss: 0.010334021411836147\n",
            "Epoch: 45540 | Train loss: 0.004302398767322302 | Test loss: 0.01017087697982788\n",
            "Epoch: 45560 | Train loss: 0.004233865533024073 | Test loss: 0.010009467601776123\n",
            "Epoch: 45580 | Train loss: 0.0041653248481452465 | Test loss: 0.009846336208283901\n",
            "Epoch: 45600 | Train loss: 0.004096794407814741 | Test loss: 0.009684920310974121\n",
            "Epoch: 45620 | Train loss: 0.004028246738016605 | Test loss: 0.00952176470309496\n",
            "Epoch: 45640 | Train loss: 0.00395970931276679 | Test loss: 0.009358638897538185\n",
            "Epoch: 45660 | Train loss: 0.003891164902597666 | Test loss: 0.009198940359055996\n",
            "Epoch: 45680 | Train loss: 0.003822632133960724 | Test loss: 0.009034079499542713\n",
            "Epoch: 45700 | Train loss: 0.003754096571356058 | Test loss: 0.008872682228684425\n",
            "Epoch: 45720 | Train loss: 0.003685552626848221 | Test loss: 0.008709544315934181\n",
            "Epoch: 45740 | Train loss: 0.003617009613662958 | Test loss: 0.00854983925819397\n",
            "Epoch: 45760 | Train loss: 0.0035484747495502234 | Test loss: 0.008386701345443726\n",
            "Epoch: 45780 | Train loss: 0.003479932202026248 | Test loss: 0.008225291967391968\n",
            "Epoch: 45800 | Train loss: 0.0034113996662199497 | Test loss: 0.008062154054641724\n",
            "Epoch: 45820 | Train loss: 0.003342857351526618 | Test loss: 0.007900744676589966\n",
            "Epoch: 45840 | Train loss: 0.003274316433817148 | Test loss: 0.007739335298538208\n",
            "Epoch: 45860 | Train loss: 0.003205779939889908 | Test loss: 0.007576203439384699\n",
            "Epoch: 45880 | Train loss: 0.0031372427474707365 | Test loss: 0.007413053419440985\n",
            "Epoch: 45900 | Train loss: 0.0030686960089951754 | Test loss: 0.007249933667480946\n",
            "Epoch: 45920 | Train loss: 0.003000158118084073 | Test loss: 0.007090210914611816\n",
            "Epoch: 45940 | Train loss: 0.0029316225554794073 | Test loss: 0.00692710280418396\n",
            "Epoch: 45960 | Train loss: 0.002863087458536029 | Test loss: 0.006763964891433716\n",
            "Epoch: 45980 | Train loss: 0.0027945369947701693 | Test loss: 0.006604248192161322\n",
            "Epoch: 46000 | Train loss: 0.0027260028291493654 | Test loss: 0.0064411163330078125\n",
            "Epoch: 46020 | Train loss: 0.0026574619114398956 | Test loss: 0.00627971300855279\n",
            "Epoch: 46040 | Train loss: 0.002588920295238495 | Test loss: 0.0061165690422058105\n",
            "Epoch: 46060 | Train loss: 0.002520386828109622 | Test loss: 0.005955159664154053\n",
            "Epoch: 46080 | Train loss: 0.002451846143230796 | Test loss: 0.005792027805000544\n",
            "Epoch: 46100 | Train loss: 0.0023833157029002905 | Test loss: 0.005630612373352051\n",
            "Epoch: 46120 | Train loss: 0.0023147680331021547 | Test loss: 0.005467456765472889\n",
            "Epoch: 46140 | Train loss: 0.0022462308406829834 | Test loss: 0.0053043304942548275\n",
            "Epoch: 46160 | Train loss: 0.002177686197683215 | Test loss: 0.005144631955772638\n",
            "Epoch: 46180 | Train loss: 0.002109153661876917 | Test loss: 0.004979771561920643\n",
            "Epoch: 46200 | Train loss: 0.002040618797764182 | Test loss: 0.004818373825401068\n",
            "Epoch: 46220 | Train loss: 0.001972073456272483 | Test loss: 0.004655235912650824\n",
            "Epoch: 46240 | Train loss: 0.0019035302102565765 | Test loss: 0.004495531320571899\n",
            "Epoch: 46260 | Train loss: 0.0018349953461438417 | Test loss: 0.004332393407821655\n",
            "Epoch: 46280 | Train loss: 0.0017664529150351882 | Test loss: 0.0041709840297698975\n",
            "Epoch: 46300 | Train loss: 0.0016979187494143844 | Test loss: 0.004007846117019653\n",
            "Epoch: 46320 | Train loss: 0.0016293794615194201 | Test loss: 0.0038464367389678955\n",
            "Epoch: 46340 | Train loss: 0.0015608385438099504 | Test loss: 0.0036850273609161377\n",
            "Epoch: 46360 | Train loss: 0.0014923021662980318 | Test loss: 0.0035218955017626286\n",
            "Epoch: 46380 | Train loss: 0.0014237649738788605 | Test loss: 0.003358745714649558\n",
            "Epoch: 46400 | Train loss: 0.0013552181189879775 | Test loss: 0.0031956254970282316\n",
            "Epoch: 46420 | Train loss: 0.0012866802280768752 | Test loss: 0.003035902976989746\n",
            "Epoch: 46440 | Train loss: 0.0012181438505649567 | Test loss: 0.0028727948665618896\n",
            "Epoch: 46460 | Train loss: 0.0011496089864522219 | Test loss: 0.0027096569538116455\n",
            "Epoch: 46480 | Train loss: 0.0010810584062710404 | Test loss: 0.002549940487369895\n",
            "Epoch: 46500 | Train loss: 0.0010125242406502366 | Test loss: 0.002386808395385742\n",
            "Epoch: 46520 | Train loss: 0.0009439833811484277 | Test loss: 0.0022254050709307194\n",
            "Epoch: 46540 | Train loss: 0.000875441764947027 | Test loss: 0.0020622611045837402\n",
            "Epoch: 46560 | Train loss: 0.0008069082978181541 | Test loss: 0.0019008517265319824\n",
            "Epoch: 46580 | Train loss: 0.0007383674383163452 | Test loss: 0.0017377197509631515\n",
            "Epoch: 46600 | Train loss: 0.0006698369979858398 | Test loss: 0.0015763044357299805\n",
            "Epoch: 46620 | Train loss: 0.0006012894446030259 | Test loss: 0.0014131487114354968\n",
            "Epoch: 46640 | Train loss: 0.0005327523103915155 | Test loss: 0.001250022673048079\n",
            "Epoch: 46660 | Train loss: 0.0004642076964955777 | Test loss: 0.0010903239017352462\n",
            "Epoch: 46680 | Train loss: 0.00039567501517012715 | Test loss: 0.0009254634496755898\n",
            "Epoch: 46700 | Train loss: 0.00032714009284973145 | Test loss: 0.0007640660041943192\n",
            "Epoch: 46720 | Train loss: 0.0002585947513580322 | Test loss: 0.0006009280914440751\n",
            "Epoch: 46740 | Train loss: 0.00019005165086127818 | Test loss: 0.0004412233829498291\n",
            "Epoch: 46760 | Train loss: 0.00012151673581684008 | Test loss: 0.00027808547019958496\n",
            "Epoch: 46780 | Train loss: 5.297437382978387e-05 | Test loss: 0.00011667609214782715\n",
            "Epoch: 46800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 46820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 46840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 46860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 46880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 46900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 46920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 46940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 46960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 46980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 47980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 48980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 49980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 50980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 51980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 52980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 53980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 54980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 55980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 56980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 57980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 58980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 59980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 60980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 61980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 62980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 63980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 64980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 65980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 66980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 67980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 68980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 69980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 70980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 71980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 72980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 73980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 74980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 75980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 76980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 77980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 78980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 79980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 80980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 81980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 82980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 83980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 84980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 85980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 86980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 87980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 88980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 89980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 90980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 91980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 92980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 93980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 94980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 95980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 96980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 97980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 98980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99000 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99020 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99040 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99060 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99080 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99100 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99120 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99140 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99160 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99180 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99200 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99220 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99240 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99260 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99280 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99300 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99320 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99340 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99360 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99380 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99400 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99420 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99440 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99460 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99480 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99500 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99520 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99540 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99560 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99580 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99600 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99620 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99640 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99660 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99680 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99700 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99720 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99740 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99760 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99780 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99800 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99820 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99840 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99860 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99880 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99900 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99920 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99940 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99960 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n",
            "Epoch: 99980 | Train loss: 7.939338684082031e-05 | Test loss: 2.0128489268245175e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make predictions with the trained model on the test data.\n",
        "  * Visualize these predictions against the original training and testing data (**note:** you may need to make sure the predictions are *not* on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot)."
      ],
      "metadata": {
        "id": "x4j4TM18jwa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with the model\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  preds = model_0(X_test)\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "bbMPK5Qjjyx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd896a3-2f7d-4dcd-d03d-06db0bee92d3"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.1399],\n",
            "        [1.1429],\n",
            "        [1.1459],\n",
            "        [1.1489],\n",
            "        [1.1519],\n",
            "        [1.1549],\n",
            "        [1.1579],\n",
            "        [1.1609],\n",
            "        [1.1639],\n",
            "        [1.1669],\n",
            "        [1.1699],\n",
            "        [1.1729],\n",
            "        [1.1759],\n",
            "        [1.1789],\n",
            "        [1.1819],\n",
            "        [1.1849],\n",
            "        [1.1879],\n",
            "        [1.1909],\n",
            "        [1.1939],\n",
            "        [1.1969]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the predictions (these may need to be on a specific device)\n",
        "X_train, X_test, y_train, y_test, preds = X_train.cpu(), X_test.cpu(), y_train.cpu(), y_test.cpu(), preds.cpu()\n",
        "plot_predictions(predictions=preds)"
      ],
      "metadata": {
        "id": "K3BdmQaDpFo8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "ad03e0e6-271c-44a4-bce0-1146961aebc4"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Save your trained model's `state_dict()` to file.\n",
        "  * Create a new instance of your model class you made in 2. and load in the `state_dict()` you just saved to it.\n",
        "  * Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4."
      ],
      "metadata": {
        "id": "s2OnlMWKjzX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# 1. Create models directory\n",
        "PATH = Path(\"models\")\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Create model save path\n",
        "MODEL_NAME = \"01_pytorch_linearregressionmodel_run2.pth\"\n",
        "MODEL_PATH = PATH / MODEL_NAME\n",
        "# 3. Save the model state dict\n",
        "torch.save(model_0.state_dict(), MODEL_PATH)\n",
        "print(f\"Saving model in directory: {MODEL_PATH}\")"
      ],
      "metadata": {
        "id": "hgxhgD14qr-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f687e904-f50d-47a5-f472-96093f30b833"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model in directory: models/01_pytorch_linearregressionmodel_run2.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new instance of model and load saved state dict (make sure to put it on the target device)\n",
        "loaded_model_0 = LinearRegressionModel()\n",
        "loaded_model_0.load_state_dict(torch.load(MODEL_PATH))\n",
        "loaded_model_0.to(device)"
      ],
      "metadata": {
        "id": "P9vTgiLRrJ7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400743b9-fcf0-4c70-d5e4-bd7cec23d76f"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-115-895288acc87f>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_model_0.load_state_dict(torch.load(MODEL_PATH))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegressionModel(\n",
              "  (l1): Linear(in_features=1, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with loaded model and compare them to the previous\n",
        "X_train, X_test, y_train, y_test, preds = X_train.to(device), X_test.to(device), y_train.to(device), y_test.to(device), preds.to(device)\n",
        "\n",
        "loaded_model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  loaded_preds = loaded_model_0(X_test)\n",
        "\n",
        "loaded_preds = loaded_preds.cpu()\n",
        "plot_predictions(predictions=loaded_preds)"
      ],
      "metadata": {
        "id": "8UGX3VebrVtI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "a7b695b7-274b-4476-e155-3a09322254f4"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAJGCAYAAACZel7oAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUlBJREFUeJzt3XtclGXC//HvAAKagusJRQnPmmV4SsJyBaNIfUTNHs12XXTLfpqdtNbVNNF6zNrHzPJU66Zku3nYRKVsyUOia6JuKm1muplnFNTSGTVFhPv3Bw+TEwdngGFOn/frNS/jnmturmFvWr/d1/Udk2EYhgAAAADAx/i5egIAAAAA4AqEIQAAAAA+iTAEAAAAwCcRhgAAAAD4JMIQAAAAAJ9EGAIAAADgkwhDAAAAAHxSgKsnUFUKCwt16tQp1alTRyaTydXTAQAAAOAihmHo4sWLCg8Pl59f2fd/vCYMnTp1ShEREa6eBgAAAAA3ceLECTVr1qzM570mDNWpU0dS0RsOCQlx8WwAAAAAuIrFYlFERIQ1I5TFa8JQ8dK4kJAQwhAAAACAm26foUABAAAAgE8iDAEAAADwSYQhAAAAAD6JMAQAAADAJxGGAAAAAPgkwhAAAAAAn+Q11doVkZ+fr4KCAldPA6h2/v7+qlGjhqunAQAA4FI+GYYsFovOnTunvLw8V08FcJmgoCA1aNCAz+UCAAA+y+fCkMViUXZ2tmrXrq0GDRqoRo0aN/0wJsCbGIah/Px8mc1mZWdnSxKBCAAA+CSfC0Pnzp1T7dq11axZM0IQfFbNmjVVp04dnTx5UufOnSMMAQAAn+RTBQr5+fnKy8tTaGgoQQg+z2QyKTQ0VHl5ecrPz3f1dAAAAKqdT4Wh4rIENo4DRYp/FygSAQAAvsinwlAx7goBRfhdAAAAvszhMLR161b1799f4eHhMplMWrNmTbnjU1NTdf/996thw4YKCQlRTEyMPvvssxLj5s+fr+bNmys4OFjR0dHatWuXo1MDAAAAALs5HIYuX76sqKgozZ8/367xW7du1f33369PP/1Uu3fvVlxcnPr376+9e/dax6xYsULjx49XcnKy9uzZo6ioKCUkJOjMmTOOTg8AAAAA7OJwGOrTp4/+53/+R4MGDbJr/Jw5czRhwgTdddddatOmjV599VW1adNGH3/8sXXM7NmzNWrUKI0cOVIdOnTQO++8o1q1amnx4sWOTg9uymQyKTY2tlLnyMjIkMlk0rRp06pkTs7WvHlzNW/e3NXTAAAAQBmqfc9QYWGhLl68qHr16kmSrl27pt27dys+Pv7nSfn5KT4+XpmZmWWeJy8vTxaLxeaB8plMJocecL3Y2Fj+twAAAHCSav+coVmzZunSpUsaMmSIpKLP/SkoKFBYWJjNuLCwMB04cKDM88ycOVPTp0936ly9TXJycoljc+bMkdlsLvW5qvTtt9+qVq1alTpH9+7d9e2336pBgwZVNCsAAAD4smoNQx9++KGmT5+utWvXqlGjRpU616RJkzR+/Hjr1xaLRREREZWdolcrbXlZSkqKzGaz05eetW/fvtLnqFWrVpWcBwAAAJCqcZnc8uXL9fjjj2vlypU2S+IaNGggf39/5ebm2ozPzc1V48aNyzxfUFCQQkJCbB6oGkePHpXJZNKIESP07bffatCgQapfv75MJpOOHj0qSVq9erWGDRum1q1bq1atWgoNDVXPnj21atWqUs9Z2p6hESNGyGQy6ciRI3r77bfVvn17BQUFKTIyUtOnT1dhYaHN+LL2DBXvzbl06ZKeffZZhYeHKygoSHfeeac++uijMt/j0KFDVa9ePdWuXVu9evXS1q1bNW3aNJlMJmVkZNj981q7dq3uuusu1axZU2FhYRo1apTOnz9f6tj//Oc/mjBhgrp06aL69esrODhYbdu21cSJE3Xp0qUSP7MtW7ZY/7n4MWLECOuYxYsXa8CAAdYmxnr16ikhIUGbN2+2e/4AAAC+qlruDC1btky///3vtXz5cvXr18/mucDAQHXt2lWbNm3SwIEDJRXtK9q0aZOeeuqp6pgeynDo0CHdfffd6tixo0aMGKEffvhBgYGBkoruzAUGBuree+9VkyZNdPbsWaWlpenhhx/W22+/raefftru7/OHP/xBW7Zs0X/9138pISFBa9as0bRp03Tt2jXNmDHDrnPk5+frgQce0Pnz5zV48GD99NNPWr58uYYMGaL09HQ98MAD1rHZ2dnq0aOHTp8+rQcffFCdO3fWwYMHdf/996t3794O/YyWLl2qpKQkhYSEaPjw4apbt64++eQTxcfH69q1a9afV7HU1FS99957iouLU2xsrAoLC7Vjxw69/vrr2rJli7Zu3Wr9INTk5GSlpKTo2LFjNssYO3XqZP3nsWPHKioqSvHx8WrYsKGys7O1Zs0axcfHKzU1VQMGDHDo/QAAAFTEzoWTdXX9PxT8QB9Fj7Hv729uwXDQxYsXjb179xp79+41JBmzZ8829u7daxw7dswwDMOYOHGiMXz4cOv4v/3tb0ZAQIAxf/584/Tp09bHhQsXrGOWL19uBAUFGSkpKcb+/fuNJ554wqhbt66Rk5Nj97zMZrMhyTCbzWWOuXLlirF//37jypUrjr5trxUZGWn88jI4cuSIIcmQZEydOrXU133//fcljl28eNHo2LGjERoaaly+fNnmOUlGr169bI4lJSUZkowWLVoYp06dsh4/e/asUbduXaNOnTpGXl6e9fjmzZsNSUZycnKp72HAgAE24zdu3GhIMhISEmzG//a3vzUkGTNmzLA5/t5771nf9+bNm0t93zcym81GSEiIccsttxgHDx60Hr927Zrx61//2pBkREZG2rzm5MmTNnMsNn36dEOS8de//tXmeK9evUr873Ojw4cPlzh26tQpIzw83GjTps1N3wO/EwAAoLJ2LHjRMCQj3yTDkIq+djF7soFhGIbDy+S+/PJLde7cWZ07d5YkjR8/Xp07d9bUqVMlSadPn9bx48et4//85z/r+vXrGjt2rJo0aWJ9PPvss9YxQ4cO1axZszR16lR16tRJWVlZSk9PL1GqgOrVuHFjTZ48udTnWrZsWeJY7dq1NWLECJnNZv3rX/+y+/u89NJLatKkifXrBg0aaMCAAbp48aIOHjxo93nefPNNmzsx9913nyIjI23mkpeXp7///e9q1KiRnn/+eZvXjxw5Uu3atbP7+61Zs0YWi0W///3v1bZtW+vxGjVqlHlHq2nTpiXuFkmy3gXduHGj3d9fklq0aFHiWJMmTTR48GB99913OnbsmEPnAwAAcNTV9f/QdZMUYEjXTdKVDemunpLdHF4mFxsbK8Mwynw+JSXF5mt791489dRTXrcsLi1N2rxZiouTEhNdPRvHRUVFlfoXd0k6c+aMXnvtNf3jH//QsWPHdOXKFZvnT506Zff36dq1a4ljzZo1kyRduHDBrnPUrVu31GDQrFkzm4r2gwcPKi8vT926dVNQUJDNWJPJpB49etgdwL766itJUs+ePUs8FxMTo4CAkr9ehmFoyZIlSklJ0b59+2Q2m232Rjnyc5Okw4cPa+bMmfr888+VnZ2tvLw8m+dPnTqlyMhIh84JAADgiOAH+ihgzV5rIKp5/4OunpLdqr1a21ekpUkDBkj+/tKcOdLatZ4XiMq6M/fjjz/qrrvu0vHjx3XPPfcoPj5edevWlb+/v7KysrR27doSfykvT2nlF8VBoqCgwK5zhIaGlno8ICDAJmwUfx5VWW2GjtyNNJvNZZ7L399f9evXL3H8mWee0bx58xQREaHExEQ1adLEGsqmT5/u0M/t0KFD6t69uywWi+Li4tS/f3+FhITIz89PGRkZ2rJli0PnAwAAqIjoMTO0U0V3hGre/6BH7RkiDDnJ5s1FQaigoOjPjAzPC0Nlfdjne++9p+PHj+uVV17RlClTbJ577bXXtHbt2uqYXoUUB68zZ86U+vwvWw3LUxzASjtXQUGBfvjhBzVt2tR67MyZM5o/f77uvPNOZWZm2nzuUk5OjsOfm/Xmm2/q/Pnz+uCDD/Tb3/7W5rnRo0dbm+gAAACqQnklCdFjZkgeFIKKVVu1tq+Ji/s5CBUUSL9olfZo33//vSSV2lT2z3/+s7qn45B27dopKChIu3fvLnHXxDAMmyV1NxMVFSWp9PecmZmp69ev2xw7fPiwDMNQfHx8iQ+gLevn5u/vL6n0O2Rl/e9gGIa++OILO98FAADAze1cOFnRT76qe9buVfSTr2rnwtL3lXsawpCTJCYWLY175hnPXCJXnuI9KNu2bbM5/uGHH+rTTz91xZTsFhQUpIcffli5ubmaM2eOzXNLly7VgQMH7D7XgAEDFBISosWLF+s///mP9Xh+fn6JO2bSzz+37du32yzdO3nypCZNmlTq96hXr54k6cSJE2We75f/O7z22mvat2+f3e8DAADgZjy5JKE8LJNzosRE7wpBxYYPH67XX39dTz/9tDZv3qzIyEh99dVX2rRpkx566CGlpqa6eorlmjlzpjZu3KiJEydqy5Yt1s8Z+uSTT/Tggw8qPT1dfn43/+8EoaGhevvttzVixAjdddddeuSRRxQaGqpPPvlENWvWtGnIk35ueVu1apW6deum++67T7m5ufrkk0903333We/03Kh379766KOPNHjwYPXp00fBwcGKiopS//79NXr0aC1ZskSDBw/WkCFDVL9+fe3YsUN79uxRv379tG7duir7mQEAAN/mySUJ5eHOEBzWrFkzbdmyRffdd582btyod999V9euXdP69evVv39/V0/vpiIiIpSZman//u//1vbt2zVnzhydOXNG69evV+vWrSWVXupQmqSkJK1evVpt2rTR+++/r/fff1/33HOPNm7cWGoTX0pKip5//nmdP39ec+fO1Y4dOzR+/Hh9+OGHpZ5/1KhRmjBhgs6dO6fXX39dL730klatWiVJ6ty5s9avX68uXbooNTVVixcvVt26dfXFF1+oW7duFfzpAAAAlBQ9ZoZ2LnhR2wZ20c4FL3pUSUJ5TEZ5PdkexGKxKDQ0VGazucy/yF69elVHjhxRixYtFBwcXM0zhCe49957lZmZKbPZrNq1a7t6Ok7H7wQAALhReSUJnsSebCCxTA4+6vTp0yWWsf31r3/VF198oQceeMAnghAAAMCNiksSrpukgDV7tVPy6EBkD8IQfNIdd9yhzp07q0OHDtbPR8rIyFCdOnU0a9YsV08PAACg2pVakuDlYYg9Q/BJo0eP1pkzZ7R06VLNmzdPBw8e1KOPPqpdu3apY8eOrp4eAABAtQt+oI81CHlTSUJ5uDMEnzRjxgzNmOHd/6UDAADAEdFjZminiu4I1bz/Qa9fIicRhgAAAACfUl5JQvSYGV6/NO5GhCEAAADAR/hiSUJ52DMEAAAA+IhSSxJ8GGEIAAAA8BG+WJJQHpbJAQAAAD7CF0sSykMYAgAAALwMJQn2IQwBAAAAXoSSBPuxZwgAAADwIpQk2I8wBAAAAHgRShLsRxiC24iNjZXJZHL1NOySkpIik8mklJQUV08FAADARvSYGdq54EVtG9hFOxe8yBK5chCGfIjJZHLoUdWmTZsmk8mkjIyMKj+3J8rIyJDJZNK0adNcPRUAAOCBdi6crC2DumjnwsklnoseM0OxqbsJQjdBgYIPSU5OLnFszpw5MpvNpT5X3ZYuXaqffvrJ1dMAAABwe5QkVA3CkA8p7Q5ESkqKzGazW9yduPXWW109BQAAAI9QakkCYchhLJNDqa5du6bZs2erS5cuuuWWW1SnTh317NlTaWlpJcaazWZNnTpVHTp0UO3atRUSEqLWrVsrKSlJx44dk1S0H2j69OmSpLi4OOtSvObNm1vPU9qeoRv35qxfv149evRQrVq1VL9+fSUlJemHH34odf7vvvuubr/9dgUHBysiIkITJkzQ1atXZTKZFBsba/fP4ccff9To0aMVFhamWrVq6a677tLq1avLHL948WINGDBAzZs3V3BwsOrVq6eEhARt3rzZZty0adMUFxcnSZo+fbrN8sSjR49Kkv7zn/9owoQJ6tKli+rXr6/g4GC1bdtWEydO1KVLl+x+DwAAwPtQklA1uDOEEvLy8vTggw8qIyNDnTp10mOPPab8/HytW7dOAwYM0Ny5c/XUU09JkgzDUEJCgnbu3Kl77rlHDz74oPz8/HTs2DGlpaVp+PDhioyM1IgRIyRJW7ZsUVJSkjUE1a1b1645paWlad26derfv7969OihrVu3aunSpfr++++1bds2m7FTp07VK6+8orCwMI0aNUo1atTQypUrdeDAAYd+Dj/99JNiY2P19ddfKyYmRr169dKJEyc0dOhQPfDAA6W+ZuzYsYqKilJ8fLwaNmyo7OxsrVmzRvHx8UpNTdWAAQMkFQW/o0eP6v3331evXr1sAlrxzyQ1NVXvvfee4uLiFBsbq8LCQu3YsUOvv/66tmzZoq1bt6pGjRoOvScAAOBZ0g6mafORzYprEafEdonW49FjZminiu4I1bz/QZbIVZThJcxmsyHJMJvNZY65cuWKsX//fuPKlSvVODP3FhkZafzyMnjxxRcNScZLL71kFBYWWo9bLBajW7duRmBgoJGdnW0YhmH8+9//NiQZAwcOLHHuq1evGhcvXrR+nZycbEgyNm/eXOpcevXqVWIuS5YsMSQZAQEBxrZt26zHr1+/bsTGxhqSjMzMTOvxgwcPGv7+/kbTpk2N3Nxcm7l36NDBkGT06tXr5j+YG+Y7atQom+Pp6emGJEOSsWTJEpvnDh8+XOI8p06dMsLDw402bdrYHN+8ebMhyUhOTi71+588edLIy8srcXz69OmGJOOvf/2rXe+jPPxOAADgvtYeWGv0f0TGnLtNRv9HZKw9sNbVU/IY9mQDwzAMlsk5UdrBNI1LH6e0gyWXlrmrwsJCLVy4UK1atbIu3ypWp04dTZ06VdeuXVNqaqrN62rWrFniXEFBQapdu3aVzOvRRx/VPffcY/3a399fSUlJkqR//etf1uPLli1TQUGBnn/+eTVq1Mhm7lOmTHHoey5dulSBgYF6+eWXbY4nJCTovvvuK/U1LVq0KHGsSZMmGjx4sL777jvrskF7NG3aVIGBgSWOF9+V27hxo93nAgAAnufMsr8obbk0dqehtOXS2WXvuXpKXodlck6SdjBNA5YPkL/JX3N2ztHaR9ba3Np0VwcPHtT58+cVHh5u3eNzo7Nnz0qSdcnZbbfdpjvvvFPLli3TyZMnNXDgQMXGxqpTp07y86u6rN21a9cSx5o1ayZJunDhgvXYV199JUm69957S4y/MUzdjMVi0ZEjR9ShQwc1bty4xPM9e/bUpk2bShw/fPiwZs6cqc8//1zZ2dnKy8uzef7UqVOKjIy0aw6GYWjJkiVKSUnRvn37ZDabVVhYaHMuAADgveKOyqYkIfaoq2fkfQhDTrL5yGb5m/xVYBTI3+SvjKMZHhGGfvzxR0nSN998o2+++abMcZcvX5YkBQQE6PPPP9e0adO0atUqPf/885Kkhg0b6qmnntLkyZPl7+9f6XmFhISUOBYQUHT5FhQUWI9ZLBZJsrkrVCwsLMzu71feeco616FDh9S9e3dZLBbFxcWpf//+CgkJkZ+fnzIyMrRly5YS4ag8zzzzjObNm6eIiAglJiaqSZMmCgoKklRUuuDIuQAAgOdp9dDj0vsfq8DPpIBCQ60eeszVU/I6hCEniWsRpzk751gDUWzzWFdPyS7FoWPw4MH66KOP7HpN/fr1NXfuXL399ts6cOCAPv/8c82dO1fJycmqUaOGJk2a5Mwp2yie/5kzZ0rcgcnNza3QeUpT2rnefPNNnT9/Xh988IF++9vf2jw3evRobdmyxe7vf+bMGc2fP1933nmnMjMzVatWLetzOTk5pd61AwAAnqmskgQlJkpr18o/I0OKjS36GlWKPUNOktguUWsfWatnop/xmCVyUtGyt5CQEH355ZfKz8936LUmk0m33Xabxo4dqw0bNkiSTRV38R2iG+/kVLWoqChJ0hdffFHiue3bt9t9npCQELVo0UKHDh1STk5Oief/+c9/ljj2/fffS5K1Ma6YYRilzqe8n8fhw4dlGIbi4+NtglBZ3xsAAHimtINp+su0AWo+/S39ZdqAknvNExOl2bMJQk5CGHKixHaJmp0w22OCkFS09GzMmDE6duyYXnjhhVID0b59+6x3TI4ePWr9XJwbFd85CQ4Oth6rV6+eJOnEiRNOmHmRRx55RH5+fnrjjTd07tw56/HLly9rxgzHKieHDx+ua9euaerUqTbH169fX+p+oeI7Ub+s+n7ttde0b9++EuPL+3kUn2v79u02+4ROnjxZrXfaAACAc1GS4Fosk0MJ06dP1549e/T2229r3bp1+vWvf61GjRopOztbX3/9tb766itlZmaqUaNGysrK0kMPPaTu3btbywaKP1vHz89P48aNs563+MNWX3zxRX3zzTcKDQ1V3bp1re1oVaFdu3aaOHGiXn31VXXs2FFDhgxRQECAUlNT1bFjR+3bt8/uYocJEyYoNTVVixYt0jfffKNf//rXOnHihFauXKl+/fpp3bp1NuNHjx6tJUuWaPDgwRoyZIjq16+vHTt2aM+ePaWOb9++vcLDw7V8+XIFBQWpWbNmMplMevrpp60NdKtWrVK3bt103333KTc3V5988onuu+8+610oAADg2ShJcC3uDKGEoKAg/eMf/9C7776rxo0ba9WqVZozZ462bt2qJk2aaOHCherYsaMkqVu3bvrjH/8ok8mkdevW6Y033lBGRobi4+P1xRdfKPGGW7odOnTQkiVL1KBBA82dO1cvvfSSZs2aVeXznzFjhhYsWKBf/epXeuedd7Ry5Uo9/PDDWrBggaTSyxhKc8stt2jLli164okn9N1332nOnDk6cOCAVqxYoYcffrjE+M6dO2v9+vXq0qWLUlNTtXjxYtWtW1dffPGFunXrVmK8v7+/UlNTdffdd2vZsmWaOnWqXnrpJZ0/f16SlJKSoueff17nz5/X3LlztWPHDo0fP14ffvhhJX46AADAnbR66HEFGCoqSTBESUI1MxmGYbh6ElXBYrEoNDRUZrO5zL/sXr16VUeOHFGLFi1slm/BN2zcuFH333+/JkyYoNdff93V03EL/E4AAOB8ZRYkWAekSZQkVCl7soHEMjl4obNnz6pevXo2ld4XLlyw7rUZOHCgi2YGAAB8TXFBwn1HTfpL8znStFKKtRITCUEuQhiC1/nb3/6mWbNmqXfv3goPD9fp06eVnp6uM2fOaMSIEYqJiXH1FAEAgI8oLki4bjL07A7pvXbvSdMIPu6CMASv06NHD3Xt2lUbN27Ujz/+KH9/f91222166aWX9OSTT7p6egAAwIdQkODeCEPwOt27d9fatWtdPQ0AAAC1euhx6f2PiwoSCg0KEtwMYQgAAACopDJLEhITpbVr5U9BglsiDAEAAACVcNOSBAoS3BafMwQAAABUQnFJwtidhtKWS2eXvefqKcFOhCEAAACgEihJ8FyEIQAAAKASWj30uAIMFZUkGKIkwYOwZwgAAACww86Fk3V1/T8U/EAfRY+Z8fMTlCR4LMIQAAAAcBM7F05W9JOvFi2HW7NXO6WSgYgQ5HFYJgcAAADcxNX1/7DZF3RlQ7qrp4QqQBgCAAAAbiL4gT7WIBRgSDXvf9DVU0IVIAzB5Y4ePSqTyaQRI0bYHI+NjZXJZHLa923evLmaN2/utPMDAADvET1mhnYueFHbBnbRzgUv2i6Rg8ciDPmY4uBx4yMwMFARERF69NFH9e9//9vVU6wyI0aMkMlk0tGjR109FQAA4CF2LpysLYO6aOfCySWeix4zQ7GpuwlCXoQCBR/VqlUr/fa3v5UkXbp0STt27NCyZcuUmpqqTZs26Z577nHxDKWlS5fqp59+ctr5N23a5LRzAwAAz3PTkgR4HcKQj2rdurWmTZtmc2zKlCmaMWOGJk+erIyMDJfM60a33nqrU8/fqlUrp54fAAB4llJLEghDXo1lcrB6+umnJUn/+te/JEkmk0mxsbHKzs7W7373OzVu3Fh+fn42QWnr1q3q37+/GjRooKCgILVp00ZTpkwp9Y5OQUGBXn/9dbVu3VrBwcFq3bq1Zs6cqcLCwlLnU96eobVr1+qBBx5Q/fr1FRwcrObNm2v48OHat2+fpKL9QO+//74kqUWLFtYlgbGxsdZzlLVn6PLly0pOTlb79u0VHBysevXqqV+/fvriiy9KjJ02bZpMJpMyMjL04YcfqlOnTqpZs6aaNGmiZ599VleuXCnxmlWrVqlXr15q1KiRgoODFR4ervj4eK1atarU9woAAKoHJQm+hztDKOHGAPLDDz8oJiZG9erV0yOPPKKrV68qJCREkrRw4UKNHTtWdevWVf/+/dWoUSN9+eWXmjFjhjZv3qzNmzcrMDDQeq4nnnhCixcvVosWLTR27FhdvXpVs2fP1vbt2x2a3/PPP6/Zs2erXr16GjhwoBo1aqQTJ05o48aN6tq1q+644w4999xzSklJ0VdffaVnn31WdevWlaSbFiZcvXpVvXv31q5du9SlSxc999xzys3N1YoVK/TZZ59p2bJl+u///u8Sr5s3b57S09M1YMAA9e7dW+np6Xr77bd17tw5/e1vf7OOW7hwoZ588kk1adJEgwYNUv369ZWTk6Ndu3Zp9erVGjx4sEM/CwAAUHWix8zQThXdEap5/4MskfMFhpcwm82GJMNsNpc55sqVK8b+/fuNK1euVOPM3MuRI0cMSUZCQkKJ56ZOnWpIMuLi4gzDMAxJhiRj5MiRxvXr123GfvPNN0ZAQIARFRVlnDt3zua5mTNnGpKMWbNmWY9t3rzZkGRERUUZly5dsh4/efKk0aBBA0OSkZSUZHOeXr16Gb+8RD/++GNDktGxY8cS3zc/P9/Iycmxfp2UlGRIMo4cOVLqzyIyMtKIjIy0OTZ9+nRDkvGb3/zGKCwstB7fs2ePERgYaNStW9ewWCzW48nJyYYkIzQ01Dhw4ID1+E8//WS0bdvW8PPzM7Kzs63Hu3TpYgQGBhq5ubkl5vPL91Md+J0AAPiiHQteNDIGdjZ2LHjR1VOBk9iTDQzDMFgm50xpadK4cUV/uplDhw5p2rRpmjZtmv7whz/o17/+tV5++WUFBwdrxoyf/ytIYGCg/vSnP8nf39/m9e+++66uX7+uuXPnqn79+jbPTZgwQQ0bNtSyZcusx5YuXSpJmjp1qm655Rbr8aZNm+rZZ5+1e94LFiyQJL311lslvm9AQIDCwsLsPldp3n//fdWoUUOvvfaazR2yzp07KykpSRcuXNCaNWtKvO7ZZ59Vu3btrF/XrFlTw4YNU2FhoXbv3m0ztkaNGqpRo0aJc/zy/QAAgKpXXJJwz9q9in7y1VJb4+A7WCbnLGlp0oABkr+/NGeOtHatlJjo6llZff/995o+fbqkor+ch4WF6dFHH9XEiRPVsWNH67gWLVqoQYMGJV6/Y8cOSdJnn31WaitbjRo1dODAAevXX331lSSpZ8+eJcaWdqwsu3btUlBQkHr16mX3a+xlsVh0+PBh3XbbbWrWrFmJ5+Pi4rRo0SJlZWVp+PDhNs917dq1xPjic1y4cMF67JFHHtGECRN0xx136NFHH1VcXJzuvfde69JDAADgXJQk4EaEIWfZvLkoCBUUFP2ZkeFWYSghIUHp6ek3HVfWnZYff/xRkmzuIpXHbDbLz8+v1GDlyN0cs9mspk2bys+v6m9qWiyWcufTpEkTm3E3Ki3MBAQU/XoVFBRYj73wwguqX7++Fi5cqDfeeEOzZs1SQECA+vXrpzfffFMtWrSo9PsAAABlC36gjwLW7KUkAZJok3OeuLifg1BBgXRDi5knKavNrfgv/xaLRYZhlPkoFhoaqsLCQp07d67EuXJzc+2eT926dZWTk1NmA11lFL+nsuaTk5NjM64iTCaTfv/73+tf//qXzp49q9WrV+uhhx7S2rVr9V//9V82wQkAAFS96DEztHPBi9o2sIt2LniRkgQfRxhylsTEoqVxzzzjdkvkqkJ0dLSkn5fL3UxUVJQk6Z///GeJ50o7Vpbu3bsrLy9PW7ZsuenY4n1O9gaMkJAQtWzZUocOHVJ2dnaJ54srxTt16mT3fMtTv359DRw4UCtWrFDv3r21f/9+HTp0qErODQCAr9u5cLK2DOpS6p6g6DEzFJu6myAEwpBTJSZKs2d7XRCSpCeffFIBAQF6+umndfz48RLPX7hwQXv37rV+XbzH5uWXX9bly5etx7Ozs/XWW2/Z/X3Hjh0rqaiwoHipXrHr16/b3NWpV6+eJOnEiRN2nz8pKUn5+fmaNGmSzZ2tf//730pJSVFoaKgGDhxo9/l+KSMjw+a8kpSfn299L8HBwRU+NwAAKEJJAuzFniFUyB133KEFCxZozJgxateunfr27atWrVrp4sWLOnz4sLZs2aIRI0bonXfekVRUPjBy5EgtWbJEHTt21KBBg5SXl6cVK1bo7rvv1ieffGLX9+3bt69eeOEFzZo1S23atNGgQYPUqFEjZWdna9OmTXrhhRf03HPPSZJ69+6tWbNm6YknntDgwYN1yy23KDIyskT5wY0mTJigdevW6YMPPtC3336r++67T2fOnNGKFSt0/fp1LVq0SHXq1Knwz23gwIEKCQnR3XffrcjISOXn52vDhg3av3+/Hn74YUVGRlb43AAAoAglCbAXYQgVNmrUKHXq1EmzZ8/W1q1b9fHHHys0NFS33nqrxo0bp6SkJJvxixYtUtu2bbVo0SLNmzdPzZo10/jx4zVkyBC7w5Ak/e///q9iYmI0b948ffTRR7p69aqaNGmi3r176/7777eO69Onj/70pz9p0aJFeuONN5Sfn69evXqVG4aCg4P1+eef6/XXX9eKFSv05ptvqlatWurVq5defPFF3XvvvY7/oG4wc+ZMpaena9euXfr44491yy23qFWrVlq4cKEee+yxSp0bAAAUoSQB9jIZv1yz46EsFotCQ0NlNpvL3OB+9epVHTlyRC1atGA5EiB+JwAA3mvnwsm6siFdNe9/kL1BPsiebCBxZwgAAAAeaufCybq6/h8KfqBPicATPWYGS+NwU4QhAAAAeJzikoTrJilgzV7tlLgDBIfRJgcAAACPU2pJAuAgwhAAAAA8TvADfaxBiJIEVBTL5AAAAOBxosfM0E6JkgRUCmEIAAAAbouSBDiTT4YhL2kTByqN3wUAgDujJAHO5vCeoa1bt6p///4KDw+XyWTSmjVryh1/+vRpPfroo2rbtq38/Pz03HPPlRiTkpIik8lk83DGZ574+/tLkvLz86v83IAnKv5dKP7dAADAnVCSAGdzOAxdvnxZUVFRmj9/vl3j8/Ly1LBhQ02ZMkVRUVFljgsJCdHp06etj2PHjjk6tZuqUaOGgoKCZDab+S/i8HmGYchsNisoKEg1atRw9XQAACiBkgQ4m8PL5Pr06aM+ffrYPb558+Z66623JEmLFy8uc5zJZFLjxo3tPm9eXp7y8vKsX1ssFrte16BBA2VnZ+vkyZMKDQ1VjRo1ZDKZ7P6+gKczDEP5+fkym826dOmSmjZt6uopAQBQKkoS4Gxus2fo0qVLioyMVGFhobp06aJXX31Vt99+e5njZ86cqenTpzv8fUJCQiRJ586dU3Z2doXnC3i6oKAgNW3a1Po7AQCAq1CSAFdxizDUrl07LV68WHfeeafMZrNmzZqlHj166JtvvlGzZs1Kfc2kSZM0fvx469cWi0URERF2fb+QkBCFhIQoPz9fBQUFVfIeAE/i7+/P0jgAgFugJAGu5BZhKCYmRjExMdave/Toodtuu03vvvuuXnnllVJfExQUpKCgoEp93xo1avAXQgAAABcqtSSBMIRq4nCBQnWoUaOGOnfurEOHDrl6KgAAAHAiShLgSm5xZ+iXCgoK9PXXX6tv376ungoAAACciJIEuJLDYejSpUs2d2yOHDmirKws1atXT7feeqsmTZqk7OxsLV261DomKyvL+tqzZ88qKytLgYGB6tChgyTp5Zdf1t13363WrVvrwoUL+t///V8dO3ZMjz/+eCXfHgAAAFytvIIEiZIEuI7DYejLL79UXFyc9eviEoOkpCSlpKTo9OnTOn78uM1rOnfubP3n3bt368MPP1RkZKSOHj0qSTp//rxGjRqlnJwc/epXv1LXrl21fft2a1gCAACAZ6IgAe7MZHjJp49aLBaFhobKbDZTFQwAAOAmtgzqonvW7rXuC9o2sItiU3e7elrwcvZmA7csUAAAAIB3oCAB7swtCxQAAADgHShIgDtjmRwAAAAq7WYlCUB1sjcbcGcIAAAAlUJJAjwVe4YAAABQKVfX/8O6J+i6qWhJHOAJCEMAAACoFEoS4KlYJgcAAIBKoSQBnooCBQAAANiFkgR4CgoUAAAAUGUoSYA3Ys8QAAAAboqSBHgjwhAAAABuipIEeCOWyQEAAOCmKEmAN6JAAQAAAFZpadLmzVJcnJSY6OrZABVjbzZgmRwAAAAkFQWhAQOkuXOL/kxLc/WMAOciDAEAAEBS0R0hf3+poKDoz4wMV88IcC7CEAAAACQVLY0rDkIFBVJsrKtnBDgXBQoAAACQVLRHaO3aojtCsbHsGYL3IwwBAAD4mPJKEhITCUHwHSyTAwAA8CGUJAA/IwwBAAD4EEoSgJ8RhgAAAHwIJQnAz9gzBAAA4EMoSQB+RhgCAADwQpQkADfHMjkAAAAvQ0kCYB/CEAAAgJehJAGwD2EIAADAy1CSANiHPUMAAABehpIEwD6EIQAAAA9FSQJQOSyTAwAA8ECUJACVRxgCAADwQJQkAJVHGAIAAPBAlCQAlceeIQAAAA9ESQJQeYQhAAAAN0ZJAuA8LJMDAABwU5QkAM5FGAIAAHBTlCQAzkUYAgAAcFOUJADOxZ4hAAAAN0VJAuBchCEAAAAXKq8gQaIkAXAmlskBAAC4CAUJgGsRhgAAAFyEggTAtQhDAAAALkJBAuBa7BkCAABwsrL2BVGQALiWyTAMw9WTqAoWi0WhoaEym80KCQlx9XQAAAAk/bwvqPjuz9q1hB7A2ezNBiyTAwAAcCL2BQHuizAEAADgROwLAtwXe4YAAACciH1BgPsiDAEAAFSB8j48lQ9OBdwTy+QAAAAqiQ9PBTwTYQgAAKCSKEkAPBNhCAAAoJIoSQA8E3uGAAAAKomSBMAzEYYAAADsREkC4F1YJgcAAGAHShIA70MYAgAAsAMlCYD3IQwBAADYgZIEwPuwZwgAAMAOlCQA3ocwBAAAcANKEgDfwTI5AACA/0NJAuBbCEMAAAD/h5IEwLcQhgAAAP4PJQmAb2HPEAAAwP+hJAHwLYQhAADgcyhJACCxTA4AAPgYShIAFCMMAQAAn0JJAoBihCEAAOBTKEkAUIw9QwAAwKdQkgCgGGEIAAB4JUoSANwMy+QAAIDXoSQBgD0IQwAAwOtQkgDAHoQhAADgdShJAGAP9gwBAACvQ0kCAHs4fGdo69at6t+/v8LDw2UymbRmzZpyx58+fVqPPvqo2rZtKz8/Pz333HOljvv73/+u9u3bKzg4WB07dtSnn37q6NQAAICPSUuTxo0rfU9QYqI0ezZBCEDZHA5Dly9fVlRUlObPn2/X+Ly8PDVs2FBTpkxRVFRUqWO2b9+uYcOG6bHHHtPevXs1cOBADRw4UPv27XN0egAAwEdQkgCgskyGYRgVfrHJpNWrV2vgwIF2jY+NjVWnTp00Z84cm+NDhw7V5cuX9cknn1iP3X333erUqZPeeecdu85tsVgUGhoqs9mskJAQe98CAADwUOPGFQWh4r1BzzxTdCcIAOzNBm5RoJCZman4+HibYwkJCcrMzCzzNXl5ebJYLDYPAADgOyhJAFBZbhGGcnJyFBYWZnMsLCxMOTk5Zb5m5syZCg0NtT4iIiKcPU0AAOBGiksSnnmm6E/2BgFwlFuEoYqYNGmSzGaz9XHixAlXTwkAADgBJQkAnMUtqrUbN26s3Nxcm2O5ublq3Lhxma8JCgpSUFCQs6cGAABcqLgkwd9fmjOHO0AAqpZb3BmKiYnRpk2bbI5t2LBBMTExLpoRAABwB5s3/7wnyN+/6HODAKCqOHxn6NKlSzp06JD16yNHjigrK0v16tXTrbfeqkmTJik7O1tLly61jsnKyrK+9uzZs8rKylJgYKA6dOggSXr22WfVq1cvvfHGG+rXr5+WL1+uL7/8Un/+858r+fYAAIAni4sruiNESQIAZ3C4WjsjI0NxcXEljiclJSklJUUjRozQ0aNHlXHDf7oxmUwlxkdGRuro0aPWr//+979rypQpOnr0qNq0aaM//elP6tu3r93zolobAADvlJZWdEcoNpYlcgDsY282qNTnDLkTwhAAAJ4pLa1oOVxcHGEHQNXwqM8ZAgAAvqm4IGHu3KI/S2uMAwBnIQwBAACXoSABgCsRhgAAgMvExf0chChIAFDd3OJzhgAAgG9KTCz67CAKEgC4AmEIAAA4XXklCYmJhCAArsEyOQAA4FSUJABwV4QhAADgVJQkAHBXhCEAAOBUlCQAcFfsGQIAAE5FSQIAd0UYAgAAVYKSBACehmVyAACg0ihJAOCJCEMAAKDSKEkA4IkIQwAAoNIoSQDgidgzBAAAKo2SBACeiDAEAADsRkkCAG/CMjkAAGAXShIAeBvCEAAAsAslCQC8DWEIAADYhZIEAN6GPUMAAMAulCQA8DaEIQAAYIOSBAC+gmVyAADAipIEAL6EMAQAAKwoSQDgSwhDAADAipIEAL6EPUMAAMCKkgQAvoQwBACAD6IkAQBYJgcAgM+hJAEAihCGAADwMZQkAEARwhAAAD6GkgQAKMKeIQAAfAwlCQBQhDAEAICXoiQBAMrHMjkAALwQJQkAcHOEIQAAvBAlCQBwc4QhAAC8ECUJAHBz7BkCAMALUZIAADdHGAIAwINRkgAAFccyOQAAPBQlCQBQOYQhAAA8FCUJAFA5hCEAADwUJQkAUDnsGQIAwENRkgAAlUMYAgDAjZVXkCBRkgAAlcEyOQAA3BQFCQDgXIQhAADcFAUJAOBchCEAANwUBQkA4FzsGQIAwE1RkAAAzkUYAgDAxcorSaAgAQCch2VyAAC4ECUJAOA6hCEAAFyIkgQAcB3CEAAALkRJAgC4DnuGAABwIUoSAMB1CEMAAFQDShIAwP2wTA4AACejJAEA3BNhCAAAJ6MkAQDcE2EIAAAnoyQBANwTe4YAAHAyShIAwD0RhgAAqCKUJACAZ2GZHAAAVYCSBADwPIQhAACqACUJAOB5CEMAAFQBShIAwPOwZwgAAAeUtS+IkgQA8DwmwzAMV0+iKlgsFoWGhspsNiskJMTV0wEAeKHifUHFd3/WriX0AIA7sjcbsEwOAAA7sS8IALwLYQgAADuxLwgAvAt7hgAAsBP7ggDAuxCGAAD4BT48FQB8A8vkAAC4AR+eCgC+gzAEAMANKEkAAN9BGAIA4AaUJACA72DPEAAAN6AkAQB8B2EIAOCTKEkAALBMDgDgcyhJAABIhCEAgA+iJAEAIBGGAAA+iJIEAIBUgTC0detW9e/fX+Hh4TKZTFqzZs1NX5ORkaEuXbooKChIrVu3VkpKis3z06ZNk8lksnm0b9/e0akBAGCX4pKEZ54p+pP9QQDgmxwOQ5cvX1ZUVJTmz59v1/gjR46oX79+iouLU1ZWlp577jk9/vjj+uyzz2zG3X777Tp9+rT1sW3bNkenBgCAjbQ0ady40vcEJSZKs2cThADAlzncJtenTx/16dPH7vHvvPOOWrRooTfeeEOSdNttt2nbtm168803lZCQ8PNEAgLUuHFju8+bl5envLw869cWi8Xu1wIAvF9xSYK/vzRnDneAAAAlOX3PUGZmpuLj422OJSQkKDMz0+bYd999p/DwcLVs2VK/+c1vdPz48XLPO3PmTIWGhlofERERVT53AIDnoiQBAHAzTg9DOTk5CgsLszkWFhYmi8WiK1euSJKio6OVkpKi9PR0LVy4UEeOHFHPnj118eLFMs87adIkmc1m6+PEiRNOfR8AAM9CSQIA4Gbc4kNXb1x2d+eddyo6OlqRkZFauXKlHnvssVJfExQUpKCgoOqaIgDAwxSXJGRkFAUhlsgBAH7J6WGocePGys3NtTmWm5urkJAQ1axZs9TX1K1bV23bttWhQ4ecPT0AgIdLSytaEhcXVzLwJCYSggAAZXP6MrmYmBht2rTJ5tiGDRsUExNT5msuXbqk77//Xk2aNHH29AAAHqy4JGHu3KI/S2uNAwCgLA6HoUuXLikrK0tZWVmSiqqzs7KyrIUHkyZN0u9+9zvr+NGjR+vw4cOaMGGCDhw4oAULFmjlypUaN26cdcwLL7ygLVu26OjRo9q+fbsGDRokf39/DRs2rJJvDwDgzShJAABUhsNh6Msvv1Tnzp3VuXNnSdL48ePVuXNnTZ06VZJ0+vRpmya4Fi1aaN26ddqwYYOioqL0xhtv6C9/+YtNrfbJkyc1bNgwtWvXTkOGDFH9+vW1Y8cONWzYsLLvDwDgxShJAABUhskwDMPVk6gKFotFoaGhMpvNCgkJcfV0AADVJC2NkgQAgC17s4FbtMkBAFCW8goSJEoSAAAV5/QCBQAAKoqCBACAMxGGAABui4IEAIAzEYYAAG6LggQAgDOxZwgA4LYSE6W1aylIAAA4B2EIAOBy5ZUkUJAAAHAWlskBAFyKkgQAgKsQhgAALkVJAgDAVQhDAACXoiQBAOAq7BkCALgUJQkAAFchDAEAqgUlCQAAd8MyOQCA01GSAABwR4QhAIDTUZIAAHBHhCEAgNNRkgAAcEfsGQIAOB0lCQAAd0QYAgBUGUoSAACehGVyAIAqQUkCAMDTEIYAAFWCkgQAgKchDAEAqgQlCQAAT8OeIQBAlaAkAQDgaQhDAACHUJIAAPAWLJMDANiNkgQAgDchDAEA7EZJAgDAmxCGAAB2oyQBAOBN2DMEALAbJQkAAG9CGAIAlEBJAgDAF7BMDgBgg5IEAICvIAwBAGxQkgAA8BWEIQCADUoSAAC+gj1DAAAblCQAAHwFYQgAfBQlCQAAX8cyOQDwQZQkAABAGAIAn0RJAgAAhCEA8EmUJAAAwJ4hAPBJlCQAAEAYAgCvRkkCAABlY5kcAHgpShIAACgfYQgAvBQlCQAAlI8wBABeipIEAADKx54hAPBSlCQAAFA+whAAeLDyChIkShIAACgPy+QAwENRkAAAQOUQhgDAQ1GQAABA5RCGAMBDUZAAAEDlsGcIADwUBQkAAFQOYQgA3Fx5JQkUJAAAUHEskwMAN0ZJAgAAzkMYAgA3RkkCAADOQxgCADdGSQIAAM7DniEAcGOUJAAA4DyEIQBwA5QkAABQ/VgmBwAuRkkCAACuQRgCABejJAEAANcgDAGAi1GSAACAa7BnCABcjJIEAABcgzAEANWEkgQAANwLy+QAoBpQkgAAgPshDAFANaAkAQAA90MYAoBqQEkCAADuhz1DAFANKEkAAMD9EIYAoApRkgAAgOdgmRwAVBFKEgAA8CyEIQCoIpQkAADgWQhDAFBFKEkAAMCzsGcIAKoIJQkAAHgWwhAAOIiSBAAAvAPL5ADAAZQkAADgPQhDAOAAShIAAPAehCEAcAAlCQAAeA/2DAFAKcraF0RJAgAA3sNkGIbh6klUBYvFotDQUJnNZoWEhLh6OgA8WPG+oOK7P2vXEnoAAPAk9mYDh5fJbd26Vf3791d4eLhMJpPWrFlz09dkZGSoS5cuCgoKUuvWrZWSklJizPz589W8eXMFBwcrOjpau3btcnRqAFAl2BcEAIBvcDgMXb58WVFRUZo/f75d448cOaJ+/fopLi5OWVlZeu655/T444/rs88+s45ZsWKFxo8fr+TkZO3Zs0dRUVFKSEjQmTNnHJ0eAFQa+4IAAPANlVomZzKZtHr1ag0cOLDMMX/84x+1bt067du3z3rskUce0YULF5Seni5Jio6O1l133aV58+ZJkgoLCxUREaGnn35aEydOtGsuLJMDUJXS0tgXBACAp3LaMjlHZWZmKj4+3uZYQkKCMjMzJUnXrl3T7t27bcb4+fkpPj7eOqY0eXl5slgsNg8AcERamjRuXOmfFZSYKM2eTRACAMCbOT0M5eTkKCwszOZYWFiYLBaLrly5onPnzqmgoKDUMTk5OWWed+bMmQoNDbU+IiIinDJ/AN6JD08FAAAe+zlDkyZNktlstj5OnDjh6ikB8CCUJAAAAKeHocaNGys3N9fmWG5urkJCQlSzZk01aNBA/v7+pY5p3LhxmecNCgpSSEiIzQMA7EVJAgAAcHoYiomJ0aZNm2yObdiwQTExMZKkwMBAde3a1WZMYWGhNm3aZB0DAFWt+MNTn3mGzxECAMBXBTj6gkuXLunQoUPWr48cOaKsrCzVq1dPt956qyZNmqTs7GwtXbpUkjR69GjNmzdPEyZM0O9//3t9/vnnWrlypdatW2c9x/jx45WUlKRu3bqpe/fumjNnji5fvqyRI0dWwVsE4MvS0oqWxMXFlQw8iYmEIAAAfJnDYejLL79UXFyc9evx48dLkpKSkpSSkqLTp0/r+PHj1udbtGihdevWady4cXrrrbfUrFkz/eUvf1FCQoJ1zNChQ3X27FlNnTpVOTk56tSpk9LT00uUKgCAI4pLEvz9pTlzuAMEAABsVepzhtwJnzME4JfGjStqiyveG/TMM0V12QAAwLu5zecMAYCrUJIAAADK4/AyOQDwFMUlCRkZRUGIJXIAAOBGhCEAHq28ggSJkgQAAFA2lskB8FjFBQlz5xb9mZbm6hkBAABPQhgC4LE2b/55P5C/f9FyOAAAAHsRhgB4LAoSAABAZbBnCIDHoiABAABUBmEIgNsrrySBggQAAFBRLJMD4NYoSQAAAM5CGALg1ihJAAAAzkIYAuDWKEkAAADOwp4hAG6NkgQAAOAshCEAboGSBAAAUN1YJgfA5ShJAAAArkAYAuBylCQAAABXIAwBcDlKEgAAgCuwZwiAy1GSAAAAXIEwBKDaUJIAAADcCcvkAFQLShIAAIC7IQwBqBaUJAAAAHdDGAJQLShJAAAA7oY9QwCqBSUJAADA3RCGAFQpShIAAICnYJkcgCpDSQIAAPAkhCEAVYaSBAAA4EkIQwCqDCUJAADAk7BnCECVoSQBAAB4EsIQAIdRkgAAALwBy+QAOISSBAAA4C0IQwAcQkkCAADwFoQhAA6hJAEAAHgL9gwBcAglCQAAwFsQhgCUipIEAADg7VgmB6AEShIAAIAvIAwBKIGSBAAA4AsIQwBKoCQBAAD4AvYMASiBkgQAAOALCEOAD6MkAQAA+DKWyQE+ipIEAADg6whDgI+iJAEAAPg6whDgoyhJAAAAvo49Q4CPoiQBAAD4OsIQ4OUoSQAAACgdy+QAL0ZJAgAAQNkIQ4AXoyQBAACgbIQhwItRkgAAAFA29gwBXoySBAAAgLIRhgAPV15BgkRJAgAAQFlYJgd4MAoSAAAAKo4wBHgwChIAAAAqjjAEeDAKEgAAACqOPUOAB6MgAQAAoOIIQ4AHKK8kgYIEAACAimGZHODmKEkAAABwDsIQ4OYoSQAAAHAOwhDg5ihJAAAAcA72DAFujpIEAAAA5yAMAW6CkgQAAIDqxTI5wA1QkgAAAFD9CEOAG6AkAQAAoPoRhgA3QEkCAABA9WPPEOAGKEkAAACofoQhoBpRkgAAAOA+WCYHVBNKEgAAANwLYQioJpQkAAAAuBfCEFBNKEkAAABwL+wZAqoJJQkAAADuhTAEVDFKEgAAADwDy+SAKkRJAgAAgOcgDAFViJIEAAAAz0EYAqoQJQkAAACegz1DQBWiJAEAAMBzVOjO0Pz589W8eXMFBwcrOjpau3btKnNsfn6+Xn75ZbVq1UrBwcGKiopSenq6zZhp06bJZDLZPNq3b1+RqQHVIi1NGjeu9D1BiYnS7NkEIQAAAHfncBhasWKFxo8fr+TkZO3Zs0dRUVFKSEjQmTNnSh0/ZcoUvfvuu5o7d67279+v0aNHa9CgQdq7d6/NuNtvv12nT5+2PrZt21axdwQ4GSUJAAAA3sHhMDR79myNGjVKI0eOVIcOHfTOO++oVq1aWrx4canjP/jgA7344ovq27evWrZsqTFjxqhv37564403bMYFBASocePG1keDBg3KnUdeXp4sFovNA6gOlCQAAAB4B4fC0LVr17R7927Fx8f/fAI/P8XHxyszM7PU1+Tl5Sk4ONjmWM2aNUvc+fnuu+8UHh6uli1b6je/+Y2OHz9e7lxmzpyp0NBQ6yMiIsKRtwJUGCUJAAAA3sGhMHTu3DkVFBQoLCzM5nhYWJhycnJKfU1CQoJmz56t7777ToWFhdqwYYNSU1N1+vRp65jo6GilpKQoPT1dCxcu1JEjR9SzZ09dvHixzLlMmjRJZrPZ+jhx4oQjbwWosOKShGeeKfqTvUEAAACeyeltcm+99ZZGjRql9u3by2QyqVWrVho5cqTNsro+ffpY//nOO+9UdHS0IiMjtXLlSj322GOlnjcoKEhBQUHOnj58WFpa0ZK4uLiSgScxkRAEAADg6Ry6M9SgQQP5+/srNzfX5nhubq4aN25c6msaNmyoNWvW6PLlyzp27JgOHDig2rVrq2XLlmV+n7p166pt27Y6dOiQI9MDqgwlCQAAAN7PoTAUGBiorl27atOmTdZjhYWF2rRpk2JiYsp9bXBwsJo2barr169r1apVGjBgQJljL126pO+//15NmjRxZHpAlaEkAQAAwPs53CY3fvx4LVq0SO+//76+/fZbjRkzRpcvX9bIkSMlSb/73e80adIk6/idO3cqNTVVhw8f1j//+U89+OCDKiws1IQJE6xjXnjhBW3ZskVHjx7V9u3bNWjQIPn7+2vYsGFV8BYBx1GSAAAA4P0c3jM0dOhQnT17VlOnTlVOTo46deqk9PR0a6nC8ePH5ef3c8a6evWqpkyZosOHD6t27drq27evPvjgA9WtW9c65uTJkxo2bJh++OEHNWzYUPfee6927Nihhg0bVv4dAhVQXJKQkVEUhNgfBAAA4H1MhmEYrp5EVbBYLAoNDZXZbFZISIirpwMPUV5JAgAAADyTvdnA4WVygLegJAEAAMC3EYbgsyhJAAAA8G2EIfgsShIAAAB8m9M/dBVwpZt9cColCQAAAL6LAgV4reI9QcV3ftauJfAAAAD4AgoU4PPYEwQAAIDyEIbgtdgTBAAAgPKwZwheiz1BAAAAKA9hCB7vZiUJhCAAAACUhmVy8Gh8cCoAAAAqijAEj0ZJAgAAACqKMASPRkkCAAAAKoo9Q/BolCQAAACgoghD8AiUJAAAAKCqsUwObo+SBAAAADgDYQhuj5IEAAAAOANhCG6PkgQAAAA4A3uG4PYoSQAAAIAzEIbgNihJAAAAQHVimRzcAiUJAAAAqG6EIbgFShIAAABQ3QhDcAuUJAAAAKC6sWcIboGSBAAAAFQ3whCqFSUJAAAAcBcsk0O1oSQBAAAA7oQwhGpDSQIAAADcCWEI1YaSBAAAALgT9gyh2lCSAAAAAHdCGEKVoyQBAAAAnoBlcqhSlCQAAADAUxCGUKUoSQAAAICnIAyhSlGSAAAAAE/BniFUKUoSAAAA4CkIQ6gQShIAAADg6VgmB4dRkgAAAABvQBiCwyhJAAAAgDcgDMFhlCQAAADAG7BnCA6jJAEAAADegDCEMlGSAAAAAG/GMjmUipIEAAAAeDvCEEpFSQIAAAC8HWEIpaIkAQAAAN6OPUMoFSUJAAAA8HaEIR9HSQIAAAB8FcvkfBglCQAAAPBlhCEfRkkCAAAAfBlhyIdRkgAAAABfxp4hH0ZJAgAAAHwZYcjLlVeQIFGSAAAAAN/FMjkvRkECAAAAUDbCkBejIAEAAAAoG2HIi1GQAAAAAJSNPUNejIIEAAAAoGyEIS9QXkkCBQkAAABA6Vgm5+EoSQAAAAAqhjDk4ShJAAAAACqGMOThKEkAAAAAKoY9Qx6OkgQAAACgYghDHoKSBAAAAKBqsUzOA1CSAAAAAFQ9wpAHoCQBAAAAqHqEIQ9ASQIAAABQ9dgz5AEoSQAAAACqHmHIjVCSAAAAAFQflsm5CUoSAAAAgOpFGHITlCQAAAAA1Ysw5CYoSQAAAACqF3uG3AQlCQAAAED1IgxVM0oSAAAAAPfAMrlqREkCAAAA4D4IQ9WIkgQAAADAfVQoDM2fP1/NmzdXcHCwoqOjtWvXrjLH5ufn6+WXX1arVq0UHBysqKgopaenV+qcnoqSBAAAAMB9OByGVqxYofHjxys5OVl79uxRVFSUEhISdObMmVLHT5kyRe+++67mzp2r/fv3a/To0Ro0aJD27t1b4XN6quKShGeeKfqT/UEAAACA65gMwzAceUF0dLTuuusuzZs3T5JUWFioiIgIPf3005o4cWKJ8eHh4Zo8ebLGjh1rPTZ48GDVrFlTf/3rXyt0ztJYLBaFhobKbDYrJCTEkbdU5corSQAAAADgXPZmA4fuDF27dk27d+9WfHz8zyfw81N8fLwyMzNLfU1eXp6Cg4NtjtWsWVPbtm2r8DmLz2uxWGwe7oCSBAAAAMAzOBSGzp07p4KCAoWFhdkcDwsLU05OTqmvSUhI0OzZs/Xdd9+psLBQGzZsUGpqqk6fPl3hc0rSzJkzFRoaan1EREQ48lachpIEAAAAwDM4vU3urbfeUps2bdS+fXsFBgbqqaee0siRI+XnV7lvPWnSJJnNZuvjxIkTVTTjyqEkAQAAAPAMDn3oaoMGDeTv76/c3Fyb47m5uWrcuHGpr2nYsKHWrFmjq1ev6ocfflB4eLgmTpyoli1bVvickhQUFKSgoCBHpl8tiksSMjKKghB7hgAAAAD35NDtmcDAQHXt2lWbNm2yHissLNSmTZsUExNT7muDg4PVtGlTXb9+XatWrdKAAQMqfU53lZgozZ5NEAIAAADcmUN3hiRp/PjxSkpKUrdu3dS9e3fNmTNHly9f1siRIyVJv/vd79S0aVPNnDlTkrRz505lZ2erU6dOys7O1rRp01RYWKgJEybYfU4AAAAAqGoOh6GhQ4fq7Nmzmjp1qnJyctSpUyelp6dbCxCOHz9usx/o6tWrmjJlig4fPqzatWurb9+++uCDD1S3bl27zwkAAAAAVc3hzxlyV+70OUMAAAAAXMcpnzMEAAAAAN6CMAQAAADAJxGGAAAAAPgkwhAAAAAAn0QYAgAAAOCTCEMAAAAAfBJhCAAAAIBPIgwBAAAA8EmEIQAAAAA+iTAEAAAAwCcRhgAAAAD4JMIQAAAAAJ9EGAIAAADgkwhDAAAAAHwSYQgAAACATyIMAQAAAPBJAa6eQFUxDEOSZLFYXDwTAAAAAK5UnAmKM0JZvCYMXbx4UZIUERHh4pkAAAAAcAcXL15UaGhomc+bjJvFJQ9RWFioU6dOqU6dOjKZTC6di8ViUUREhE6cOKGQkBCXzgWeg+sGFcF1g4ri2kFFcN2gIlxx3RiGoYsXLyo8PFx+fmXvDPKaO0N+fn5q1qyZq6dhIyQkhH9RwGFcN6gIrhtUFNcOKoLrBhVR3ddNeXeEilGgAAAAAMAnEYYAAAAA+CTCkBMEBQUpOTlZQUFBrp4KPAjXDSqC6wYVxbWDiuC6QUW483XjNQUKAAAAAOAI7gwBAAAA8EmEIQAAAAA+iTAEAAAAwCcRhgAAAAD4JMIQAAAAAJ9EGKqg+fPnq3nz5goODlZ0dLR27dpV7vi///3vat++vYKDg9WxY0d9+umn1TRTuBNHrptFixapZ8+e+tWvfqVf/epXio+Pv+l1Bu/k6L9vii1fvlwmk0kDBw507gThthy9di5cuKCxY8eqSZMmCgoKUtu2bfn/Kx/k6HUzZ84ctWvXTjVr1lRERITGjRunq1evVtNs4Q62bt2q/v37Kzw8XCaTSWvWrLnpazIyMtSlSxcFBQWpdevWSklJcfo8S0MYqoAVK1Zo/PjxSk5O1p49exQVFaWEhASdOXOm1PHbt2/XsGHD9Nhjj2nv3r0aOHCgBg4cqH379lXzzOFKjl43GRkZGjZsmDZv3qzMzExFRETogQceUHZ2djXPHK7k6HVT7OjRo3rhhRfUs2fPapop3I2j1861a9d0//336+jRo/roo4908OBBLVq0SE2bNq3mmcOVHL1uPvzwQ02cOFHJycn69ttv9d5772nFihV68cUXq3nmcKXLly8rKipK8+fPt2v8kSNH1K9fP8XFxSkrK0vPPfecHn/8cX322WdOnmkpDDise/fuxtixY61fFxQUGOHh4cbMmTNLHT9kyBCjX79+Nseio6ON//f//p9T5wn34uh180vXr1836tSpY7z//vvOmiLcUEWum+vXrxs9evQw/vKXvxhJSUnGgAEDqmGmcDeOXjsLFy40WrZsaVy7dq26pgg35Oh1M3bsWKN37942x8aPH2/cc889Tp0n3JckY/Xq1eWOmTBhgnH77bfbHBs6dKiRkJDgxJmVjjtDDrp27Zp2796t+Ph46zE/Pz/Fx8crMzOz1NdkZmbajJekhISEMsfD+1Tkuvmln376Sfn5+apXr56zpgk3U9Hr5uWXX1ajRo302GOPVcc04YYqcu2kpaUpJiZGY8eOVVhYmO644w69+uqrKigoqK5pw8Uqct306NFDu3fvti6lO3z4sD799FP17du3WuYMz+ROfzcOqPbv6OHOnTungoIChYWF2RwPCwvTgQMHSn1NTk5OqeNzcnKcNk+4l4pcN7/0xz/+UeHh4SX+5QHvVZHrZtu2bXrvvfeUlZVVDTOEu6rItXP48GF9/vnn+s1vfqNPP/1Uhw4d0pNPPqn8/HwlJydXx7ThYhW5bh599FGdO3dO9957rwzD0PXr1zV69GiWyaFcZf3d2GKx6MqVK6pZs2a1zYU7Q4AHeO2117R8+XKtXr1awcHBrp4O3NTFixc1fPhwLVq0SA0aNHD1dOBhCgsL1ahRI/35z39W165dNXToUE2ePFnvvPOOq6cGN5aRkaFXX31VCxYs0J49e5Samqp169bplVdecfXUALtwZ8hBDRo0kL+/v3Jzc22O5+bmqnHjxqW+pnHjxg6Nh/epyHVTbNasWXrttde0ceNG3Xnnnc6cJtyMo9fN999/r6NHj6p///7WY4WFhZKkgIAAHTx4UK1atXLupOEWKvLvnCZNmqhGjRry9/e3HrvtttuUk5Oja9euKTAw0KlzhutV5Lp56aWXNHz4cD3++OOSpI4dO+ry5ct64oknNHnyZPn58d/dUVJZfzcOCQmp1rtCEneGHBYYGKiuXbtq06ZN1mOFhYXatGmTYmJiSn1NTEyMzXhJ2rBhQ5nj4X0qct1I0p/+9Ce98sorSk9PV7du3apjqnAjjl437du319dff62srCzrIzEx0drWExERUZ3ThwtV5N8599xzjw4dOmQN0JL0n//8R02aNCEI+YiKXDc//fRTicBTHKgNw3DeZOHR3OrvxtVe2eAFli9fbgQFBRkpKSnG/v37jSeeeMKoW7eukZOTYxiGYQwfPtyYOHGidfwXX3xhBAQEGLNmzTK+/fZbIzk52ahRo4bx9ddfu+otwAUcvW5ee+01IzAw0Pjoo4+M06dPWx8XL1501VuACzh63fwSbXK+y9Fr5/jx40adOnWMp556yjh48KDxySefGI0aNTL+53/+x1VvAS7g6HWTnJxs1KlTx1i2bJlx+PBhY/369UarVq2MIUOGuOotwAUuXrxo7N2719i7d68hyZg9e7axd+9e49ixY4ZhGMbEiRON4cOHW8cfPnzYqFWrlvGHP/zB+Pbbb4358+cb/v7+Rnp6erXPnTBUQXPnzjVuvfVWIzAw0OjevbuxY8cO63O9evUykpKSbMavXLnSaNu2rREYGGjcfvvtxrp166p5xnAHjlw3kZGRhqQSj+Tk5OqfOFzK0X/f3Igw5NscvXa2b99uREdHG0FBQUbLli2NGTNmGNevX6/mWcPVHLlu8vPzjWnTphmtWrUygoODjYiICOPJJ580zp8/X/0Th8ts3ry51L+zFF8rSUlJRq9evUq8plOnTkZgYKDRsmVLY8mSJdU+b8MwDJNhcA8TAAAAgO9hzxAAAAAAn0QYAgAAAOCTCEMAAAAAfBJhCAAAAIBPIgwBAAAA8EmEIQAAAAA+iTAEAAAAwCcRhgAAAAD4JMIQAAAAAJ9EGAIAAADgkwhDAAAAAHzS/wfZMa5RhLLMmAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_preds = loaded_preds.to(device)\n",
        "preds == loaded_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr2cckoeMarY",
        "outputId": "f20371bf-34e2-45ef-b3a9-cec561623425"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    }
  ]
}